{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFlkp5nctujWtqk408XT0F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shruthimohan03/video-summarizer/blob/main/Time_based_chunking_each_chunk_is_a_cluster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNXoP3iB8FxI",
        "outputId": "cb9fd529-0acb-4f02-873e-51b27cc7e504"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "# Define phrases to remove (extended list)\n",
        "REMOVE_PHRASES = [\n",
        "    r'\\b[Ii] repeat\\b',\n",
        "    r'\\buh\\b', r'\\bum\\b', r'\\bhmm\\b', r'\\buhh\\b', r'\\bmmm\\b',\n",
        "    r'\\bokay\\b', r'\\bOkay\\b', r'\\bOK\\b', r'\\bok\\b',\n",
        "    r'\\bso\\b', r'\\bSo\\b',\n",
        "    r'\\byou know\\b', r'\\blike\\b', r'\\bbasically\\b',\n",
        "    r'\\bactually\\b', r'\\bkind of\\b', r'\\bsort of\\b',\n",
        "    r'\\bat the end of the day\\b', r'\\bwell\\b', r'\\bright\\b',\n",
        "    r'\\bjust\\b', r'\\bnow\\b', r'\\bthen\\b',\n",
        "    r'\\bin particular\\b', r'\\bin general\\b',\n",
        "    r'\\bI mean\\b', r'\\blet\\'s see\\b', r'\\bas I said\\b',\n",
        "    r'\\bremember\\b', r'\\bto be honest\\b', r'\\bI suppose\\b',\n",
        "    r'\\bin fact\\b', r'\\bI think\\b', r'\\bfor example\\b',\n",
        "    r'\\bin this particular\\b', r'\\bfor instance\\b',\n",
        "    r'\\bas you know\\b', r'\\bas you can see\\b',\n",
        "    r'\\blet me just\\b', r'\\bI\\'ll just\\b',\n",
        "    r'\\bif you think about it\\b', r'\\bas we discussed\\b',\n",
        "    r'\\bsuppose\\b', r'\\bby that I mean\\b',\n",
        "    r'\\bspecifically\\b', r'\\bbriefly\\b',\n",
        "    r'\\bas you have seen\\b', r'\\bas we have discussed\\b',\n",
        "    r'\\bas I have mentioned\\b',\n",
        "    r'\\bnow coming back to\\b', r'\\bnow, coming back to\\b',\n",
        "    r'\\btoday in this lecture\\b', r'\\bin this lecture\\b',\n",
        "    r'\\bas the name would suggest\\b', r'\\bremember the name\\b',\n",
        "    r'\\bthat is\\b', r'\\bthat means\\b',\n",
        "    r'\\bas I was saying\\b', r'\\bas we were discussing\\b'\n",
        "]\n",
        "\n",
        "REMOVE_REGEX = re.compile('|'.join(REMOVE_PHRASES))\n",
        "\n",
        "def load_transcript(file_path):\n",
        "    \"\"\"Load and preprocess a transcript from a text file.\"\"\"\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "        return preprocess_transcript(text)\n",
        "\n",
        "def preprocess_transcript(text):\n",
        "    \"\"\"Apply comprehensive preprocessing to transcript text.\"\"\"\n",
        "    # Convert to lowercase (optional, depending on requirements)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove filler phrases\n",
        "    text = REMOVE_REGEX.sub('', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Fix common transcription issues\n",
        "    text = fix_transcription_errors(text)\n",
        "\n",
        "    # Normalize punctuation\n",
        "    text = normalize_punctuation(text)\n",
        "\n",
        "    # Split into sentences (using NLTK for better segmentation)\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Optional: Remove short sentences (likely fragments)\n",
        "    sentences = [s for s in sentences if len(s.split()) > 3]\n",
        "\n",
        "    return sentences\n",
        "\n",
        "def fix_transcription_errors(text):\n",
        "    \"\"\"Fix common transcription errors.\"\"\"\n",
        "    # Fix repeated words (e.g., \"the the\")\n",
        "    text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)\n",
        "\n",
        "    # Fix common ASR errors\n",
        "    replacements = {\n",
        "        'Sanda': 'Sandhi',\n",
        "        'Sunday operation': 'Sandhi operation',\n",
        "        'GIFs law': 'Zipf\\'s law',\n",
        "        'GIF\\'s law': 'Zipf\\'s law',\n",
        "        'Heaps law': 'Heaps\\' law',\n",
        "        'Giph law': 'Zipf\\'s law',\n",
        "        'Tokenarage': 'Tokenizer',\n",
        "        'tokenarize': 'tokenize',\n",
        "        'lamtaization': 'lemmatization',\n",
        "        'lamtization': 'lemmatization',\n",
        "        'perqueous': 'previous',\n",
        "        'griddle': 'greedy',\n",
        "        'Eglidi': 'greedy',\n",
        "        'on the relation': 'sandhi relation',\n",
        "        'portus algorithm': 'Porter\\'s algorithm',\n",
        "        'clean it': 'kleene',\n",
        "        'affix is': 'affixes',\n",
        "        'prefossessing': 'preprocessing',\n",
        "        'infix': 'infix',\n",
        "        'perqueous': 'previous',\n",
        "        'coppers': 'corpus'\n",
        "    }\n",
        "\n",
        "    for old, new in replacements.items():\n",
        "        text = re.sub(r'\\b' + re.escape(old) + r'\\b', new, text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def normalize_punctuation(text):\n",
        "    \"\"\"Standardize punctuation in transcript.\"\"\"\n",
        "    # Fix spacing around punctuation\n",
        "    #text = re.sub(r'\\s+([.,;:!?])', r'\\1', text)\n",
        "\n",
        "    # Standardize quotes\n",
        "    text = re.sub(r'[\"\"\"]', '\"', text)\n",
        "    text = re.sub(r\"['\\']\", \"'\", text)\n",
        "\n",
        "    # Fix ellipses\n",
        "    text = re.sub(r'\\.\\.\\.', '…', text)\n",
        "\n",
        "    # Fix dashes\n",
        "    text = re.sub(r'--', '—', text)\n",
        "\n",
        "    #remove commas\n",
        "    text = re.sub(r',', '', text)\n",
        "    return text\n",
        "\n",
        "'''def remove_stopwords(sentences, exclude_list=None):\n",
        "    \"\"\"Remove stopwords from sentences.\"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    if exclude_list:\n",
        "        stop_words = stop_words - set(exclude_list)\n",
        "\n",
        "    result = []\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "        result.append(' '.join(filtered_words))\n",
        "\n",
        "    return result\n",
        "\n",
        "def stem_sentences(sentences):\n",
        "    \"\"\"Apply Porter stemming to sentences.\"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    result = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        stemmed_words = [stemmer.stem(word) for word in words]\n",
        "        result.append(' '.join(stemmed_words))\n",
        "\n",
        "    return result\n",
        "\n",
        "def lemmatize_sentences(sentences):\n",
        "    \"\"\"Apply lemmatization to sentences.\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    result = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "        result.append(' '.join(lemmatized_words))\n",
        "\n",
        "    return result'''\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/transcribed_text_nptel_video_1.txt\"\n",
        "\n",
        "    # Basic preprocessing\n",
        "    sentences = load_transcript(file_path)\n",
        "    print(f\"Number of sentences after basic preprocessing: {len(sentences)}\")\n",
        "\n",
        "    # Optional: Further processing based on needs\n",
        "    # sentences_without_stopwords = remove_stopwords(sentences)\n",
        "    # lemmatized_sentences = lemmatize_sentences(sentences)\n",
        "    # stemmed_sentences = stem_sentences(sentences)\n",
        "\n",
        "    # Print first few sentences\n",
        "    for i, sentence in enumerate(sentences[:15]):\n",
        "        print(f\"Sentence {i+1}: {sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oULsR5up726C",
        "outputId": "eccd8966-6d34-49db-eb6e-5f35d76fcd4f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences after basic preprocessing: 320\n",
            "Sentence 1: hello everyone welcome back to the final lecture of the first week.\n",
            "Sentence 2: in the last lecture we were discussing about various empirical laws.\n",
            "Sentence 3: gifs law and heaps law that how the words in the vocabulary are distributed in a corpus.\n",
            "Sentence 4: and we saw that the distribution is not very uniform.\n",
            "Sentence 5: there are certain words that are very very common.\n",
            "Sentence 6: we saw that roughly 100 words in the vocabulary make for 50 percent of the corpus but by that i mean that number of tokens.\n",
            "Sentence 7: and on the other hand there are 50 percent of the words in the vocabulary that occur only once.\n",
            "Sentence 8: and we discussed what are the various relationships among the vocabulary size and the number of tokens that i observe in a corpus.\n",
            "Sentence 9: and also how they grow with us to each other.\n",
            "Sentence 10: and gifs law gives gave me a relation between the frequency and the rank of a word.\n",
            "Sentence 11: we will start with the basic key processing in language.\n",
            "Sentence 12: we will cover the basic concepts and what are the challenges that one might face while doing the processing.\n",
            "Sentence 13: we are going to the basics of text processing.\n",
            "Sentence 14: we will start with the problem of tokenization.\n",
            "Sentence 15: the name token token is an individual word in my corpus.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abzPM6Mn70Lv",
        "outputId": "acd91541-8763-41ff-c55a-8402cb556c7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟢 Chunk 1: we saw that roughly 100 words in the vocabulary make for 50 percent of the corpus but by that i mean that number of tokens.\n",
            "   🔑 Keywords: 50, corpus, law, vocabulary, words\n",
            "\n",
            "🟢 Chunk 2: and we discussed what are the various relationships among the vocabulary size and the number of tokens that i observe in a corpus.\n",
            "   🔑 Keywords: corpus, percent, processing, vocabulary, words\n",
            "\n",
            "🟢 Chunk 3: we are going to the basics of text processing.\n",
            "   🔑 Keywords: characters, processing, sequence, various, words\n",
            "\n",
            "🟢 Chunk 4: you might not have to go to the individual sentence and you can talk about what are various words that are present in this document.\n",
            "   🔑 Keywords: characters, document, sequence, various, words\n",
            "\n",
            "🟢 Chunk 5: this is sentence one sentence two and on and this task is called sentence segmentation.\n",
            "   🔑 Keywords: document, individual, segmentation, sentence, task\n",
            "\n",
            "🟢 Chunk 6: this is sentence one sentence two and on and this task is called sentence segmentation.\n",
            "   🔑 Keywords: end, segmentation, sentence, task, trivial\n",
            "\n",
            "🟢 Chunk 7: can you think of a scenario where i have a dot in english and but it is not the end of sentence.\n",
            "   🔑 Keywords: dot, end, english, exclamation, sentence\n",
            "\n",
            "🟢 Chunk 8: whenever i have a exclamation or question mark i can say probably this is the end of the sentence but is the case the same with a dot.\n",
            "   🔑 Keywords: dot, end, exclamation, problem, sentence\n",
            "\n",
            "🟢 Chunk 9: even if it looks at trivial task we face this problem that can i always call dot as the end of the sentence.\n",
            "   🔑 Keywords: dot, end, problem, sentence, trivial\n",
            "\n",
            "🟢 Chunk 10: if you think of these as two classes end of the sentence or not end of the sentence.\n",
            "   🔑 Keywords: classes, divide, end, problem, sentence\n",
            "\n",
            "🟢 Chunk 11: you are classifying into one of the two classes.\n",
            "   🔑 Keywords: binary, classes, divide, end, sentence\n",
            "\n",
            "🟢 Chunk 12: in this case i have to build a binary classifier.\n",
            "   🔑 Keywords: build, classes, classifier, end, sentence\n",
            "\n",
            "🟢 Chunk 13: my classifiers that i will build can be some rules that i write by hand some simple if-ten-else rules.\n",
            "   🔑 Keywords: build, end, example, sentence, simple\n",
            "\n",
            "🟢 Chunk 14: i can have this simple if-ten-else decision tree here.\n",
            "   🔑 Keywords: blank, decision, end, lines, tree\n",
            "\n",
            "🟢 Chunk 15: if it is a period if it is not a period it is easy to say that this is not the end of the sentence.\n",
            "   🔑 Keywords: blank, end, lines, period, sentence\n",
            "\n",
            "🟢 Chunk 16: it is not i will check if the final punctuation is a period.\n",
            "   🔑 Keywords: check, end, period, say, sentence\n",
            "\n",
            "🟢 Chunk 17: if the answer is no  this word is not an abbreviation and this will be the end of the sentence.\n",
            "   🔑 Keywords: abbreviation, abbreviations, answer, sentence, word\n",
            "\n",
            "🟢 Chunk 18: i see the word ending with dot.\n",
            "   🔑 Keywords: dot, use, various, want, word\n",
            "\n",
            "🟢 Chunk 19: again by feature you can think of a simple rule whether the word i am currently at is a number or i can use the fact where the case of the word with dot is uppercase or lowercase.\n",
            "   🔑 Keywords: dot, feature, number, uppercase, word\n",
            "\n",
            "🟢 Chunk 20: same with lowercase lowercase will give me more probability that this is not an abbreviation.\n",
            "   🔑 Keywords: lowercase, number, uppercase, use, word\n",
            "\n",
            "🟢 Chunk 21: similarly i can also use the case of the word after dot.\n",
            "   🔑 Keywords: abbreviation, features, lowercase, starts, uppercase\n",
            "\n",
            "🟢 Chunk 22: same thing i can do with the next word after dot.\n",
            "   🔑 Keywords: dot, end, length, sentence, word\n",
            "\n",
            "🟢 Chunk 23: and i can also use probably the what is the probability that the word ending with dot occurs at the end of the sentence.\n",
            "   🔑 Keywords: dot, end, sentence, use, word\n",
            "\n",
            "🟢 Chunk 24: this problem you will see is again language dependent.\n",
            "   🔑 Keywords: hindi, language, problem, sentence, use\n",
            "\n",
            "🟢 Chunk 25: all these are my observations that i use as my features.\n",
            "   🔑 Keywords: end, features, language, observations, problem\n",
            "\n",
            "🟢 Chunk 26: all these are my observations that i use as my features.\n",
            "   🔑 Keywords: class, features, observations, set, word\n",
            "\n",
            "🟢 Chunk 27: you have to choose a feature value splits my data into certain parts and i have certain criteria to find out what is the best split.\n",
            "   🔑 Keywords: algorithms, criteria, features, length, word\n",
            "\n",
            "🟢 Chunk 28: all these are quite popular classifiers for various nlp applications.\n",
            "   🔑 Keywords: algorithms, certain, choose, criteria, particular\n",
            "\n",
            "🟢 Chunk 29: we talked about token and type distinction.\n",
            "   🔑 Keywords: 11, classifiers, different, tokenization, words\n",
            "\n",
            "🟢 Chunk 30: you have 11 word tokens but how many unique words are there.\n",
            "   🔑 Keywords: 11, different, tokens, word, words\n",
            "\n",
            "🟢 Chunk 31: my tokenization is to find out each of the 11 word tokens from the sentence.\n",
            "   🔑 Keywords: 11, tokens, unique, use, word\n",
            "\n",
            "🟢 Chunk 32: but still it is good to know what are the challenges that are involved when i have tried to design a tokenization algorithm.\n",
            "   🔑 Keywords: lands, single, token, tokens, treated\n",
            "\n",
            "🟢 Chunk 33: similarly if you see what are do i treated as a single token or two tokens what are this problem you might have to solve in the same step whether i treated as a single token or multiple tokens same with i am shouldn't and on.\n",
            "   🔑 Keywords: multiple, single, token, tokens, treated\n",
            "\n",
            "🟢 Chunk 34: another problem can be how do i handle hyphens in my data.\n",
            "   🔑 Keywords: dot, entity, problem, single, tokens\n",
            "\n",
            "🟢 Chunk 35: in the sentence itself you see two different hyphens one is with initiative another is show hyphen time.\n",
            "   🔑 Keywords: hyphen, hyphens, sentence, time, use\n",
            "\n",
            "🟢 Chunk 36: in the sentence itself you see two different hyphens one is with initiative another is show hyphen time.\n",
            "   🔑 Keywords: hyphen, hyphens, initiative, sentence, time\n",
            "\n",
            "🟢 Chunk 37: you will end up within hyphen.\n",
            "   🔑 Keywords: end, hyphens, say, sentence, year\n",
            "\n",
            "🟢 Chunk 38: again when you are doing tokenization you have problem at how do i handle all these hyphens.\n",
            "   🔑 Keywords: handle, hyphens, interpret, problem, sentence\n",
            "\n",
            "🟢 Chunk 39: you have four words in english when you are putting in french they make a compound.\n",
            "   🔑 Keywords: compound, english, german, problem, words\n",
            "\n",
            "🟢 Chunk 40: you have four words in english when you are putting in french they make a compound.\n",
            "   🔑 Keywords: chinese, compound, english, german, words\n",
            "\n",
            "🟢 Chunk 41: what do you see in chinese words are written without any spaces in between.\n",
            "   🔑 Keywords: chinese, japanese, problem, sentence, words\n",
            "\n",
            "🟢 Chunk 42: the same problem is there even for sanskrit.\n",
            "   🔑 Keywords: problem, sanskrit, sequence, word, words\n",
            "\n",
            "🟢 Chunk 43: it is composed of multiple words in sanskrit and they are combined with this sandhi relation.\n",
            "   🔑 Keywords: sanskrit, sentence, single, word, words\n",
            "\n",
            "🟢 Chunk 44: in japanese and in chinese when you try to combine various words together you simply concatenate them.\n",
            "   🔑 Keywords: sanskrit, sentence, single, word, words\n",
            "\n",
            "🟢 Chunk 45: from here i have to find out the actual words are briyath plus na that gives me this briyana and this is very very common in sanskrit that you are always combining words by doing sanda operation.\n",
            "   🔑 Keywords: combining, sanskrit, sentence, word, words\n",
            "\n",
            "🟢 Chunk 46: you see in sanskrit the longest word is composed of 431 characters it is the compound and you have greek and african and other languages.\n",
            "   🔑 Keywords: problem, sanskrit, segmentation, word, words\n",
            "\n",
            "🟢 Chunk 47: when i talk about this problem of tokenization in sanskrit or in english this problem is also called word segmentation.\n",
            "   🔑 Keywords: algorithm, problem, simplest, word, words\n",
            "\n",
            "🟢 Chunk 48: you will find out what is the maximum match as per my dictionary in the string you break there and put the pointer from at the next correct and again do the same thing.\n",
            "   🔑 Keywords: algorithm, dictionary, maximum, string, words\n",
            "\n",
            "🟢 Chunk 49: if you are given a hashtag and you have to analyze that you have to segment it into various words.\n",
            "   🔑 Keywords: cases, hashtags, match, maximum, words\n",
            "\n",
            "🟢 Chunk 50: first we have a generative model that says how do i generate a sentence in sanskrit?\n",
            "   🔑 Keywords: alphabet, sanskrit, segment, various, words\n",
            "\n",
            "🟢 Chunk 51: i have my set of inserted words also called padaj in sanskrit and i have the relation of sunday between them and how i generate a sentence.\n",
            "   🔑 Keywords: generate, sanskrit, set, sunday, words\n",
            "\n",
            "🟢 Chunk 52: when i have a set of words i can combine them together with an operation of sunday.\n",
            "   🔑 Keywords: operation, sentence, set, sunday, words\n",
            "\n",
            "🟢 Chunk 53: the inverse problem whenever i am given a sentence w i have to analyze it by inverting the relations of sunday.\n",
            "   🔑 Keywords: problem, segmentarage, sentence, sunday, word\n",
            "\n",
            "🟢 Chunk 54: we talked about this problem that the same word might be written multiple different ways u dot s dot a versus u s a.  i should be able to match them together especially if you are doing information retrieval you are giving a query and you are retrieving from some document.\n",
            "   🔑 Keywords: doing, dot, problem, ways, word\n",
            "\n",
            "🟢 Chunk 55: similarly you might want to keep u s for united states in upper case and not do the case folding.\n",
            "   🔑 Keywords: case, dot, folding, query, word\n",
            "\n",
            "🟢 Chunk 56: and this is important for the application of machine translation also because if you do a case folding here you will know u s in lower case something else was such u s in united states.\n",
            "   🔑 Keywords: application, cars, case, folding, want\n",
            "\n",
            "🟢 Chunk 57: you have which stems that are the originally head words and the affixes that are what are the different units as for plural etcetera you are applying to them to make the individual word.\n",
            "   🔑 Keywords: cars, derived, head, individual, word\n",
            "\n",
            "🟢 Chunk 58: you have which stems that are the originally head words and the affixes that are what are the different units as for plural etcetera you are applying to them to make the individual word.\n",
            "   🔑 Keywords: actual, etcetera, head, morphology, word\n",
            "\n",
            "🟢 Chunk 59: some examples are for prefix you have un-ant-e etcetera for english and a teapra etcetera for hindi or sanskrit suffix etiation etcetera and tarqa k etcetera for hindi.\n",
            "   🔑 Keywords: actual, automate, etcetera, hindi, word\n",
            "\n",
            "🟢 Chunk 60: if you try to do a stemming here this is you will find from example e is removed from compressed edged removed and on.\n",
            "   🔑 Keywords: actual, automate, stemming, try, word\n",
            "\n",
            "🟢 Chunk 61: i take a word if it ends with s e s i remove e s from there and i end with s.  example is carousers goes to carous.\n",
            "   🔑 Keywords: algorithm, ends, example, stemming, word\n",
            "\n",
            "🟢 Chunk 62: i take a word if it ends with s e s i remove e s from there and i end with s.  example is carousers goes to carous.\n",
            "   🔑 Keywords: ends, goes, remove, vowel, word\n",
            "\n",
            "🟢 Chunk 63: if you did not have this vowel you would have converted king to k and that there is some other rules if the word ends with aishinal i put a t e relational relational to relate and if the words and what ends with i z r i can i remove that r digitize i z digitize a t e r to a t e and if the word ends with l i remove that l if the word ends with able i remove that able if the words and with a t e i remove that a t e.  that these are some steps that i take from my corpus for each word i converted to its time.\n",
            "   🔑 Keywords: able, ends, remove, vowel, word\n",
            "\n",
            "🟢 Chunk 64: if you did not have this vowel you would have converted king to k and that there is some other rules if the word ends with aishinal i put a t e relational relational to relate and if the words and what ends with i z r i can i remove that r digitize i z digitize a t e r to a t e and if the word ends with l i remove that l if the word ends with able i remove that able if the words and with a t e i remove that a t e.  that these are some steps that i take from my corpus for each word i converted to its time.\n",
            "   🔑 Keywords: able, ends, remove, week, word\n",
            "\n",
            "📝 Final Summary:\n",
            " we saw that roughly 100 words in the vocabulary make for 50 percent of the corpus but by that i mean that number of tokens. and we discussed what are the various relationships among the vocabulary size and the number of tokens that i observe in a corpus. we are going to the basics of text processing. you might not have to go to the individual sentence and you can talk about what are various words that are present in this document. this is sentence one sentence two and on and this task is called sentence segmentation. this is sentence one sentence two and on and this task is called sentence segmentation. can you think of a scenario where i have a dot in english and but it is not the end of sentence. whenever i have a exclamation or question mark i can say probably this is the end of the sentence but is the case the same with a dot. even if it looks at trivial task we face this problem that can i always call dot as the end of the sentence. if you think of these as two classes end of the sentence or not end of the sentence. you are classifying into one of the two classes. in this case i have to build a binary classifier. my classifiers that i will build can be some rules that i write by hand some simple if-ten-else rules. i can have this simple if-ten-else decision tree here. if it is a period if it is not a period it is easy to say that this is not the end of the sentence. it is not i will check if the final punctuation is a period. if the answer is no  this word is not an abbreviation and this will be the end of the sentence. i see the word ending with dot. again by feature you can think of a simple rule whether the word i am currently at is a number or i can use the fact where the case of the word with dot is uppercase or lowercase. same with lowercase lowercase will give me more probability that this is not an abbreviation. similarly i can also use the case of the word after dot. same thing i can do with the next word after dot. and i can also use probably the what is the probability that the word ending with dot occurs at the end of the sentence. this problem you will see is again language dependent. all these are my observations that i use as my features. all these are my observations that i use as my features. you have to choose a feature value splits my data into certain parts and i have certain criteria to find out what is the best split. all these are quite popular classifiers for various nlp applications. we talked about token and type distinction. you have 11 word tokens but how many unique words are there. my tokenization is to find out each of the 11 word tokens from the sentence. but still it is good to know what are the challenges that are involved when i have tried to design a tokenization algorithm. similarly if you see what are do i treated as a single token or two tokens what are this problem you might have to solve in the same step whether i treated as a single token or multiple tokens same with i am shouldn't and on. another problem can be how do i handle hyphens in my data. in the sentence itself you see two different hyphens one is with initiative another is show hyphen time. in the sentence itself you see two different hyphens one is with initiative another is show hyphen time. you will end up within hyphen. again when you are doing tokenization you have problem at how do i handle all these hyphens. you have four words in english when you are putting in french they make a compound. you have four words in english when you are putting in french they make a compound. what do you see in chinese words are written without any spaces in between. the same problem is there even for sanskrit. it is composed of multiple words in sanskrit and they are combined with this sandhi relation. in japanese and in chinese when you try to combine various words together you simply concatenate them. from here i have to find out the actual words are briyath plus na that gives me this briyana and this is very very common in sanskrit that you are always combining words by doing sanda operation. you see in sanskrit the longest word is composed of 431 characters it is the compound and you have greek and african and other languages. when i talk about this problem of tokenization in sanskrit or in english this problem is also called word segmentation. you will find out what is the maximum match as per my dictionary in the string you break there and put the pointer from at the next correct and again do the same thing. if you are given a hashtag and you have to analyze that you have to segment it into various words. first we have a generative model that says how do i generate a sentence in sanskrit? i have my set of inserted words also called padaj in sanskrit and i have the relation of sunday between them and how i generate a sentence. when i have a set of words i can combine them together with an operation of sunday. the inverse problem whenever i am given a sentence w i have to analyze it by inverting the relations of sunday. we talked about this problem that the same word might be written multiple different ways u dot s dot a versus u s a.  i should be able to match them together especially if you are doing information retrieval you are giving a query and you are retrieving from some document. similarly you might want to keep u s for united states in upper case and not do the case folding. and this is important for the application of machine translation also because if you do a case folding here you will know u s in lower case something else was such u s in united states. you have which stems that are the originally head words and the affixes that are what are the different units as for plural etcetera you are applying to them to make the individual word. you have which stems that are the originally head words and the affixes that are what are the different units as for plural etcetera you are applying to them to make the individual word. some examples are for prefix you have un-ant-e etcetera for english and a teapra etcetera for hindi or sanskrit suffix etiation etcetera and tarqa k etcetera for hindi. if you try to do a stemming here this is you will find from example e is removed from compressed edged removed and on. i take a word if it ends with s e s i remove e s from there and i end with s.  example is carousers goes to carous. i take a word if it ends with s e s i remove e s from there and i end with s.  example is carousers goes to carous. if you did not have this vowel you would have converted king to k and that there is some other rules if the word ends with aishinal i put a t e relational relational to relate and if the words and what ends with i z r i can i remove that r digitize i z digitize a t e r to a t e and if the word ends with l i remove that l if the word ends with able i remove that able if the words and with a t e i remove that a t e.  that these are some steps that i take from my corpus for each word i converted to its time. if you did not have this vowel you would have converted king to k and that there is some other rules if the word ends with aishinal i put a t e relational relational to relate and if the words and what ends with i z r i can i remove that r digitize i z digitize a t e r to a t e and if the word ends with l i remove that l if the word ends with able i remove that able if the words and with a t e i remove that a t e.  that these are some steps that i take from my corpus for each word i converted to its time.\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load SBERT model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def chunk_sentences(sentences, chunk_size):\n",
        "    \"\"\"Divide transcript into overlapping time-based chunks.\"\"\"\n",
        "    return [sentences[i:i + chunk_size] for i in range(0, len(sentences), chunk_size // 2)]\n",
        "\n",
        "def extract_central_sentence(sentences, embeddings):\n",
        "    \"\"\"Select the most central sentence in the chunk.\"\"\"\n",
        "    centroid = np.mean(embeddings, axis=0)\n",
        "    closest_idx = np.argmin(np.linalg.norm(embeddings - centroid, axis=1))\n",
        "    return sentences[closest_idx]\n",
        "\n",
        "def extract_keywords(sentences, num_keywords=5):\n",
        "    \"\"\"Extract top keywords using TF-IDF.\"\"\"\n",
        "    vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=num_keywords)\n",
        "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "    feature_array = np.array(vectorizer.get_feature_names_out())\n",
        "    return list(feature_array)\n",
        "\n",
        "def summarize_transcript(sentences, chunk_size):\n",
        "    \"\"\"Summarize transcript by treating each chunk as a cluster.\"\"\"\n",
        "    chunks = chunk_sentences(sentences, chunk_size)\n",
        "\n",
        "    summary = []\n",
        "    chunk_keywords = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        embeddings = np.array(model.encode(chunk))\n",
        "        central_sentence = extract_central_sentence(chunk, embeddings)\n",
        "        keywords = extract_keywords(chunk)\n",
        "\n",
        "        summary.append(central_sentence)\n",
        "        chunk_keywords.append(keywords)\n",
        "\n",
        "    # Print topics & their key sentences\n",
        "    for i, (sent, keys) in enumerate(zip(summary, chunk_keywords)):\n",
        "        print(f\"\\n🟢 Chunk {i+1}: {sent}\")\n",
        "        print(f\"   🔑 Keywords: {', '.join(keys)}\")\n",
        "\n",
        "    return \" \".join(summary)\n",
        "\n",
        "summary = summarize_transcript(sentences, chunk_size=10)\n",
        "print(\"\\n📝 Final Summary:\\n\", summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b08KT7H779_W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}