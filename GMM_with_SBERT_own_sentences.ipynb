{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2+gjYlsZubtOojp55syJO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shruthimohan03/video-summarizer/blob/main/GMM_with_SBERT_own_sentences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and Preprocess Transcript from a TXT File"
      ],
      "metadata": {
        "id": "5au0OeKrgbww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Load transcript from a text file\n",
        "def load_transcript(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # Split text into sentences (basic sentence segmentation)\n",
        "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
        "    return [s.strip() for s in sentences if s.strip()]  # Remove empty sentences\n",
        "\n",
        "# Example usage\n",
        "file_path = \"/content/text_processing.txt\"  # Update with actual file path\n",
        "sentences = load_transcript(file_path)\n"
      ],
      "metadata": {
        "id": "ZMSOAbqSgdob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess and Convert Sentences into BERT Embeddings"
      ],
      "metadata": {
        "id": "02U18Z5GeUuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load SBERT model\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "# Convert transcript sentences to embeddings\n",
        "def get_sentence_embeddings(sentences):\n",
        "    return model.encode(sentences, convert_to_numpy=True)\n",
        "\n",
        "# Generate embeddings\n",
        "sentence_vectors = get_sentence_embeddings(sentences)\n"
      ],
      "metadata": {
        "id": "K4gkNpXVeBJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply Gaussian Mixture Model (GMM) for Clustering"
      ],
      "metadata": {
        "id": "ED_OKgCTeYEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Choose the number of clusters based on BIC/AIC\n",
        "num_clusters = 5\n",
        "\n",
        "# Fit GMM\n",
        "gmm = GaussianMixture(n_components=num_clusters, random_state=42)\n",
        "gmm.fit(sentence_vectors)\n",
        "\n",
        "# Predict cluster labels\n",
        "labels = gmm.predict(sentence_vectors)\n"
      ],
      "metadata": {
        "id": "XMXOAhp8eD_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Group Sentences by Cluster"
      ],
      "metadata": {
        "id": "UCQKj0DCecyk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MAYH8Ojd3Tj",
        "outputId": "2525cb17-88cf-4db8-c8db-8bc556dab529"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟢 Topic 2:\n",
            "  - Tokenization is a foundational step in text processing where text is divided into smaller units called tokens.\n",
            "  - These tokens can be individual words or sentences, depending on the granularity required.\n",
            "  - For example, tokenizing the sentence “I’m learning NLP!” results in the tokens [\"I\", \"'m\", \"learning\", \"NLP\", \"!\"].\n",
            "  - Tokenization is essential for enabling downstream natural language processing (NLP) tasks, such as sentiment analysis and machine translation, as it breaks down complex text into manageable pieces.\n",
            "  - Libraries like NLTK and spaCy provide efficient tokenization methods, making it easy to prepare text for analysis.\n",
            "\n",
            "🟢 Topic 0:\n",
            "  - Stopword removal is the process of filtering out common words that do not carry substantial meaning, such as “the,” “is,” and “and.” Removing these stopwords reduces the dimensionality of the dataset, which can enhance the performance of NLP models by focusing on more meaningful words.\n",
            "  - For instance, the sentence “She is reading a book” becomes [\"She\", \"reading\", \"book\"] after removing stopwords.\n",
            "  - Each language has its own set of stopwords, and many NLP libraries have built-in lists to streamline this process.\n",
            "  - Lemmatization involves converting words to their base or dictionary form, known as the lemma.\n",
            "  - Unlike stemming, which often chops off word endings indiscriminately, lemmatization considers the context and part of speech of a word.\n",
            "  - For example, “running” and “ran” are both reduced to “run.” This normalization step enhances the accuracy of NLP models by treating different forms of a word equivalently.\n",
            "  - Tools like WordNetLemmatizer in NLTK and spaCy’s lemmatization functions make it straightforward to incorporate this into preprocessing pipelines.\n",
            "\n",
            "🟢 Topic 4:\n",
            "  - This step is particularly useful in applications like search engines, where reducing noise can lead to more relevant search results.\n",
            "  - This process is vital for tasks like information extraction and organizing large datasets of unstructured text.\n",
            "  - It helps NLP systems comprehend the role of each word in a sentence, leading to better analysis and generation of text.\n",
            "\n",
            "🟢 Topic 1:\n",
            "  - Named Entity Recognition (NER) is a technique used to identify and categorize entities in text, such as names, locations, dates, and organizations.\n",
            "  - For example, in the sentence “Elon Musk founded Tesla in 2003,” NER would tag “Elon Musk” as a person, “Tesla” as an organization, and “2003” as a date.\n",
            "  - NER can be approached with deep learning models like BERT or traditional NLP tools like spaCy, and pre-trained models are often available to quickly derive insights from text.\n",
            "\n",
            "🟢 Topic 3:\n",
            "  - Part-of-Speech (POS) tagging assigns grammatical categories—like nouns, verbs, and adjectives—to each word in a sentence.\n",
            "  - In “The quick brown fox jumps over the lazy dog,” the tags are (\"The\", DT), (\"quick\", JJ), (\"fox\", NN), (\"jumps\", VBZ), etc.\n",
            "  - POS tagging is crucial for understanding syntactic structures and is applied in tasks like parsing and machine translation.\n",
            "  - Libraries like NLTK and spaCy offer robust POS tagging functionalities with pre-trained models for a variety of languages.\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Group sentences by their assigned topic\n",
        "clustered_sentences = defaultdict(list)\n",
        "for i, label in enumerate(labels):\n",
        "    clustered_sentences[label].append(sentences[i])\n",
        "\n",
        "# Print topics and their sentences\n",
        "for topic, sentences in clustered_sentences.items():\n",
        "    print(f\"\\n🟢 Topic {topic}:\")\n",
        "    for s in sentences:\n",
        "        print(f\"  - {s}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract Keywords for Each Cluster"
      ],
      "metadata": {
        "id": "97pUOQ02enfW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get topic keywords, use TF-IDF on sentences inside each cluster."
      ],
      "metadata": {
        "id": "lCd5zEEFerol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Get keywords for each topic\n",
        "def get_top_keywords(sentences, num_words=5):\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform(sentences)\n",
        "    feature_array = vectorizer.get_feature_names_out()\n",
        "    tfidf_sorting = X.sum(axis=0).A1.argsort()[::-1]\n",
        "    return [feature_array[i] for i in tfidf_sorting[:num_words]]\n",
        "\n",
        "# Print top words per topic\n",
        "for topic, sentences in clustered_sentences.items():\n",
        "    keywords = get_top_keywords(sentences)\n",
        "    print(f\"\\n🔹 Topic {topic} Keywords: {keywords}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j44OEqCwd_ah",
        "outputId": "4e2ad410-c7fd-4eb4-b197-a5065809b0ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Topic 2 Keywords: ['text', 'nlp', 'tokens', 'learning', 'tokenization']\n",
            "\n",
            "🔹 Topic 0 Keywords: ['word', 'lemmatization', 'words', 'stopwords', 'nlp']\n",
            "\n",
            "🔹 Topic 4 Keywords: ['search', 'text', 'like', 'unstructured', 'tasks']\n",
            "\n",
            "🔹 Topic 1 Keywords: ['like', 'models', 'musk', 'tesla', 'elon']\n",
            "\n",
            "🔹 Topic 3 Keywords: ['tagging', 'like', 'pos', 'quick', 'fox']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like clustering happened based on: Tokenization, Stopword/word/lemmatization, importance, NER, POS"
      ],
      "metadata": {
        "id": "kS9Ryw_Y4Vd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trying it with 6 clusters to see if stemming and lemmitization are grouped differently"
      ],
      "metadata": {
        "id": "fiTNtpyl41Qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Load transcript from a text file\n",
        "def load_transcript(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # Split text into sentences (basic sentence segmentation)\n",
        "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
        "    return [s.strip() for s in sentences if s.strip()]  # Remove empty sentences\n",
        "\n",
        "# Example usage\n",
        "file_path = \"/content/text_processing.txt\"  # Update with actual file path\n",
        "sentences = load_transcript(file_path)"
      ],
      "metadata": {
        "id": "jTO394Yy5V_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load SBERT model\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "# Convert transcript sentences to embeddings\n",
        "def get_sentence_embeddings(sentences):\n",
        "    return model.encode(sentences, convert_to_numpy=True)\n",
        "\n",
        "# Generate embeddings\n",
        "sentence_vectors = get_sentence_embeddings(sentences)\n"
      ],
      "metadata": {
        "id": "51P_f9fa5MYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### GMM for clustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Choose the number of clusters based on BIC/AIC\n",
        "num_clusters = 6\n",
        "\n",
        "# Fit GMM\n",
        "gmm = GaussianMixture(n_components=num_clusters, random_state=42)\n",
        "gmm.fit(sentence_vectors)\n",
        "\n",
        "# Predict cluster labels\n",
        "labels = gmm.predict(sentence_vectors)"
      ],
      "metadata": {
        "id": "iF91PmTt409P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Group sentences by their assigned topic\n",
        "clustered_sentences = defaultdict(list)\n",
        "for i, label in enumerate(labels):\n",
        "    clustered_sentences[label].append(sentences[i])\n",
        "\n",
        "# Print topics and their sentences\n",
        "for topic, sentences in clustered_sentences.items():\n",
        "    print(f\"\\n🟢 Topic {topic}:\")\n",
        "    for s in sentences:\n",
        "        print(f\"  - {s}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZR5ZZAb4yQo",
        "outputId": "20747f1c-68ee-4459-9e5f-e8aab7b7b7c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟢 Topic 2:\n",
            "  - Tokenization is a foundational step in text processing where text is divided into smaller units called tokens.\n",
            "  - These tokens can be individual words or sentences, depending on the granularity required.\n",
            "  - For example, tokenizing the sentence “I’m learning NLP!” results in the tokens [\"I\", \"'m\", \"learning\", \"NLP\", \"!\"].\n",
            "  - Tokenization is essential for enabling downstream natural language processing (NLP) tasks, such as sentiment analysis and machine translation, as it breaks down complex text into manageable pieces.\n",
            "  - Lemmatization involves converting words to their base or dictionary form, known as the lemma.\n",
            "\n",
            "🟢 Topic 5:\n",
            "  - Libraries like NLTK and spaCy provide efficient tokenization methods, making it easy to prepare text for analysis.\n",
            "  - Tools like WordNetLemmatizer in NLTK and spaCy’s lemmatization functions make it straightforward to incorporate this into preprocessing pipelines.\n",
            "  - This process is vital for tasks like information extraction and organizing large datasets of unstructured text.\n",
            "\n",
            "🟢 Topic 0:\n",
            "  - Stopword removal is the process of filtering out common words that do not carry substantial meaning, such as “the,” “is,” and “and.” Removing these stopwords reduces the dimensionality of the dataset, which can enhance the performance of NLP models by focusing on more meaningful words.\n",
            "  - For instance, the sentence “She is reading a book” becomes [\"She\", \"reading\", \"book\"] after removing stopwords.\n",
            "  - Each language has its own set of stopwords, and many NLP libraries have built-in lists to streamline this process.\n",
            "  - Unlike stemming, which often chops off word endings indiscriminately, lemmatization considers the context and part of speech of a word.\n",
            "  - For example, “running” and “ran” are both reduced to “run.” This normalization step enhances the accuracy of NLP models by treating different forms of a word equivalently.\n",
            "\n",
            "🟢 Topic 4:\n",
            "  - This step is particularly useful in applications like search engines, where reducing noise can lead to more relevant search results.\n",
            "  - It helps NLP systems comprehend the role of each word in a sentence, leading to better analysis and generation of text.\n",
            "\n",
            "🟢 Topic 1:\n",
            "  - Named Entity Recognition (NER) is a technique used to identify and categorize entities in text, such as names, locations, dates, and organizations.\n",
            "  - For example, in the sentence “Elon Musk founded Tesla in 2003,” NER would tag “Elon Musk” as a person, “Tesla” as an organization, and “2003” as a date.\n",
            "  - NER can be approached with deep learning models like BERT or traditional NLP tools like spaCy, and pre-trained models are often available to quickly derive insights from text.\n",
            "\n",
            "🟢 Topic 3:\n",
            "  - Part-of-Speech (POS) tagging assigns grammatical categories—like nouns, verbs, and adjectives—to each word in a sentence.\n",
            "  - In “The quick brown fox jumps over the lazy dog,” the tags are (\"The\", DT), (\"quick\", JJ), (\"fox\", NN), (\"jumps\", VBZ), etc.\n",
            "  - POS tagging is crucial for understanding syntactic structures and is applied in tasks like parsing and machine translation.\n",
            "  - Libraries like NLTK and spaCy offer robust POS tagging functionalities with pre-trained models for a variety of languages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Get keywords for each topic\n",
        "def get_top_keywords(sentences, num_words=5):\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform(sentences)\n",
        "    feature_array = vectorizer.get_feature_names_out()\n",
        "    tfidf_sorting = X.sum(axis=0).A1.argsort()[::-1]\n",
        "    return [feature_array[i] for i in tfidf_sorting[:num_words]]\n",
        "\n",
        "# Print top words per topic\n",
        "for topic, sentences in clustered_sentences.items():\n",
        "    keywords = get_top_keywords(sentences)\n",
        "    print(f\"\\n🔹 Topic {topic} Keywords: {keywords}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJvVIbks4yA1",
        "outputId": "b9026ef4-0ea9-434e-c012-453bcf399437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Topic 2 Keywords: ['text', 'nlp', 'tokens', 'learning', 'words']\n",
            "\n",
            "🔹 Topic 5 Keywords: ['like', 'text', 'nltk', 'spacy', 'datasets']\n",
            "\n",
            "🔹 Topic 0 Keywords: ['word', 'reading', 'book', 'stopwords', 'nlp']\n",
            "\n",
            "🔹 Topic 4 Keywords: ['search', 'word', 'nlp', 'better', 'comprehend']\n",
            "\n",
            "🔹 Topic 1 Keywords: ['like', 'models', 'musk', 'tesla', 'elon']\n",
            "\n",
            "🔹 Topic 3 Keywords: ['tagging', 'like', 'pos', 'quick', 'fox']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Didn't work well. finetuning is a better option"
      ],
      "metadata": {
        "id": "YSUtKfor5pvo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Since the clustering is not working as per expectation, we make some changes to improve the clustering"
      ],
      "metadata": {
        "id": "6lNZl5t7VUuU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. In gmm, covariance_type='full' to sharpen the clusters: Each component has its own full covariance matrix. Allows each component to have a unique shape, orientation, and size in all dimensions. Provides the most flexibility but also increases computational cost.\n",
        "\n",
        "2. Normalize Embeddings"
      ],
      "metadata": {
        "id": "GIqsllwYoTu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Load transcript from a text file\n",
        "def load_transcript(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # Split text into sentences (basic sentence segmentation)\n",
        "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
        "    return [s.strip() for s in sentences if s.strip()]  # Remove empty sentences\n",
        "\n",
        "# Example usage\n",
        "file_path = \"/content/text_processing.txt\"  # Update with actual file path\n",
        "sentences = load_transcript(file_path)"
      ],
      "metadata": {
        "id": "MoFV4O6ko2CA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load SBERT model\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = model.encode(sentences, convert_to_tensor=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_ig90O1o2CB",
        "outputId": "f75d3cab-94e5-4daf-f459-9839463cead0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalize the embeddings"
      ],
      "metadata": {
        "id": "q3gB_5rHpOqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "normalized_embeddings = scaler.fit_transform(embeddings)\n",
        "print(len(normalized_embeddings))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRsJnz2opOBE",
        "outputId": "5f8d6693-1c50-4ae5-ebf8-918eadf66dd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### GMM for clustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Apply Gaussian Mixture Model clustering\n",
        "\n",
        "gmm = GaussianMixture(n_components=5, covariance_type=\"full\", random_state=42)\n",
        "clusters = gmm.fit_predict(normalized_embeddings)\n",
        "\n",
        "# Predict cluster labels\n",
        "labels = gmm.predict(normalized_embeddings)"
      ],
      "metadata": {
        "id": "tdM7-3a4o2CC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Group sentences by their assigned topic\n",
        "clustered_sentences = defaultdict(list)\n",
        "for i, label in enumerate(labels):\n",
        "    clustered_sentences[label].append(sentences[i])\n",
        "\n",
        "# Print topics and their sentences\n",
        "for topic, sentences in clustered_sentences.items():\n",
        "    print(f\"\\n🟢 Topic {topic}:\")\n",
        "    for s in sentences:\n",
        "        print(f\"  - {s}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13e4e255-9551-40a6-b51d-a01eec64b56b",
        "id": "gE3PzsH4o2CC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟢 Topic 2:\n",
            "  - Tokenization is a foundational step in text processing where text is divided into smaller units called tokens.\n",
            "  - These tokens can be individual words or sentences, depending on the granularity required.\n",
            "  - For example, tokenizing the sentence “I’m learning NLP!” results in the tokens [\"I\", \"'m\", \"learning\", \"NLP\", \"!\"].\n",
            "  - Tokenization is essential for enabling downstream natural language processing (NLP) tasks, such as sentiment analysis and machine translation, as it breaks down complex text into manageable pieces.\n",
            "  - This step is particularly useful in applications like search engines, where reducing noise can lead to more relevant search results.\n",
            "  - Lemmatization involves converting words to their base or dictionary form, known as the lemma.\n",
            "\n",
            "🟢 Topic 3:\n",
            "  - Libraries like NLTK and spaCy provide efficient tokenization methods, making it easy to prepare text for analysis.\n",
            "  - Tools like WordNetLemmatizer in NLTK and spaCy’s lemmatization functions make it straightforward to incorporate this into preprocessing pipelines.\n",
            "  - This process is vital for tasks like information extraction and organizing large datasets of unstructured text.\n",
            "  - Part-of-Speech (POS) tagging assigns grammatical categories—like nouns, verbs, and adjectives—to each word in a sentence.\n",
            "  - POS tagging is crucial for understanding syntactic structures and is applied in tasks like parsing and machine translation.\n",
            "  - It helps NLP systems comprehend the role of each word in a sentence, leading to better analysis and generation of text.\n",
            "  - Libraries like NLTK and spaCy offer robust POS tagging functionalities with pre-trained models for a variety of languages.\n",
            "\n",
            "🟢 Topic 0:\n",
            "  - Stopword removal is the process of filtering out common words that do not carry substantial meaning, such as “the,” “is,” and “and.” Removing these stopwords reduces the dimensionality of the dataset, which can enhance the performance of NLP models by focusing on more meaningful words.\n",
            "  - For instance, the sentence “She is reading a book” becomes [\"She\", \"reading\", \"book\"] after removing stopwords.\n",
            "  - Each language has its own set of stopwords, and many NLP libraries have built-in lists to streamline this process.\n",
            "  - Unlike stemming, which often chops off word endings indiscriminately, lemmatization considers the context and part of speech of a word.\n",
            "  - For example, “running” and “ran” are both reduced to “run.” This normalization step enhances the accuracy of NLP models by treating different forms of a word equivalently.\n",
            "\n",
            "🟢 Topic 1:\n",
            "  - Named Entity Recognition (NER) is a technique used to identify and categorize entities in text, such as names, locations, dates, and organizations.\n",
            "  - For example, in the sentence “Elon Musk founded Tesla in 2003,” NER would tag “Elon Musk” as a person, “Tesla” as an organization, and “2003” as a date.\n",
            "  - NER can be approached with deep learning models like BERT or traditional NLP tools like spaCy, and pre-trained models are often available to quickly derive insights from text.\n",
            "\n",
            "🟢 Topic 4:\n",
            "  - In “The quick brown fox jumps over the lazy dog,” the tags are (\"The\", DT), (\"quick\", JJ), (\"fox\", NN), (\"jumps\", VBZ), etc.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Get keywords for each topic\n",
        "def get_top_keywords(sentences, num_words=5):\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform(sentences)\n",
        "    feature_array = vectorizer.get_feature_names_out()\n",
        "    tfidf_sorting = X.sum(axis=0).A1.argsort()[::-1]\n",
        "    return [feature_array[i] for i in tfidf_sorting[:num_words]]\n",
        "\n",
        "# Print top words per topic\n",
        "for topic, sentences in clustered_sentences.items():\n",
        "    keywords = get_top_keywords(sentences)\n",
        "    print(f\"\\n🔹 Topic {topic} Keywords: {keywords}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be791071-d120-4bde-91c0-aa7c3d68e630",
        "id": "0uEij7Z2o2CC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Topic 2 Keywords: ['text', 'tokens', 'nlp', 'words', 'learning']\n",
            "\n",
            "🔹 Topic 3 Keywords: ['like', 'text', 'tagging', 'pos', 'spacy']\n",
            "\n",
            "🔹 Topic 0 Keywords: ['word', 'reading', 'book', 'stopwords', 'nlp']\n",
            "\n",
            "🔹 Topic 1 Keywords: ['like', 'models', 'musk', 'tesla', 'elon']\n",
            "\n",
            "🔹 Topic 4 Keywords: ['quick', 'jumps', 'fox', 'vbz', 'tags']\n"
          ]
        }
      ]
    }
  ]
}