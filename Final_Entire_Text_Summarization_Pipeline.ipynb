{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNYmtgyeVyaXdmuQV5v5uJ6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shruthimohan03/video-summarizer/blob/main/Final_Entire_Text_Summarization_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Input Acquisition"
      ],
      "metadata": {
        "id": "g7Xioe6I-GO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " inputs = [[\"https://www.youtube.com/watch?v=tA42nHmmEKw\",\"nptel_python_transcript_preprocessed.txt\",\"nptel_python_summary\"],\n",
        "          [\"https://youtu.be/Ero4yY_ttJE?si=S-6ADdxl4S7EXDkc\",\"federated_learning_transcript.txt\",\"federated_learning_summary.txt\"],\n",
        "          [\"https://youtu.be/xRmBEnI55es?si=jftY1j9b8C9EvqJa\",\"cs_transcript.txt\",\"cs_summary.txt\"],\n",
        "          [\"https://www.youtube.com/watch?v=VyzKDc2GyW4\",\"normalization_transcript.txt\",\"normalization_summary.txt\"],\n",
        "          [\"https://www.youtube.com/watch?v=t45S_MwAcOw\",'transformer_transcript.txt','transformer_summary.txt']]\n",
        "# youtube_url , file_transcript , file_summary"
      ],
      "metadata": {
        "id": "gCRSW97h-JLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Audio Processing Pipeline"
      ],
      "metadata": {
        "id": "d0pAggwN81hS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1. Audio Extraction"
      ],
      "metadata": {
        "id": "gNL_knHH9A7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ScATdinEHkx",
        "outputId": "acf50149-9a58-49a4-f2e6-ba4418c96f9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cz18WH88x4N",
        "outputId": "0ce314fc-2b61-4b98-9b9c-9d947bf162dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2025.3.31-py3-none-any.whl.metadata (172 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/172.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m163.8/172.2 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.2/172.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting SpeechRecognition\n",
            "  Downloading speechrecognition-3.14.2-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from SpeechRecognition) (4.13.1)\n",
            "Downloading yt_dlp-2025.3.31-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading speechrecognition-3.14.2-py3-none-any.whl (32.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, yt-dlp, SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.14.2 pydub-0.25.1 yt-dlp-2025.3.31\n"
          ]
        }
      ],
      "source": [
        "pip install yt-dlp pydub SpeechRecognition"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from yt_dlp import YoutubeDL\n",
        "\n",
        "def download_audio_from_youtube(youtube_url):\n",
        "    options = {\n",
        "        'format': 'bestaudio/best',\n",
        "        'outtmpl': '%(id)s.%(ext)s',  # Use video ID to avoid filename issues\n",
        "        'postprocessors': [{\n",
        "            'key': 'FFmpegExtractAudio',\n",
        "            'preferredcodec': 'wav',\n",
        "            'preferredquality': '192',\n",
        "        }],\n",
        "    }\n",
        "\n",
        "    with YoutubeDL(options) as ydl:\n",
        "        info = ydl.extract_info(youtube_url, download=True)\n",
        "        video_id = info['id']\n",
        "        wav_file = f\"{video_id}.wav\"\n",
        "\n",
        "        if os.path.exists(wav_file):\n",
        "            print(f\"Downloaded and converted audio to WAV: {wav_file}\")\n",
        "            return wav_file\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"WAV file {wav_file} not found.\")"
      ],
      "metadata": {
        "id": "UzVguLOw8zmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2 Audio-to-Text Conversion"
      ],
      "metadata": {
        "id": "js7ZKb2H9hRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6huMQC879kbD",
        "outputId": "885d3ad6-0812-4627-b478-a66ef28e5906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai-whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "045jBKVo9yaj",
        "outputId": "f0dfa317-4bc4-41f8-d076-df08d8f760fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/800.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.6.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803405 sha256=6896f07a45515994587967a431e73f8fa8a72eee46c0a55591f1d9fdeb28bd30\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/f2/ce/6eb23db4091d026238ce76703bd66da60b969d70bcc81d5d3a\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930 tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3lD_hqV90n8",
        "outputId": "7ed7fc64-e905-4c05-fb67-0353b8641461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "def transcribe_audio(audio_file_path):\n",
        "    # Load the Whisper model\n",
        "    model = whisper.load_model(\"small\", device=\"cuda\")\n",
        "\n",
        "    print('Transcribing...')\n",
        "    # Transcribe the audio\n",
        "    result = model.transcribe(audio_file_path)\n",
        "    print('Transcription is ready')\n",
        "    return result[\"text\"]"
      ],
      "metadata": {
        "id": "A-jpAYti93D6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Text Processing Pipeline"
      ],
      "metadata": {
        "id": "bDhUX67P_8t5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1 Punctuation Correction and Preprocessing"
      ],
      "metadata": {
        "id": "tWfuZ5QjAADl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install language-tool-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhF1Hphh_7ub",
        "outputId": "82454f66-6ecb-4317-ab15-087265b62253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting language-tool-python\n",
            "  Downloading language_tool_python-2.9.2-py3-none-any.whl.metadata (54 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from language-tool-python) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from language-tool-python) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from language-tool-python) (5.9.5)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from language-tool-python) (0.10.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->language-tool-python) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->language-tool-python) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->language-tool-python) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->language-tool-python) (2025.1.31)\n",
            "Downloading language_tool_python-2.9.2-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: language-tool-python\n",
            "Successfully installed language-tool-python-2.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import language_tool_python\n",
        "\n",
        "def punctuate_text(text):\n",
        "    tool = language_tool_python.LanguageTool('en-US')\n",
        "\n",
        "    # Only correct punctuation-related errors\n",
        "    punctuation_matches = [\n",
        "        match for match in tool.check(text)\n",
        "        if 'PUNCTUATION' in match.ruleIssueType.upper()\n",
        "    ]\n",
        "    # Apply the corrections\n",
        "    corrected_text = language_tool_python.utils.correct(text, punctuation_matches)\n",
        "    return corrected_text"
      ],
      "metadata": {
        "id": "spylvC4vAvfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EVALUATION OF THE TRANSCRIPT: USING WORD ERROR RATE"
      ],
      "metadata": {
        "id": "p6ZHNs5jA1sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install jiwer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go0PgcXGAxNV",
        "outputId": "db322681-5775-4aeb-f4d6-c07928cefc3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jiwer\n",
            "  Downloading jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.1.8)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading jiwer-3.1.0-py3-none-any.whl (22 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.1.0 rapidfuzz-3.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''def import_txt_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "    return content\n",
        "\n",
        "reference_file_path = \"/kaggle/input/transcripts/reference_nptel_python_transcript.txt\"\n",
        "reference = import_txt_file(reference_file_path)'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "-Xg9LaZCA7iz",
        "outputId": "dadbdda0-d377-4b48-d510-d02b599420a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nreference_file_path = \"/kaggle/input/transcripts/reference_nptel_python_transcript.txt\"\\nreference = import_txt_file(reference_file_path)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''import re\n",
        "import string\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove newlines and extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Strip leading and trailing spaces\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "cleaned_reference = preprocess_text(reference)\n",
        "cleaned_hypothesis = preprocess_text(corrected)'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "18f5VufYA85K",
        "outputId": "ffe2c641-89ed-445b-a1f3-517b28874299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cleaned_reference = preprocess_text(reference)\\ncleaned_hypothesis = preprocess_text(corrected)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''from jiwer import wer\n",
        "\n",
        "error_rate = wer(cleaned_reference, cleaned_hypothesis)\n",
        "print(\"Word Error Rate:\", error_rate)'''"
      ],
      "metadata": {
        "id": "jmqu7s1KBD7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2. Extractive Text Summarization"
      ],
      "metadata": {
        "id": "9pbEgipPBf7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. Sentence Segmentation"
      ],
      "metadata": {
        "id": "CU-flfD1FFKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_segmentation(text):\n",
        "  from nltk.tokenize import sent_tokenize\n",
        "  sentences = sent_tokenize(text)\n",
        "  sentences = [s for s in sentences if len(s.split()) > 3]\n",
        "  return sentences"
      ],
      "metadata": {
        "id": "m2FK9fvaE8JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Sentence Embedding with SBERT"
      ],
      "metadata": {
        "id": "fEw-S9lCFIDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embedding(sentences):\n",
        "  from sentence_transformers import SentenceTransformer\n",
        "  model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "  sentence_embeddings = model.encode(sentences, convert_to_tensor=False)\n",
        "  return sentence_embeddings"
      ],
      "metadata": {
        "id": "C7MVAftEFMl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c. Optimal Cluster Estimation using Bayesian Information Criterion (BIC)"
      ],
      "metadata": {
        "id": "orV0_RUBFPK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Smart Thresholding for range of clusters depending on the number of sentences present in the transcript"
      ],
      "metadata": {
        "id": "j0H1e_98FT0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def get_cluster_range_percent(N, min_ratio=0.05, max_ratio=0.2):\n",
        "    min_c = max(3, math.ceil(N * min_ratio))\n",
        "    max_c = min(N - 1, math.ceil(N * max_ratio))\n",
        "    return (min_c, max_c)"
      ],
      "metadata": {
        "id": "GJldJCGlFRN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Range of clusters to try\n",
        "def get_cluster_range(N):\n",
        "  clusters=get_cluster_range_percent(N)\n",
        "  n_components = np.arange(clusters[0],clusters[1]+1)\n",
        "  return n_components"
      ],
      "metadata": {
        "id": "nJi7nl0dc-oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_BIC(n_components,sentence_embeddings):\n",
        "  from sklearn.mixture import GaussianMixture\n",
        "  bics = []\n",
        "  for n in n_components:\n",
        "      gmm = GaussianMixture(n_components=n, covariance_type='full', random_state=42)\n",
        "      gmm.fit(sentence_embeddings)\n",
        "      bics.append(gmm.bic(sentence_embeddings))\n",
        "  return bics"
      ],
      "metadata": {
        "id": "n2VdMSSGdLRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Plotting BIC values"
      ],
      "metadata": {
        "id": "KNGaH3GGdDhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_BIC(n_components,bics):\n",
        "  plt.plot(n_components, bics, marker='o')\n",
        "  plt.xlabel('Number of clusters')\n",
        "  plt.ylabel('BIC Score')\n",
        "  plt.title('BIC to choose optimal number of clusters')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "AthDMDy4dO8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Finding the optimal number of clusters"
      ],
      "metadata": {
        "id": "CnscK7NTdR-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_optimal_clusters(bics):\n",
        "  optimal_clusters = n_components[np.argmin(bics)]\n",
        "  print(\"Optimal number of clusters:\", optimal_clusters)\n",
        "  return optimal_clusters"
      ],
      "metadata": {
        "id": "9qdwEVLgdVQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "d. Sentence Clustering using Gaussian Mixture Models (GMM) (the optimal no of clusters is used)"
      ],
      "metadata": {
        "id": "p9nWVNQidiMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def best_gmm_model(optimal_clusters,sentence_embeddings):\n",
        "  from sklearn.mixture import GaussianMixture\n",
        "  best_gmm = GaussianMixture(n_components=optimal_clusters, covariance_type='full', random_state=42)\n",
        "  best_gmm.fit(sentence_embeddings)\n",
        "\n",
        "  # Predict cluster labels\n",
        "  labels = best_gmm.predict(sentence_embeddings)\n",
        "  return labels,best_gmm"
      ],
      "metadata": {
        "id": "ZFkNnoYUdlfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "e. Summary Sentence Selection"
      ],
      "metadata": {
        "id": "hY0RY4BpdvI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summary_sentences(labels, optimal_clusters, sentences, sentence_embeddings, best_gmm):\n",
        "  import numpy as np\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  from math import ceil\n",
        "\n",
        "  summary_sentences = []\n",
        "  seen_indices = set()\n",
        "\n",
        "  # You can tune this ratio\n",
        "  summary_ratio = 0.2\n",
        "  min_sentences_per_cluster = 1\n",
        "  max_sentences_per_cluster = 3\n",
        "\n",
        "  for cluster_idx in range(optimal_clusters):\n",
        "      cluster_indices = np.where(labels == cluster_idx)[0]\n",
        "      cluster_size = len(cluster_indices)\n",
        "\n",
        "      # Skip clusters with only one sentence (likely noise)\n",
        "      if cluster_size <= 1:\n",
        "          continue\n",
        "\n",
        "      cluster_embeddings = np.array([sentence_embeddings[i] for i in cluster_indices])\n",
        "      cluster_sentences = [sentences[i] for i in cluster_indices]\n",
        "\n",
        "      # Get centroid of the cluster\n",
        "      centroid = best_gmm.means_[cluster_idx].reshape(1, -1)\n",
        "\n",
        "      # Rank all cluster sentences by similarity to centroid\n",
        "      similarities = cosine_similarity(centroid, cluster_embeddings)[0]\n",
        "      sorted_idx = np.argsort(similarities)[::-1]  # Descending order\n",
        "\n",
        "      # Determine how many sentences to extract from this cluster\n",
        "      num_to_select = min(\n",
        "          max(min_sentences_per_cluster, ceil(summary_ratio * cluster_size)),\n",
        "          max_sentences_per_cluster\n",
        "      )\n",
        "\n",
        "      selected_indices = sorted_idx[:num_to_select]\n",
        "\n",
        "      for idx in selected_indices:\n",
        "          global_idx = cluster_indices[idx]\n",
        "\n",
        "          if global_idx not in seen_indices:\n",
        "              summary_sentences.append((global_idx, sentences[global_idx]))\n",
        "              seen_indices.add(global_idx)\n",
        "\n",
        "  # Sort by original sentence order for coherence\n",
        "  summary_sentences = sorted(summary_sentences, key=lambda x: x[0])\n",
        "\n",
        "  # Final summary\n",
        "  summary = \" \".join([s[1] for s in summary_sentences])\n",
        "\n",
        "  # Print summary\n",
        "  for _, sent in summary_sentences:\n",
        "      print(sent)\n",
        "  return summary"
      ],
      "metadata": {
        "id": "jBfDp1LadvkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RUN THE ENTIRE AUDIO + TEXT PIPELINE"
      ],
      "metadata": {
        "id": "xoeaf29miJ8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for item in inputs:\n",
        "  # 2.1) Audio Extraction\n",
        "  wav_file = download_audio_from_youtube(item[0])\n",
        "\n",
        "  # 2.2) Audio-to-Text Conversion\n",
        "  transcription = transcribe_audio(wav_file)\n",
        "  print(\"\\nTranscription:\\n\")\n",
        "  print(transcription)\n",
        "\n",
        "  # 3.1) Punctuation Correction and Preprocessing\n",
        "  corrected = punctuate_text(transcription)\n",
        "  print(\"Corrected:\", corrected)\n",
        "\n",
        "  # saving the preprocessed transcript\n",
        "  with open(item[1], \"w\") as file:\n",
        "    file.write(corrected)\n",
        "  print(f\"Transcription has been saved to {item[1]}\")\n",
        "\n",
        "  ## EXTRACTIVE SUMMARIZATION:\n",
        "  # a. Sentence Segmentation\n",
        "  sentences=sentence_segmentation(corrected)\n",
        "\n",
        "  # b. Sentence Embedding with SBERT\n",
        "  sentence_embeddings=embedding(sentences)\n",
        "\n",
        "  # c. Optimal Cluster Estimation using Bayesian Information Criterion (BIC)\n",
        "  N = len(sentences)\n",
        "  n_components=get_cluster_range(N)\n",
        "  bics=get_BIC(n_components,sentence_embeddings)\n",
        "  ## plot\n",
        "  #plot_BIC(n_components,bics)\n",
        "  optimal_clusters=find_optimal_clusters(bics)\n",
        "\n",
        "  # d. Sentence Clustering using Gaussian Mixture Models (GMM) (the optimal no of clusters is used)\n",
        "  labels,best_gmm=best_gmm_model(optimal_clusters,sentence_embeddings)\n",
        "  summary=summary_sentences(labels, optimal_clusters, sentences, sentence_embeddings, best_gmm)\n",
        "\n",
        "  # saving the summary\n",
        "  with open(item[2],'w') as file:\n",
        "    file.write(summary)\n",
        "  print('Summary file saved')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RErN8jQDiNEX",
        "outputId": "cd118cec-7d1a-419b-f0c0-ad96596c0f33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=tA42nHmmEKw\n",
            "[youtube] tA42nHmmEKw: Downloading webpage\n",
            "[youtube] tA42nHmmEKw: Downloading tv client config\n",
            "[youtube] tA42nHmmEKw: Downloading player 9599b765-main\n",
            "[youtube] tA42nHmmEKw: Downloading tv player API JSON\n",
            "[youtube] tA42nHmmEKw: Downloading ios player API JSON\n",
            "[youtube] tA42nHmmEKw: Downloading m3u8 information\n",
            "[info] tA42nHmmEKw: Downloading 1 format(s): 251-6\n",
            "[download] Destination: tA42nHmmEKw.webm\n",
            "[download] 100% of   13.18MiB in 00:00:01 at 11.38MiB/s  \n",
            "[ExtractAudio] Destination: tA42nHmmEKw.wav\n",
            "Deleting original file tA42nHmmEKw.webm (pass -k to keep)\n",
            "Downloaded and converted audio to WAV: tA42nHmmEKw.wav\n",
            "Transcribing...\n",
            "Transcription is ready\n",
            "\n",
            "Transcription:\n",
            "\n",
            " Welcome to this course on Python for Data Science. This is a 4-week course where we are going to teach you some very basic programming aspects in Python. And since this is a course that is geared towards data science towards end of the course based on what has been taught in the course we will also show you two different case studies. One is what we call as a function approximation case study, another one a classification case study and then tell you how to solve those case studies using the programming platform that you have learned. So in this first introductory lecture I am just going to talk about why are we looking at Python for data science. So to look at that first we are going to look at what data science is. This is something that you would have seen in other videos of courses in NPTEL and other places. Data science is basically the science of analyzing raw data and deriving insights from this data. And you could use multiple techniques to derive insights, you could use simple statistical techniques to derive insights, you could use more complicated and more sophisticated machine learning techniques to derive insights and so on. Nonetheless the key focus of data science is in actually deriving these insights using whatever techniques that you want to use. Now there is a lot of excitement about data science and this excitement comes because it is been shown that you can get very valuable insights from large data and you can get insights about how different variables change together, how one variable affects another variable and so on with large data which is not very easy to simply see by very simple computation. So you need to invest some time and energy into understanding how you could look at this data and derive these insights from data and from a utilitarian viewpoint if you look at data science in industries, if you do proper data science it allows these industries to make better decisions. These decisions could be in multiple fields for example companies could make better purchasing decisions, better hiring decisions, better decisions in terms of how to operate their processes and so on. So when we talk about decisions, the decisions could be across multiple verticals in an industry and data science is not only useful from an industrial perspective, it is also useful in actual sciences themselves so where you look at lots of data to model your system or test your hypotheses or theories about systems and so on. So when we talk about data science we start by assuming that we have a large amount of data for the problem of interest and we are going to basically look at this data, we are going to inspect the data, we are going to clean and curate the data then we will do some transformation of the data modeling and so on before we can derive insights that are valuable to the organization or to test a theory and so on. Now coming to a more practical viewpoint of what we do once we have data, I have these four bullet points which roughly tell you supposing you were solving a data science problem, what are the steps you will do? So you will start with just having data, someone gives you data and you are trying to derive insights from this data. So the very first step is really to bring this data into your system, so you have to read the data so that the data comes into this programming platform so that you can use this data. Now data could be in multiple formats so you could have data in a simple excel sheet or some other format, so we will teach you how to pull data in to your programming platform from multiple data formats, so that is the first step really if you think about how you are going to solve a problem, the steps would be first to simply read the data and then once you read the data many times you have to do some processing with this data, you could have data that is not correct for example we all know that if you have your mobile numbers there are 10 numbers in a mobile number and if there is a column of mobile numbers and then say there is a one row where there are just five numbers then you know there is something wrong, so this is a very simple check I am talking about in real data processing this gets much more complicated. So once you bring the data in when you try to process this data you are going to get errors such as this, so how do you remove such errors, how do you clean the data is one activity that usually proceeds doing you more useful stuff with the data. This is not the only issue that we look at there could be data that is missing, so for example there is a variable for which you get a value in multiple situations but in some situations the value is missing, so what do you do with this data, do you throw the record away or you do something to fill the data and so on, so these are all data processing cleaning steps, so in this course we will tell you the tools that are available in python so that you can do this data processing cleaning and so on. Now what you have done at this point is you have been able to get the data into the system, you have been able to process and clean the data and get to a certain data file or data structure that is reasonably complete so that you think you can work with this data set at which point what you will do is you will try to summarize this data and usually summarization of this data very simple technique would be very very simple statistical measures that you will compute, you could for example compute a median, mode, mean of a particular column, so those are simple ideas of summarizing the data, you could compute variance and so on, so we are going to teach you how to use this notions of statistical quantities that you can use to summarize the data. Once you summarize the data then another activity which is usually taken up is what is called visualization right, so visualization means you look at this data more pictorially to get insights about the data before you bring in heavy-duty algorithms to bear on this data and this is a creative aspect of data science, the same data could be visualized by multiple people in multiple ways and some visualizations are not only eye-catching but are also much more informative than other types of visualization, so this notion of plotting this data so that some of the attributes or aspects of the data are made apparent is this notion of visualization and there are tools in python that will teach you in terms of how you visualize this data, so at this point you have taken the data, you have cleaned the data, got a set of data points or data structure that you can work with, you have done some basic summary of this data that gives you some insights, you also looked at it more visually and you have got some more insights but when you have large amount of data, big data the last step is really deriving those insights which are not readily apparent either through visualization or through simple summary of data, so how do we then go and look at more sophisticated analytics or analysis of data so that these insights come out and that is where machine learning comes and as a part of this course when you see the progress of this course you will notice that you will go through all of this so that you are ready to look at data science problems in a structured format and then use python as a tool to solve some of these problems. Now why python for doing all of this, the number one reason is that there are these python libraries which already are geared towards doing many of the things that we talked about so that it becomes easy for one to program and very quickly you can get some interesting outcomes out of what we are trying to do, so there are as we talked about in the previous slide you need to do data manipulation and preprocessing, there are lots of functions libraries in python where you can do data wrangling manipulation and so on. From a data summary viewpoint there are many of these statistical calculations that you want to do are already preprogrammed and you have to simply invoke them with your data to be able to show data summary. The next step we talked about visualization there are libraries in python which can be used to do the visualization and finally for the more sophisticated analysis that we talked about all kinds of machine learning algorithms are already pre-coded available as libraries in python. So again once you understand some bit about these functions and once you get comfortable working in python then applying certain machine learning algorithms for these problems become trivial you simply call these libraries and then run these algorithms. At a higher level so in the previous slide we talked about flow process for how I get the data in, clean it and all the way up to insights and then parallely we said why python makes it easy for us to do all of this. If you go back, if you go forward a little more and then ask in terms of the other advantages of python which are little more than just very simple data science activities python provides you several libraries and it is being continuously improved so anytime there is any algorithm those are coming into the set of libraries so in that sense it is very varied and there is also a good user community so if there are some issues with new libraries and so on those are fixed so that you get robust library to work with and we talk about data and data can be of different scale so the examples that you will see in this course are data of reasonably small size but in real life problems you are going to look at data which is much larger which we call as big data. So python has an ability to integrate with big data frameworks like Hadoop, Spark and so on and python also allows you to do more sophisticated programming, object oriented programming and functional programming. Python with all of this sophisticated tools and abilities is still reasonably simple language to learn it is reasonably fast to prototype and it also gives you the ability to work with data which is in your local machine or in a cloud and so on. So these are all things that one looks for when one looks at a programming platform which is capable of solving problems in real life right so these are real problems that you can solve these are not only toy examples but real applications that you can build data science applications that you can build with python and just as another pointer in terms of why I we believe that python is something that lot of our students and professionals in India should learn as you know there are tools which are paid tools for machine learning with all of these libraries and so on and there are also open source tools and in India based on a survey most people of course prefer open source tools for a variety of reasons cause being one because it is free to use but also if it just free to use but it does not have a robust user community then it is not really very useful that is where python really scores in terms of a robust user community which can help with people working in python so it is both open source and there is robust user community both of which are advantages for python and if you think of other competing languages for machine learning if you look at this chart in India about 44% of the people who have surveyed said they use python or they prefer python and of course close second in fact R was much more preferred a few years back but over the last few years in India python is starting to become the programming platform of choice so in that sense it is a good language to learn because the opportunities for jobs and so on are lot more when you are comfortable with python as a language. So with this I will stop this brief introduction on why python for data science I hope I have given you an idea of the fact that while we are going to teach you python as a programming language please keep in mind that each module that we teach in this is actually geared towards data science so as we teach python we will make the connections to how you will use some of the things that you are seeing in data science and all of this will culminate with these two case studies that will bring all of these ideas together in terms of both giving you an idea and an understanding of how a data science problem will be solved and also how it will be solved in python which is a program of choice currently in India. So I hope this short 4 week course helps you quickly get on to this programming platform and then learn data science and then you can enhance your skills with much more detailed understanding of both the programming language and data science techniques. Thank you. you\n",
            "Corrected:  Welcome to this course on Python for Data Science. This is a 4-week course where we are going to teach you some very basic programming aspects in Python. And since this is a course that is geared towards data science towards end of the course based on what has been taught in the course we will also show you two different case studies. One is what we call as a function approximation case study, another one a classification case study and then tell you how to solve those case studies using the programming platform that you have learned. So in this first introductory lecture I am just going to talk about why are we looking at Python for data science. So to look at that first we are going to look at what data science is. This is something that you would have seen in other videos of courses in NPTEL and other places. Data science is basically the science of analyzing raw data and deriving insights from this data. And you could use multiple techniques to derive insights, you could use simple statistical techniques to derive insights, you could use more complicated and more sophisticated machine learning techniques to derive insights and so on. Nonetheless the key focus of data science is in actually deriving these insights using whatever techniques that you want to use. Now there is a lot of excitement about data science and this excitement comes because it is been shown that you can get very valuable insights from large data and you can get insights about how different variables change together, how one variable affects another variable and so on with large data which is not very easy to simply see by very simple computation. So you need to invest some time and energy into understanding how you could look at this data and derive these insights from data and from a utilitarian viewpoint if you look at data science in industries, if you do proper data science it allows these industries to make better decisions. These decisions could be in multiple fields for example companies could make better purchasing decisions, better hiring decisions, better decisions in terms of how to operate their processes and so on. So when we talk about decisions, the decisions could be across multiple verticals in an industry and data science is not only useful from an industrial perspective, it is also useful in actual sciences themselves so where you look at lots of data to model your system or test your hypotheses or theories about systems and so on. So when we talk about data science we start by assuming that we have a large amount of data for the problem of interest and we are going to basically look at this data, we are going to inspect the data, we are going to clean and curate the data then we will do some transformation of the data modeling and so on before we can derive insights that are valuable to the organization or to test a theory and so on. Now coming to a more practical viewpoint of what we do once we have data, I have these four bullet points which roughly tell you supposing you were solving a data science problem, what are the steps you will do? So you will start with just having data, someone gives you data and you are trying to derive insights from this data. So the very first step is really to bring this data into your system, so you have to read the data so that the data comes into this programming platform so that you can use this data. Now data could be in multiple formats so you could have data in a simple excel sheet or some other format, so we will teach you how to pull data in to your programming platform from multiple data formats, so that is the first step really if you think about how you are going to solve a problem, the steps would be first to simply read the data and then once you read the data many times you have to do some processing with this data, you could have data that is not correct for example we all know that if you have your mobile numbers there are 10 numbers in a mobile number and if there is a column of mobile numbers and then say there is a one row where there are just five numbers then you know there is something wrong, so this is a very simple check I am talking about in real data processing this gets much more complicated. So once you bring the data in when you try to process this data you are going to get errors such as this, so how do you remove such errors, how do you clean the data is one activity that usually proceeds doing you more useful stuff with the data. This is not the only issue that we look at there could be data that is missing, so for example there is a variable for which you get a value in multiple situations but in some situations the value is missing, so what do you do with this data, do you throw the record away or you do something to fill the data and so on, so these are all data processing cleaning steps, so in this course we will tell you the tools that are available in python so that you can do this data processing cleaning and so on. Now what you have done at this point is you have been able to get the data into the system, you have been able to process and clean the data and get to a certain data file or data structure that is reasonably complete so that you think you can work with this data set at which point what you will do is you will try to summarize this data and usually summarization of this data very simple technique would be very very simple statistical measures that you will compute, you could for example compute a median, mode, mean of a particular column, so those are simple ideas of summarizing the data, you could compute variance and so on, so we are going to teach you how to use this notions of statistical quantities that you can use to summarize the data. Once you summarize the data then another activity which is usually taken up is what is called visualization right, so visualization means you look at this data more pictorially to get insights about the data before you bring in heavy-duty algorithms to bear on this data and this is a creative aspect of data science, the same data could be visualized by multiple people in multiple ways and some visualizations are not only eye-catching but are also much more informative than other types of visualization, so this notion of plotting this data so that some of the attributes or aspects of the data are made apparent is this notion of visualization and there are tools in python that will teach you in terms of how you visualize this data, so at this point you have taken the data, you have cleaned the data, got a set of data points or data structure that you can work with, you have done some basic summary of this data that gives you some insights, you also looked at it more visually and you have got some more insights but when you have large amount of data, big data the last step is really deriving those insights which are not readily apparent either through visualization or through simple summary of data, so how do we then go and look at more sophisticated analytics or analysis of data so that these insights come out and that is where machine learning comes and as a part of this course when you see the progress of this course you will notice that you will go through all of this so that you are ready to look at data science problems in a structured format and then use python as a tool to solve some of these problems. Now why python for doing all of this, the number one reason is that there are these python libraries which already are geared towards doing many of the things that we talked about so that it becomes easy for one to program and very quickly you can get some interesting outcomes out of what we are trying to do, so there are as we talked about in the previous slide you need to do data manipulation and preprocessing, there are lots of functions libraries in python where you can do data wrangling manipulation and so on. From a data summary viewpoint there are many of these statistical calculations that you want to do are already preprogrammed and you have to simply invoke them with your data to be able to show data summary. The next step we talked about visualization there are libraries in python which can be used to do the visualization and finally for the more sophisticated analysis that we talked about all kinds of machine learning algorithms are already pre-coded available as libraries in python. So again once you understand some bit about these functions and once you get comfortable working in python then applying certain machine learning algorithms for these problems become trivial you simply call these libraries and then run these algorithms. At a higher level so in the previous slide we talked about flow process for how I get the data in, clean it and all the way up to insights and then parallely we said why python makes it easy for us to do all of this. If you go back, if you go forward a little more and then ask in terms of the other advantages of python which are little more than just very simple data science activities python provides you several libraries and it is being continuously improved so anytime there is any algorithm those are coming into the set of libraries so in that sense it is very varied and there is also a good user community so if there are some issues with new libraries and so on those are fixed so that you get robust library to work with and we talk about data and data can be of different scale so the examples that you will see in this course are data of reasonably small size but in real life problems you are going to look at data which is much larger which we call as big data. So python has an ability to integrate with big data frameworks like Hadoop, Spark and so on and python also allows you to do more sophisticated programming, object oriented programming and functional programming. Python with all of this sophisticated tools and abilities is still reasonably simple language to learn it is reasonably fast to prototype and it also gives you the ability to work with data which is in your local machine or in a cloud and so on. So these are all things that one looks for when one looks at a programming platform which is capable of solving problems in real life right so these are real problems that you can solve these are not only toy examples but real applications that you can build data science applications that you can build with python and just as another pointer in terms of why I we believe that python is something that lot of our students and professionals in India should learn as you know there are tools which are paid tools for machine learning with all of these libraries and so on and there are also open source tools and in India based on a survey most people of course prefer open source tools for a variety of reasons cause being one because it is free to use but also if it just free to use but it does not have a robust user community then it is not really very useful that is where python really scores in terms of a robust user community which can help with people working in python so it is both open source and there is robust user community both of which are advantages for python and if you think of other competing languages for machine learning if you look at this chart in India about 44% of the people who have surveyed said they use python or they prefer python and of course close second in fact R was much more preferred a few years back but over the last few years in India python is starting to become the programming platform of choice so in that sense it is a good language to learn because the opportunities for jobs and so on are lot more when you are comfortable with python as a language. So with this I will stop this brief introduction on why python for data science I hope I have given you an idea of the fact that while we are going to teach you python as a programming language please keep in mind that each module that we teach in this is actually geared towards data science so as we teach python we will make the connections to how you will use some of the things that you are seeing in data science and all of this will culminate with these two case studies that will bring all of these ideas together in terms of both giving you an idea and an understanding of how a data science problem will be solved and also how it will be solved in python which is a program of choice currently in India. So I hope this short 4 week course helps you quickly get on to this programming platform and then learn data science and then you can enhance your skills with much more detailed understanding of both the programming language and data science techniques. Thank you. you\n",
            "Transcription has been saved to nptel_python_transcript_preprocessed.txt\n",
            "Optimal number of clusters: 3\n",
            "So in this first introductory lecture I am just going to talk about why are we looking at Python for data science.\n",
            "Data science is basically the science of analyzing raw data and deriving insights from this data.\n",
            "Nonetheless the key focus of data science is in actually deriving these insights using whatever techniques that you want to use.\n",
            "So when we talk about decisions, the decisions could be across multiple verticals in an industry and data science is not only useful from an industrial perspective, it is also useful in actual sciences themselves so where you look at lots of data to model your system or test your hypotheses or theories about systems and so on.\n",
            "So you will start with just having data, someone gives you data and you are trying to derive insights from this data.\n",
            "Now why python for doing all of this, the number one reason is that there are these python libraries which already are geared towards doing many of the things that we talked about so that it becomes easy for one to program and very quickly you can get some interesting outcomes out of what we are trying to do, so there are as we talked about in the previous slide you need to do data manipulation and preprocessing, there are lots of functions libraries in python where you can do data wrangling manipulation and so on.\n",
            "If you go back, if you go forward a little more and then ask in terms of the other advantages of python which are little more than just very simple data science activities python provides you several libraries and it is being continuously improved so anytime there is any algorithm those are coming into the set of libraries so in that sense it is very varied and there is also a good user community so if there are some issues with new libraries and so on those are fixed so that you get robust library to work with and we talk about data and data can be of different scale so the examples that you will see in this course are data of reasonably small size but in real life problems you are going to look at data which is much larger which we call as big data.\n",
            "Summary file saved\n",
            "[youtube] Extracting URL: https://youtu.be/Ero4yY_ttJE?si=S-6ADdxl4S7EXDkc\n",
            "[youtube] Ero4yY_ttJE: Downloading webpage\n",
            "[youtube] Ero4yY_ttJE: Downloading tv client config\n",
            "[youtube] Ero4yY_ttJE: Downloading player 9599b765-main\n",
            "[youtube] Ero4yY_ttJE: Downloading tv player API JSON\n",
            "[youtube] Ero4yY_ttJE: Downloading ios player API JSON\n",
            "[youtube] Ero4yY_ttJE: Downloading m3u8 information\n",
            "[info] Ero4yY_ttJE: Downloading 1 format(s): 251\n",
            "[download] Destination: Ero4yY_ttJE.webm\n",
            "[download] 100% of   10.87MiB in 00:00:00 at 42.45MiB/s  \n",
            "[ExtractAudio] Destination: Ero4yY_ttJE.wav\n",
            "Deleting original file Ero4yY_ttJE.webm (pass -k to keep)\n",
            "Downloaded and converted audio to WAV: Ero4yY_ttJE.wav\n",
            "Transcribing...\n",
            "Transcription is ready\n",
            "\n",
            "Transcription:\n",
            "\n",
            " In this video, we're going to look at federated learning in the most simple manner. Let's say you have a group of friends and each of you has your own notebook. You all want to sort a puzzle together but instead of sharing your notebooks, you only share your ideas. You work on the puzzle in your own notebook then send your updates to a group leader. The leader combines everyone's ideas to make better progress on the puzzle and then shares the updated solution with the group. This is basically how federated learning works. It allows devices like phones or computers to learn from their data without ever sharing it. Let us understand this with a more real-world scenario. Now before we go ahead and look at an example, let's look at the formal definition of federated learning. So it is basically training a decentralized machine learning model. Now what this basically means will be understanding by the end of this video. So let's build up to federated learning. So as I said, we'll be discussing a real-world scenario. So in this case, say you launch an application that requires constant updates to stay up to date with the latest trends and changes. So say for example, this particular application is a self-driving car. Now in order for this app to constantly learn about say the new traffic rules, the new turns or the new diversions, this app will need to be trained on the new updated data and then that updated data needs to find the new learning and then to be sent on the user's device. Right? So now the normal way or the traditional approach of doing this would be through centralized data that is used by the machine learning algorithms. Now what happens here is basically the data is collected from all the users or from all the clients. Basically the millions of devices that are using that particular application will be having some data collected over time. Now all that data is going to be sent to a central database. Right? Now what happens is using this particular central database, a machine learning algorithm is applied and it is used to find the new patterns or the new learnings and further make predictions. Right? So in this case, I have shown a neural network, but it could be any machine learning algorithm. So once the model is trained on this particular central database, it is deployed back to the users. Right? So basically the clients or the users are sending data to the central database and what the central database is doing is it is sending the new update or the new version of the particular app back to the users. Alright? So periodically new data is collected and sent to the server to retrain and then update the model and then the central database sends the updated version back to the users. This happens in a loop. Right? So say for example, we have a smart keyboard. Right? So you guys must have noticed whenever you type a particular sentence or a particular word, in the beginning of the word itself, you get suggestions like PA could be part or it could be paragraph. Right? So these suggestions also work on a similar principle. So this smart keyboard app basically collects your data over time and then sends it to the central server. Right? From all the users, the data that is collected, it is sent to the central server. Then that server trains the model to improve the word predictions or the auto corrections. So this is basically how the machine learning algorithms use centralized data to update the model versions. Right? But apart from the advantages of this particular method, there are some disadvantages as well. The main issue is the security issue or the data privacy concerns. Right? So if you are collecting sensitive data like messages or say health records, risk exposing the user's private information. Now data breaches or unauthorized access can lead to severe consequences. We all know that. Right? So data loss is another disadvantage and it is difficult to handle. Overloading basically the updates that are being sent through the network can cause delays or latency. Right? So that's another disadvantage. And now you guys must suggest that instead of sending the data to a centralized unit, why not train it on the individual devices itself? Right? So again, this method is going to ensure faster updates since there's not going to be any network breakdown involved. And there is also going to be more privacy since there won't be any data sharing. However, again, there's a drawback with this particular method. The data on the individual device will not be sufficient and the changes or the updates will be limited to just the user's experience rather than the actual trends that are occurring out there. Now, what do I mean by that? So say for example, we take our smart keyboard example. Right? Over here what's happening if user one comes across a particular slang. Say we have two users and user one comes across a particular slang say, what's up? And user two has never used this particular slang. Now, if the user two types WAS, there is a huge possibility that user two also gets the suggestion of this particular word in this smart keyboard app. Why is this happening? Because in this particular case, the data is being collected from all the users and then being trained in a centralized database. Right? But if we were to consider individual device updates, in this case, if user one has come across the new slang, what's up and the user two has never come across it, there's a possibility that user two never gets the suggestion for this particular word on his smart keyboard app until and unless he has typed that particular word on his keyboard at some time. Right? So this is the major disadvantage of having individual device updates. Now, these two above concerns or the above problems can be solved through what is known as federated learning, the topic for this particular video. Right? So now you guys must be clear of why exactly we need federated learning. So let's go ahead and understand it in detail. So let me give you guys a quick review on what is exactly federated learning. So federated learning, unlike the traditional approach adopt something known as a decentralized. Decentralized approach. So using this approach, the machine learning algorithms are directly trained on the user's device rather than on the central database. Right? So as you guys can see in this particular image, we have these IoT devices or individual devices over here. And rather than sharing the data itself, the raw data that has been collected by the app, we are sharing an updated model. Now, what is this updated model? This is basically the locally trained model on the devices itself. So what's happening here is every time, or you can say periodically, the data, the new data that has been collected by the individual devices, it has been trained locally. And then the updated model that has been trained by the devices is being sent to the central server. Right? Now the central server collects all these updated models from all the devices rather than the raw data. Right? So over here, the privacy concerns or the data privacy concerns have been solved directly. Right? So we are not sharing the data directly, but we're sharing the updated model version that has been trained on the devices itself with the central server. Now this central server basically acts as a leader, which we discussed in our example initially. Now this leader is going to combine or collaborate all these updated models and come up with a global model. That is come up with one particular solution and send that solution or share that solution with all the users or all the devices that are available. Right? So let us look at it quickly once again. So each device is going to download the current global model that the central server sends and it is going to train it locally using the private data. Right? So no sharing of data is happening in federated learning. Now once the local training is complete, the device is going to generate an update that summarizes the patterns it learned. Right? So it is not sharing the data. I'm repeating once again. It is only sharing the patterns or the new learnings that has been found by the device. It could be any learning. So say for example, this particular model gets to know that usually thank is followed by you. So this particular finding is going to be shared with the central server. Right? So this update is going to be a set of model parameters, not the actual user data. Right? So these updates are going to be encrypted. This particular update or this particular finding is going to be sent in an encrypted mode to the central server. So this technique is also known as secure aggregation. Now what the secure aggregation does is basically secure aggregation. Now what the secure aggregation does is basically it ensures that individual updates cannot be traced back to a specific user. Right? So all the risk of data privacy or privacy concerns have been solved through federated learning as we just saw in this particular image. Now in the end, the central server is going to combine all the updates from the millions of devices to improve the global model. Right? And this process is going to use algorithms like federated averaging. Right? So the improved global model is going to be sent back to all the devices making the app smarter without ever sharing any private data. So over here, there's another image for your understanding. So this is the particular example. Thank you for the person has only typed F and he's getting the suggestion of feedback, which is appropriate in this case. So all the local data is being trained locally and the updates are being sent in an encrypted format to the central server. The central server combines all these local updates and sends a new global model to all the users. Right? So this is basically how federated learning works in short. So these are some of the applications of federated learning. So one is self-driving car, one is digital health care, interest based, advertising, industry, personalizing, smartphone, bank fraud detection and many more. Right? So this is only the beginning. What have we learned? Our major learning is that we can learn from everyone without learning about anyone. So this is basically how you can summarize federated learning. All right. So I hope you guys were able to get some insights out of this video and I hope you guys got a hang of federated learning. If you guys liked the video, do hit the like button and thank you and see you in the next video. Thank you.\n",
            "Corrected:  In this video, we're going to look at federated learning in the most simple manner. Let's say you have a group of friends and each of you has your own notebook. You all want to sort a puzzle together but instead of sharing your notebooks, you only share your ideas. You work on the puzzle in your own notebook then send your updates to a group leader. The leader combines everyone's ideas to make better progress on the puzzle and then shares the updated solution with the group. This is basically how federated learning works. It allows devices like phones or computers to learn from their data without ever sharing it. Let us understand this with a more real-world scenario. Now before we go ahead and look at an example, let's look at the formal definition of federated learning. So it is basically training a decentralized machine learning model. Now what this basically means will be understanding by the end of this video. So let's build up to federated learning. So as I said, we'll be discussing a real-world scenario. So in this case, say you launch an application that requires constant updates to stay up to date with the latest trends and changes. So say for example, this particular application is a self-driving car. Now in order for this app to constantly learn about say the new traffic rules, the new turns or the new diversions, this app will need to be trained on the new updated data and then that updated data needs to find the new learning and then to be sent on the user's device. Right? So now the normal way or the traditional approach of doing this would be through centralized data that is used by the machine learning algorithms. Now what happens here is basically the data is collected from all the users or from all the clients. Basically the millions of devices that are using that particular application will be having some data collected over time. Now all that data is going to be sent to a central database. Right? Now what happens is using this particular central database, a machine learning algorithm is applied and it is used to find the new patterns or the new learnings and further make predictions. Right? So in this case, I have shown a neural network, but it could be any machine learning algorithm. So once the model is trained on this particular central database, it is deployed back to the users. Right? So basically the clients or the users are sending data to the central database and what the central database is doing is it is sending the new update or the new version of the particular app back to the users. Alright? So periodically new data is collected and sent to the server to retrain and then update the model and then the central database sends the updated version back to the users. This happens in a loop. Right? So say for example, we have a smart keyboard. Right? So you guys must have noticed whenever you type a particular sentence or a particular word, in the beginning of the word itself, you get suggestions like PA could be part or it could be paragraph. Right? So these suggestions also work on a similar principle. So this smart keyboard app basically collects your data over time and then sends it to the central server. Right? From all the users, the data that is collected, it is sent to the central server. Then that server trains the model to improve the word predictions or the auto corrections. So this is basically how the machine learning algorithms use centralized data to update the model versions. Right? But apart from the advantages of this particular method, there are some disadvantages as well. The main issue is the security issue or the data privacy concerns. Right? So if you are collecting sensitive data like messages or say health records, risk exposing the user's private information. Now data breaches or unauthorized access can lead to severe consequences. We all know that. Right? So data loss is another disadvantage and it is difficult to handle. Overloading basically the updates that are being sent through the network can cause delays or latency. Right? So that's another disadvantage. And now you guys must suggest that instead of sending the data to a centralized unit, why not train it on the individual devices itself? Right? So again, this method is going to ensure faster updates since there's not going to be any network breakdown involved. And there is also going to be more privacy since there won't be any data sharing. However, again, there's a drawback with this particular method. The data on the individual device will not be sufficient and the changes or the updates will be limited to just the user's experience rather than the actual trends that are occurring out there. Now, what do I mean by that? So say for example, we take our smart keyboard example. Right? Over here what's happening if user one comes across a particular slang. Say we have two users and user one comes across a particular slang say, what's up? And user two has never used this particular slang. Now, if the user two types WAS, there is a huge possibility that user two also gets the suggestion of this particular word in this smart keyboard app. Why is this happening? Because in this particular case, the data is being collected from all the users and then being trained in a centralized database. Right? But if we were to consider individual device updates, in this case, if user one has come across the new slang, what's up and the user two has never come across it, there's a possibility that user two never gets the suggestion for this particular word on his smart keyboard app until and unless he has typed that particular word on his keyboard at some time. Right? So this is the major disadvantage of having individual device updates. Now, these two above concerns or the above problems can be solved through what is known as federated learning, the topic for this particular video. Right? So now you guys must be clear of why exactly we need federated learning. So let's go ahead and understand it in detail. So let me give you guys a quick review on what is exactly federated learning. So federated learning, unlike the traditional approach adopt something known as a decentralized. Decentralized approach. So using this approach, the machine learning algorithms are directly trained on the user's device rather than on the central database. Right? So as you guys can see in this particular image, we have these IoT devices or individual devices over here. And rather than sharing the data itself, the raw data that has been collected by the app, we are sharing an updated model. Now, what is this updated model? This is basically the locally trained model on the devices itself. So what's happening here is every time, or you can say periodically, the data, the new data that has been collected by the individual devices, it has been trained locally. And then the updated model that has been trained by the devices is being sent to the central server. Right? Now the central server collects all these updated models from all the devices rather than the raw data. Right? So over here, the privacy concerns or the data privacy concerns have been solved directly. Right? So we are not sharing the data directly, but we're sharing the updated model version that has been trained on the devices itself with the central server. Now this central server basically acts as a leader, which we discussed in our example initially. Now this leader is going to combine or collaborate all these updated models and come up with a global model. That is come up with one particular solution and send that solution or share that solution with all the users or all the devices that are available. Right? So let us look at it quickly once again. So each device is going to download the current global model that the central server sends and it is going to train it locally using the private data. Right? So no sharing of data is happening in federated learning. Now once the local training is complete, the device is going to generate an update that summarizes the patterns it learned. Right? So it is not sharing the data. I'm repeating once again. It is only sharing the patterns or the new learnings that has been found by the device. It could be any learning. So say for example, this particular model gets to know that usually thank is followed by you. So this particular finding is going to be shared with the central server. Right? So this update is going to be a set of model parameters, not the actual user data. Right? So these updates are going to be encrypted. This particular update or this particular finding is going to be sent in an encrypted mode to the central server. So this technique is also known as secure aggregation. Now what the secure aggregation does is basically secure aggregation. Now what the secure aggregation does is basically it ensures that individual updates cannot be traced back to a specific user. Right? So all the risk of data privacy or privacy concerns have been solved through federated learning as we just saw in this particular image. Now in the end, the central server is going to combine all the updates from the millions of devices to improve the global model. Right? And this process is going to use algorithms like federated averaging. Right? So the improved global model is going to be sent back to all the devices making the app smarter without ever sharing any private data. So over here, there's another image for your understanding. So this is the particular example. Thank you for the person has only typed F and he's getting the suggestion of feedback, which is appropriate in this case. So all the local data is being trained locally and the updates are being sent in an encrypted format to the central server. The central server combines all these local updates and sends a new global model to all the users. Right? So this is basically how federated learning works in short. So these are some of the applications of federated learning. So one is self-driving car, one is digital health care, interest based, advertising, industry, personalizing, smartphone, bank fraud detection and many more. Right? So this is only the beginning. What have we learned? Our major learning is that we can learn from everyone without learning about anyone. So this is basically how you can summarize federated learning. All right. So I hope you guys were able to get some insights out of this video and I hope you guys got a hang of federated learning. If you guys liked the video, do hit the like button and thank you and see you in the next video. Thank you.\n",
            "Transcription has been saved to federated_learning_transcript.txt\n",
            "Optimal number of clusters: 6\n",
            " In this video, we're going to look at federated learning in the most simple manner.\n",
            "This is basically how federated learning works.\n",
            "It allows devices like phones or computers to learn from their data without ever sharing it.\n",
            "Let us understand this with a more real-world scenario.\n",
            "Now all that data is going to be sent to a central database.\n",
            "So basically the clients or the users are sending data to the central database and what the central database is doing is it is sending the new update or the new version of the particular app back to the users.\n",
            "So periodically new data is collected and sent to the server to retrain and then update the model and then the central database sends the updated version back to the users.\n",
            "Say we have two users and user one comes across a particular slang say, what's up?\n",
            "Now, if the user two types WAS, there is a huge possibility that user two also gets the suggestion of this particular word in this smart keyboard app.\n",
            "But if we were to consider individual device updates, in this case, if user one has come across the new slang, what's up and the user two has never come across it, there's a possibility that user two never gets the suggestion for this particular word on his smart keyboard app until and unless he has typed that particular word on his keyboard at some time.\n",
            "So let's go ahead and understand it in detail.\n",
            "This is basically the locally trained model on the devices itself.\n",
            "So what's happening here is every time, or you can say periodically, the data, the new data that has been collected by the individual devices, it has been trained locally.\n",
            "Now the central server collects all these updated models from all the devices rather than the raw data.\n",
            "So we are not sharing the data directly, but we're sharing the updated model version that has been trained on the devices itself with the central server.\n",
            "So the improved global model is going to be sent back to all the devices making the app smarter without ever sharing any private data.\n",
            "So this is the particular example.\n",
            "So this is basically how federated learning works in short.\n",
            "Summary file saved\n",
            "[youtube] Extracting URL: https://youtu.be/xRmBEnI55es?si=jftY1j9b8C9EvqJa\n",
            "[youtube] xRmBEnI55es: Downloading webpage\n",
            "[youtube] xRmBEnI55es: Downloading tv client config\n",
            "[youtube] xRmBEnI55es: Downloading player 9599b765-main\n",
            "[youtube] xRmBEnI55es: Downloading tv player API JSON\n",
            "[youtube] xRmBEnI55es: Downloading ios player API JSON\n",
            "[youtube] xRmBEnI55es: Downloading m3u8 information\n",
            "[info] xRmBEnI55es: Downloading 1 format(s): 251\n",
            "[download] Destination: xRmBEnI55es.webm\n",
            "[download] 100% of    7.93MiB in 00:00:00 at 48.51MiB/s  \n",
            "[ExtractAudio] Destination: xRmBEnI55es.wav\n",
            "Deleting original file xRmBEnI55es.webm (pass -k to keep)\n",
            "Downloaded and converted audio to WAV: xRmBEnI55es.wav\n",
            "Transcribing...\n",
            "Transcription is ready\n",
            "\n",
            "Transcription:\n",
            "\n",
            " So for the last portion of this first lecture module, I'd like to come back to a question that I asked earlier. How do we build intelligent machines? If we really want to think about this problem kind of as engineers, if we forget everything that we know about learning, reinforcement learning and so on, and you have to build an intelligent machine, where would you start? Maybe a very logical place to start is to think about all of the things that the brain has to do. If you're a human brain, think of them as independent modules and try to program in modules that do that. Breaking up the brain into individual parts and thinking about their function is a very old study that goes back before even the modern scientific era. Of course, today's understanding is a little more sophisticated than it was in the 19th century, but there's still a lot of work trying to anatomically chop up the brain into parts with different functions. So then it's very tempting as an engineer to think about building individual computer components to reproduce those functions. But this quickly becomes very difficult because there are a lot of these parts. Implementing each of them is very hard, and implementing all of them might require an enormous amount of work. What if we instead consider whether learning could be seen as the basis of intelligence? I'm going to make an argument for why this might be true. This is not by any means a widely accepted or universally accepted opinion, but perhaps we can just entertain that notion and see where it leads us. One way we could argue for this notion is to say, well, there are some things that we can all do, like walking, but there are some other things that clearly we have to learn because there's no way that humans could, through evolution, be prepared for things like driving a car. Cars weren't around when we evolved. So the argument here would be that we can learn a huge variety of things. So that second category is some things we can only learn. It's a very large category. There's a huge variety of things we can learn, including extremely difficult things. And while there might be some things that we could argue are innate, the range of things we can learn is so broad that perhaps it basically captures everything that we care about. And those things that are innate, we could have learned them also if they weren't innate to the begin with. So therefore, our learning mechanisms are likely powerful enough to do basically everything that we associate with intelligence. Maybe you agree or disagree with this notion, but if you humor this notion for a second, we could think about how we could use it to refine our plan for building intelligence. Now, we could simply take this recipe and use it to, instead of designing the functionality of each module, instead of designing a learning algorithm for each module. So we say, well, okay, we're not going to implement the visual cortex or the motor cortex by hand. What we'll instead implement is a learning algorithm for the visual cortex and a separate algorithm for the motor cortex. This kind of thing was a dominant way of thinking about machine learning in the 90s and early 2000s. But what if we hypothesize that perhaps there's a single flexible algorithm that can do all of these things? Perhaps not only is learning the basis of intelligence, but in fact, learning with a single powerful algorithm as the basis of intelligence. It's a very provocative notion, but it's also a very appealing one because it suggests that we could save ourselves a lot of work. Instead of designing a separate algorithm for each module, we simply design one algorithm that is broad enough and flexible enough to acquire all these capabilities. And there is a bit of evidence, a bit of circumstantial evidence, to suggest that something like this might in fact be close to what's going on in the real brain. These pieces of evidence, they all have the general flavor of illustrating some degree of flexibility that is unusual or unexpected. For instance, you could acquire a degree of visual acuity by using your tongue. You could take a camera with electrodes attached to it and place those electrodes on your tongue and then close your eyes. Or if you're blind, try to use your tongue to see. And then form some test of visual acuity and you will in fact with a fair bit of practice acquire a degree of visual acuity at your tongue. A more extreme experiment, which is sometimes referred to as the ferret rewiring experiment, was performing animals on ferrets where the ferrets optic nerve was disconnected from the visual cortex and reconnected to the auditory cortex surgically when the ferret was very, very young. When the ferret grew up and over the course of its development, it actually recovered a degree of visual acuity, which means that its auditory cortex was essentially learning how to see. So, if different sensory cortices can be repurposed to perform each other's jobs, perhaps in some sense they're all implementing the same flexible algorithm. We could take this idea even further and hypothesize that not only sensory cortices, but a lot of the functionality of the brain could in fact be performed by a single flexible algorithm. We don't know if this is true, but it's a very appealing notion. If in fact this is true, we could ask, well, what kind of algorithm could it be? What must a single algorithm be able to do? It has to be able to interpret rich sensory inputs. It has to deal with complex, rich, open world problems. And it has to choose complex actions, which means that it has to reason about decision-making and control. Where have we seen that before? Well, these are the two parts of deep reinforcement learning. The deep deals with handling complex open world inputs and the reinforcement learning provides the formalism for decision-making and control. And, in fact, there's a little, again, some circumstantial evidence that both deep learning and reinforcement learning, at least individually, provide some sensible model of how the brain processes information. So, this is an older paper that's about a decade old at this point called Unsupervised Learning Models of Primary Cortical Receptive Fields and Receptive Field Tasticity that tries to analyze the kind of features that are known to exist in the brain and compares them to the kind of features that are observed in primate sensory corpses. For example, they take simple stimuli like these kind of grading-type stimuli, which are known to stimulate individual receptive fields, individual cortex, and they analyze the kind of features that are learned from these stimuli by deep neural networks. And then they compare the statistics of those features to features that are known to exist in the primate visual cortex from Experiments on Monkeys. They do a similar experiment on auditory features, so they expose a deep neural network to a range of auditory stimuli, look at the statistics of the features that emerge, and, again, compare those to the statistics of features in the brain. And they even have kind of a funny experiment on the sense of touch, where they take human subjects, they have them manipulate an object with a glove that's been dusted with some white dust, and they use where the dust was deposited on the glove to train a deep neural network to essentially represent touch-sensing features. And they compare them to features that are known to exist from Experiments on Monkeys, where a monkey's hand is placed on a drum with indentations, which rotates and then the monkey's sense of touch is recorded from neurons in its brain. And again, they compare the statistics of these features and find that the neural network learned features with similar statistics. Now, there are a few conclusions we might draw from these experiments. For example, we might conclude that the deep neural network works the same way that the brain works. But I think there's actually a more simple explanation. It's probably not about the deep neural network per se. It's probably about the observation that any large, heavily over-parameterized model will discover features with these statistics because they're just the right features for this data. So the features, in some sense, are perhaps a property of the data itself, and a powerful enough model, regardless of its internal design, would acquire those features because they're the right ones. There's also quite a bit of evidence in favor of reinforcement learning as a model of how the brain learns. And in fact, reinforcement learning was studied in psychology and neuroscience long before it was a field of study in computer science. So percepts that anticipate reward become associated with similar firing patterns as the reward itself is a known observation, sometimes referred to as spiked endopenoplasticity. The basal ganglia appears to be related to the reward system in the brain. And model-free RL-like adaptation is often a good fit for experimental data of animal adaptation, but not always. All right. So we might conclude from this, I for some inclined, that if in fact there is a single flexible algorithm that can acquire the broad range of behaviors that we associate with human intelligence, that algorithm perhaps ought to look a little bit like a reinforcement learning algorithm and perhaps equipped with a large capacity of representations like deep models. So then we could say, well, what can deep learning in RL do well now? Basically, how close are we to that and what seems to be missing? Well, current deep reinforcement learning algorithms are pretty good at some things. They're good at acquiring a high degree of proficiency and domains governed by simple known rules like board games and video games. They're good at learning simple skills with raw sensory input, given enough experience. And they're pretty good at learning to imitate given enough human-provided expert behavior. But they still fall short in a few very important ways compared to human intelligence or even animal intelligence. Humans can learn incredibly quickly. Deep reinforcement learning algorithms are not usually known for their efficiency. They usually require a very large amount of experience. And it may be because humans are very good at reusing past knowledge. So humans can adapt quickly. For instance, this is a commonly done motor control experiment where a person moves a sliver, a perturbation is introduced requiring the person to react, and then the experiment measures how many trials a human needs to learn how to overcome any perturbation introduced by the sliver. Humans learn this very quickly in just a couple of trials. But chances are humans aren't learning entirely from scratch. They have past experience with physical manipulation, moving their bodies, and with responding to perturbation forces. Transfer learning in deep RL is still an open problem, but its goal is to essentially address this capability. It's also often not clear in reality what the reward function should be. So in classical reinforcement learning, we typically assume the reward function is known and it's correct, but in the real world, it's far less clear. It's also not clear what the role of prediction should be. So should we learn by modeling the entire world and then planning through that model? Or should we learn directly from trial and error? Or should we do a little bit of both? But to sum all this up, one of the things that deep reinforcement learning could potentially offer us is a way to think about the acquisition of intelligence in a more unified algorithmic way without having to think about designing individual algorithms or individual modules. But this is not by any means a new idea. Going from this kind of modular, very complex model to a simple learning-driven model is something that is in some sense as old as computer science itself. Here's a quote that I like very much on this topic. Instead of trying to produce a program to simulate the adult mind, why not rather try to produce one which simulates the child's? If this were then subjected to an appropriate course of education, one would obtain the adult brain. Who wrote this? Don't try.\n",
            "Corrected:  So for the last portion of this first lecture module, I'd like to come back to a question that I asked earlier. How do we build intelligent machines? If we really want to think about this problem kind of as engineers, if we forget everything that we know about learning, reinforcement learning and so on, and you have to build an intelligent machine, where would you start? Maybe a very logical place to start is to think about all of the things that the brain has to do. If you're a human brain, think of them as independent modules and try to program in modules that do that. Breaking up the brain into individual parts and thinking about their function is a very old study that goes back before even the modern scientific era. Of course, today's understanding is a little more sophisticated than it was in the 19th century, but there's still a lot of work trying to anatomically chop up the brain into parts with different functions. So then it's very tempting as an engineer to think about building individual computer components to reproduce those functions. But this quickly becomes very difficult because there are a lot of these parts. Implementing each of them is very hard, and implementing all of them might require an enormous amount of work. What if we instead consider whether learning could be seen as the basis of intelligence? I'm going to make an argument for why this might be true. This is not by any means a widely accepted or universally accepted opinion, but perhaps we can just entertain that notion and see where it leads us. One way we could argue for this notion is to say, well, there are some things that we can all do, like walking, but there are some other things that clearly we have to learn because there's no way that humans could, through evolution, be prepared for things like driving a car. Cars weren't around when we evolved. So the argument here would be that we can learn a huge variety of things. So that second category is some things we can only learn. It's a very large category. There's a huge variety of things we can learn, including extremely difficult things. And while there might be some things that we could argue are innate, the range of things we can learn is so broad that perhaps it basically captures everything that we care about. And those things that are innate, we could have learned them also if they weren't innate to the begin with. So therefore, our learning mechanisms are likely powerful enough to do basically everything that we associate with intelligence. Maybe you agree or disagree with this notion, but if you humor this notion for a second, we could think about how we could use it to refine our plan for building intelligence. Now, we could simply take this recipe and use it to, instead of designing the functionality of each module, instead of designing a learning algorithm for each module. So we say, well, okay, we're not going to implement the visual cortex or the motor cortex by hand. What we'll instead implement is a learning algorithm for the visual cortex and a separate algorithm for the motor cortex. This kind of thing was a dominant way of thinking about machine learning in the 90s and early 2000s. But what if we hypothesize that perhaps there's a single flexible algorithm that can do all of these things? Perhaps not only is learning the basis of intelligence, but in fact, learning with a single powerful algorithm as the basis of intelligence. It's a very provocative notion, but it's also a very appealing one because it suggests that we could save ourselves a lot of work. Instead of designing a separate algorithm for each module, we simply design one algorithm that is broad enough and flexible enough to acquire all these capabilities. And there is a bit of evidence, a bit of circumstantial evidence, to suggest that something like this might in fact be close to what's going on in the real brain. These pieces of evidence, they all have the general flavor of illustrating some degree of flexibility that is unusual or unexpected. For instance, you could acquire a degree of visual acuity by using your tongue. You could take a camera with electrodes attached to it and place those electrodes on your tongue and then close your eyes. Or if you're blind, try to use your tongue to see. And then form some test of visual acuity and you will in fact with a fair bit of practice acquire a degree of visual acuity at your tongue. A more extreme experiment, which is sometimes referred to as the ferret rewiring experiment, was performing animals on ferrets where the ferrets optic nerve was disconnected from the visual cortex and reconnected to the auditory cortex surgically when the ferret was very, very young. When the ferret grew up and over the course of its development, it actually recovered a degree of visual acuity, which means that its auditory cortex was essentially learning how to see. So, if different sensory cortices can be repurposed to perform each other's jobs, perhaps in some sense they're all implementing the same flexible algorithm. We could take this idea even further and hypothesize that not only sensory cortices, but a lot of the functionality of the brain could in fact be performed by a single flexible algorithm. We don't know if this is true, but it's a very appealing notion. If in fact this is true, we could ask, well, what kind of algorithm could it be? What must a single algorithm be able to do? It has to be able to interpret rich sensory inputs. It has to deal with complex, rich, open world problems. And it has to choose complex actions, which means that it has to reason about decision-making and control. Where have we seen that before? Well, these are the two parts of deep reinforcement learning. The deep deals with handling complex open world inputs and the reinforcement learning provides the formalism for decision-making and control. And, in fact, there's a little, again, some circumstantial evidence that both deep learning and reinforcement learning, at least individually, provide some sensible model of how the brain processes information. So, this is an older paper that's about a decade old at this point called Unsupervised Learning Models of Primary Cortical Receptive Fields and Receptive Field Tasticity that tries to analyze the kind of features that are known to exist in the brain and compares them to the kind of features that are observed in primate sensory corpses. For example, they take simple stimuli like these kind of grading-type stimuli, which are known to stimulate individual receptive fields, individual cortex, and they analyze the kind of features that are learned from these stimuli by deep neural networks. And then they compare the statistics of those features to features that are known to exist in the primate visual cortex from Experiments on Monkeys. They do a similar experiment on auditory features, so they expose a deep neural network to a range of auditory stimuli, look at the statistics of the features that emerge, and, again, compare those to the statistics of features in the brain. And they even have kind of a funny experiment on the sense of touch, where they take human subjects, they have them manipulate an object with a glove that's been dusted with some white dust, and they use where the dust was deposited on the glove to train a deep neural network to essentially represent touch-sensing features. And they compare them to features that are known to exist from Experiments on Monkeys, where a monkey's hand is placed on a drum with indentations, which rotates and then the monkey's sense of touch is recorded from neurons in its brain. And again, they compare the statistics of these features and find that the neural network learned features with similar statistics. Now, there are a few conclusions we might draw from these experiments. For example, we might conclude that the deep neural network works the same way that the brain works. But I think there's actually a more simple explanation. It's probably not about the deep neural network per se. It's probably about the observation that any large, heavily over-parameterized model will discover features with these statistics because they're just the right features for this data. So the features, in some sense, are perhaps a property of the data itself, and a powerful enough model, regardless of its internal design, would acquire those features because they're the right ones. There's also quite a bit of evidence in favor of reinforcement learning as a model of how the brain learns. And in fact, reinforcement learning was studied in psychology and neuroscience long before it was a field of study in computer science. So percepts that anticipate reward become associated with similar firing patterns as the reward itself is a known observation, sometimes referred to as spiked endopenoplasticity. The basal ganglia appears to be related to the reward system in the brain. And model-free RL-like adaptation is often a good fit for experimental data of animal adaptation, but not always. All right. So we might conclude from this, I for some inclined, that if in fact there is a single flexible algorithm that can acquire the broad range of behaviors that we associate with human intelligence, that algorithm perhaps ought to look a little bit like a reinforcement learning algorithm and perhaps equipped with a large capacity of representations like deep models. So then we could say, well, what can deep learning in RL do well now? Basically, how close are we to that and what seems to be missing? Well, current deep reinforcement learning algorithms are pretty good at some things. They're good at acquiring a high degree of proficiency and domains governed by simple known rules like board games and video games. They're good at learning simple skills with raw sensory input, given enough experience. And they're pretty good at learning to imitate given enough human-provided expert behavior. But they still fall short in a few very important ways compared to human intelligence or even animal intelligence. Humans can learn incredibly quickly. Deep reinforcement learning algorithms are not usually known for their efficiency. They usually require a very large amount of experience. And it may be because humans are very good at reusing past knowledge. So humans can adapt quickly. For instance, this is a commonly done motor control experiment where a person moves a sliver, a perturbation is introduced requiring the person to react, and then the experiment measures how many trials a human needs to learn how to overcome any perturbation introduced by the sliver. Humans learn this very quickly in just a couple of trials. But chances are humans aren't learning entirely from scratch. They have past experience with physical manipulation, moving their bodies, and with responding to perturbation forces. Transfer learning in deep RL is still an open problem, but its goal is to essentially address this capability. It's also often not clear in reality what the reward function should be. So in classical reinforcement learning, we typically assume the reward function is known and it's correct, but in the real world, it's far less clear. It's also not clear what the role of prediction should be. So should we learn by modeling the entire world and then planning through that model? Or should we learn directly from trial and error? Or should we do a little bit of both? But to sum all this up, one of the things that deep reinforcement learning could potentially offer us is a way to think about the acquisition of intelligence in a more unified algorithmic way without having to think about designing individual algorithms or individual modules. But this is not by any means a new idea. Going from this kind of modular, very complex model to a simple learning-driven model is something that is in some sense as old as computer science itself. Here's a quote that I like very much on this topic. Instead of trying to produce a program to simulate the adult mind, why not rather try to produce one which simulates the child's? If this were then subjected to an appropriate course of education, one would obtain the adult brain. Who wrote this? Don't try.\n",
            "Transcription has been saved to cs_transcript.txt\n",
            "Optimal number of clusters: 5\n",
            "This is not by any means a widely accepted or universally accepted opinion, but perhaps we can just entertain that notion and see where it leads us.\n",
            "So the argument here would be that we can learn a huge variety of things.\n",
            "So therefore, our learning mechanisms are likely powerful enough to do basically everything that we associate with intelligence.\n",
            "For instance, you could acquire a degree of visual acuity by using your tongue.\n",
            "We could take this idea even further and hypothesize that not only sensory cortices, but a lot of the functionality of the brain could in fact be performed by a single flexible algorithm.\n",
            "We don't know if this is true, but it's a very appealing notion.\n",
            "It has to be able to interpret rich sensory inputs.\n",
            "There's also quite a bit of evidence in favor of reinforcement learning as a model of how the brain learns.\n",
            "So then we could say, well, what can deep learning in RL do well now?\n",
            "Basically, how close are we to that and what seems to be missing?\n",
            "Well, current deep reinforcement learning algorithms are pretty good at some things.\n",
            "Humans can learn incredibly quickly.\n",
            "Humans learn this very quickly in just a couple of trials.\n",
            "But chances are humans aren't learning entirely from scratch.\n",
            "But to sum all this up, one of the things that deep reinforcement learning could potentially offer us is a way to think about the acquisition of intelligence in a more unified algorithmic way without having to think about designing individual algorithms or individual modules.\n",
            "Summary file saved\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=VyzKDc2GyW4\n",
            "[youtube] VyzKDc2GyW4: Downloading webpage\n",
            "[youtube] VyzKDc2GyW4: Downloading tv client config\n",
            "[youtube] VyzKDc2GyW4: Downloading player 9599b765-main\n",
            "[youtube] VyzKDc2GyW4: Downloading tv player API JSON\n",
            "[youtube] VyzKDc2GyW4: Downloading ios player API JSON\n",
            "[youtube] VyzKDc2GyW4: Downloading m3u8 information\n",
            "[info] VyzKDc2GyW4: Downloading 1 format(s): 251\n",
            "[download] Destination: VyzKDc2GyW4.webm\n",
            "[download] 100% of   15.91MiB in 00:00:00 at 47.32MiB/s  \n",
            "[ExtractAudio] Destination: VyzKDc2GyW4.wav\n",
            "Deleting original file VyzKDc2GyW4.webm (pass -k to keep)\n",
            "Downloaded and converted audio to WAV: VyzKDc2GyW4.wav\n",
            "Transcribing...\n",
            "Transcription is ready\n",
            "\n",
            "Transcription:\n",
            "\n",
            " Hi and welcome to week four. I'm Kathy Wilson and today we're going to go over some normalization examples. Normalization is a very important foundational concept in database design. And so I wanted to provide just a little bit of extra instruction in this area that's just in addition to what you can find in your book readings for this week and also in the PowerPoint presentation. So in general, normalization is a process for just evaluating and correcting any table structures or entities that allows you to minimize data redundancies. And when we are able to minimize data redundancies, we can minimize the amount of data anomalies that we find within our database. So it's going to kind of also add on to the concept of determination that we went over in chapter three. So I'm referring back to chapter three if you need a little bit more information on determination first. Okay, so let's get started. First we're going to look at this very basic example of a dependency diagram to just understand the concept before we actually go into an example utilizing actual attribute names. Okay, so how do we determine dependence to create this diagram to begin with? So what we do is we look at entities or tables that we know will be in within our database. And from that, we can determine by looking at the data that actually exists, or by talking to end users or looking at business rules to understand how the data actually flows within the database, that will help us understand how we can create this functional diagram. Okay, so by looking at this, so someone has already done that for this example. And we just have fictitious, fictitious, you know, number attributes here. But let's say that somebody has looked at this data and said, you know what, this top part right here we know is the functional dependency section. So they've looked at this and said, well, C1 and C3 together, being the concatenated primary key, they do determine attributes C2, C4, and C5. Okay, however, in addition to that, C2 can also be determined by just knowing C1. Therefore, if you had C3 and C1, you still can know C2. But in addition, just knowing C1 will allow you to know C2. That is what's called a partial dependency. So a partial dependency is anytime you have one attribute within your table that is functionally dependent on just one part of the primary key. Okay, and that's what's key in this one is in a partial dependency, it's that it's dependent on a piece of the primary key. So another dependency is called a transitive dependency. And that's what we have here. So someone has looked at this data and noticed that, hey, I can actually know what C5 is by just knowing what C4 is. So I don't actually need to know C1 and C3 to be able to get C5. And by the functional dependency up here, you can see that that is the case. If I knew C1 and C3, I could determine C5. However, just knowing C4 allows me to know C5. Okay, and when we have actual data here in this, in my second example, you'll actually, it'll be come a little bit more clear as to why C4 determines C5. But for now, just know that someone has determined that C4 alone can determine C5. Okay, so those are our transitive dependencies. So now what, what do we do with these? We can, we can know right by looking at this, this diagram that this functional dependency or this dependency diagram is in first normal form. It's in first normal form because it does contain partial dependencies, and it does contain transitive dependencies. So to move through the normal forms, you have to, to get to second normal form, first remove partial dependencies, and then to get to third normal form, remove transitive dependencies. So we will go through those steps today and know that there are higher normal forms. And in this example, I'm only going to go through third normal form because it's the most common and most, most transactional processing environments are, are in, should be in third normal form. However, know that, that there are more normal forms, and there's also a concept of denormalization, and that will all be discussed within your, your normal course content this week. So this is just a little bit extra just to kind of help in the normalization process. So this right here is what we started with. This is our first normal form table. And we first want to, to get it to second normal form, we have to remove any partial dependencies. So remember that right here is our partial dependency. So the, the steps involved in removing a partial dependency are always the same. So you take the primary key that is the determinant in this case, and create a new table as that being the primary key. Okay? And whatever attribute that is determined by that primary key also comes along with it. The one caveat to that is that C1 is, is going to stay in the original table as well. And the reason for that is because when you create this new table, there will need to be a, a foreign key, primary key relationship between the new table and the old table. So this C1 value staying in the old table will actually enable that. So C2 however gets removed. So this is the new table, and this is what's left of the old table. So C1 is still there, C3 is still there, C2 is gone, well it's gone from right here. And then we still have C4 and C5. Remember that when you rewrite the new table, you still continue to bring over any transitive dependencies that come along. So we can see that this table is right now in third normal form because there are no transitive dependent, or no partial dependencies and no transitive dependencies. However this one right here has only been moved to second normal form in this process because there's still a transitive dependency here. Okay? So now we'd like to create a database whose tables are in at least third normal form. Okay? So this is what we started with, table 1, table 2. And we see that the only problem here is with table, table 2 having a transitive dependency. So just table 1 is going to come over here and just stay exactly the same. And I'm going to skip right down here to table 3 first. So table 3, we're going to take C4, which is the determining attribute in this relationship, and it's going to become just like when we did it with the partial dependency. It's going to become the primary key in the new table, even though it was not a primary key in the old table. So that's what's different about transitive. The transitive determinant is going to become the primary key in the new table always. And just like with partial though, the dependent key or the dependent attribute is also going to come with it. Okay? So now it also has a functional dependence here. We always want to put the functional dependencies on the top of every single table that we create and keep it when we have it on the original ones. So now let's look back at the original table and this is what it now looks like. It still has C1 and C3, that didn't change. It still has C4. So remember any time you're taking out a primary, something that's going to become a primary key value in your new table, it needs to also remain in the old table. Okay? So now we have three tables that are in third normal form. So we have accomplished our goal. So now let's look at an example using real information and real data, not real data, but at least real attributes. So in this example, how do you know that you have all these functional dependencies, or well we know we have the functional dependencies, but how do you know that you have these partial and transitive dependencies? Well sometimes it's just common knowledge. We can look at this and say if we know the ISBN number, we also would know the book title. We'd also know the publisher and we would know the edition. Okay? We wouldn't necessarily know the last name of the author by just knowing the ISBN number, but we would know publisher, edition, and book title. Okay? And you can verify that by looking at the data. Another partial dependency we see here is if we know the author number, like basically the author ID, we automatically are going to know the name of the author, the last name here. Okay? And then a transitive dependency we have here is we can realize that if we have a book title, that's automatically going to tell us that we have what the publisher name is. Okay? So I want to point out here, this is a good time to explain the fact that this right here is in first normal form. However, it's very rare that something ever starts in first normal form. So when you see data, this is probably the only time that I've ever seen anything in first normal form is if it's just in spreadsheet format and it's just on somebody's computer and they have just haphazardly just created their own simple spreadsheet to just keep track of something, not thinking about database design in any way. So they're just thinking I want to keep track of my ISBN and the title and the author number and the last name of the author, publisher, royalty edition. Okay? So in that example, yes, we would be able to start from first normal form and move on from there. However, if a database professional was looking at this, they would automatically probably know that there's book information here, there's author information here, and there's publisher information here. So somebody that knows anything about database design is already going to know that there are three separate tables in this data and possibly more, but they're going to know that there's at least three. And so when they actually determine that there's going to be a book table and an author table and a publisher table and think about the attributes within it and then the relationships between them when they're maybe creating an ER diagram, they are going to automatically be in second, at least second, if not third normal form. So a lot of times you come at something, come at some sort of data, some tables, and it's already in second or third normal form just because whoever did it has the knowledge behind them to know that these are separate entities. Okay? So therefore, I don't want you to get confused when you're doing your course project, your final project for this class, when maybe you created your ER diagram and you did a great job and really thought through the relationships very well and thought about the nouns and what you're, you have the correct set of entities in front of you. And then when you get to the normalization part, you think, well, I don't understand, what do I do here? And it says show your steps of normalization. Well, you don't have to show them if you are already in third normal form. I don't ever want anybody to denormalize just to show first normal form, second normal form, and third normal form. What I do want you to do is create dependency diagrams like this for every single one of your tables. For your tables, I should definitely see this functional, you know, this, there should be this functional dependency up here. And then as you're walking through this process, look at each table and make sure there aren't any transitive or partial dependencies that you may have missed because it is possible. So even if you think that you just really nailed your database design, you still want to go through this process to make sure you didn't, you didn't leave something out, okay, or you actually left something in that should have been pulled out. So let's go through this example though, just assuming that this is our data, maybe we know nothing about database design, we just know we have all of this information, want to store it in the table. And we just know by looking at it that this is not good because we have identified partial and transitive dependencies. Okay, so just like with our last example, we're first going to start with going, moving to second normal form. So we have transitive or partial dependencies right here with ISBN is determining three attributes and author number is determining one. So that's going to create two additional tables. Okay, so we remember that we take the determining, the determinant in the partial dependency and it's going to come out and become a prime, not come out, it's going to copy and become a primary key value of the new table. Remember it's also going to also state existing in the old table. Okay, so it comes out and then all of its determining attributes come with so book title, publisher and edition. Okay, and remember that it has a transitive dependency that we have already identified here so that needs to come with it. Okay, when we move to third normal form we'll worry about that. So then we also want to look at author number, there's another, this last name, this is another partial dependency. So author number is going to be the primary key for the new table and it's going to bring along with it the last name. Okay, so now in the original table what we have left is ISBN because it's still there, it just got a copy over here but it's still in the original table. Book title is no longer there because it was a determining functionally, it was functionally dependent on ISBN so it got removed, publisher got removed and edition got removed. Okay, in addition to that last name got removed and got placed in the author table. So the only thing left is ISBN author number and royalty and we can look at this further and just make sure it looks correct because we really do not know the royalty amount unless we know the author number and the ISBN. So we couldn't just know ISBN and get the royalty and we also couldn't just know author number and get the royalty. So this looks like a pretty complete second normal form database here. Actually this one right here is in third and this one's in third and this one's in second but so these are at least in second normal form I guess. Okay, so then lastly we just want to change it to third normal form and remove this transitive dependency. So this table is going to stay the same, it's right here, this table is going to stay the same, it's right here and the only thing that's going to change is book title is going to be the new primary key and the new table and then publisher is going to actually come out of the table and be in the new book table. So and this is the original table here, ISBN, book title and addition are the last remaining things here. Okay, so now we have third normal form and so I told you that it's possible that you would have looked at this information and already known what entities you're going to have here and so if you had looked at this data and already known that there was a book and an author table, well you would have thought well there's a relationship between book and author and when you're looking at that relationship you would say you know a book can have many, one book can have many authors and one author can have many books therefore you would have seen a many to many relationship. So once you see that many to many relationship you would have resolved it by creating a bridge entity and putting the book author as the maybe the name of the new table. Okay and so then we also, you also would have known that there was a publisher table to begin with too because you would have just seen publisher information. So just know that we could have come up with this information by just knowing database design or we could have come up with it the way we did it today and walking through first, second and third normal forms. So we come out with the same exact end result however sometimes if you don't understand normalization and don't understand this process that if you don't create these at some point you may miss that there is a transitive dependency or a partial dependency that you just didn't see when you were creating your ER diagram. So it's an iterative process between normalization and ER diagramming to just make sure that you have really identified your entities correctly that you have relationships defined correctly and so this is a great, great process. So I do hope this helps you as you work through your normalization concepts this week and I hope it also helps clear up any questions that you may have had when you're trying to write your final project, create your final project and not wanting, I never want you to actually denormalize your tables just to show me the different levels of normalization. If you are just that good and automatically got to third normal form great but I still do want to see your functionally, your dependency diagrams in that section so that I can look at it and know that you didn't miss any of the partial or transitive dependencies somewhere along the line. So I hope that was helpful for this week. I hope you're enjoying the class so far and I look forward to the final presentations at the end.\n",
            "Corrected:  Hi and welcome to week four. I'm Kathy Wilson and today we're going to go over some normalization examples. Normalization is a very important foundational concept in database design. And so I wanted to provide just a little bit of extra instruction in this area that's just in addition to what you can find in your book readings for this week and also in the PowerPoint presentation. So in general, normalization is a process for just evaluating and correcting any table structures or entities that allows you to minimize data redundancies. And when we are able to minimize data redundancies, we can minimize the amount of data anomalies that we find within our database. So it's going to kind of also add on to the concept of determination that we went over in chapter three. So I'm referring back to chapter three if you need a little bit more information on determination first. Okay, so let's get started. First we're going to look at this very basic example of a dependency diagram to just understand the concept before we actually go into an example utilizing actual attribute names. Okay, so how do we determine dependence to create this diagram to begin with? So what we do is we look at entities or tables that we know will be in within our database. And from that, we can determine by looking at the data that actually exists, or by talking to end users or looking at business rules to understand how the data actually flows within the database, that will help us understand how we can create this functional diagram. Okay, so by looking at this, so someone has already done that for this example. And we just have fictitious, fictitious, you know, number attributes here. But let's say that somebody has looked at this data and said, you know what, this top part right here we know is the functional dependency section. So they've looked at this and said, well, C1 and C3 together, being the concatenated primary key, they do determine attributes C2, C4, and C5. Okay, however, in addition to that, C2 can also be determined by just knowing C1. Therefore, if you had C3 and C1, you still can know C2. But in addition, just knowing C1 will allow you to know C2. That is what's called a partial dependency. So a partial dependency is anytime you have one attribute within your table that is functionally dependent on just one part of the primary key. Okay, and that's what's key in this one is in a partial dependency, it's that it's dependent on a piece of the primary key. So another dependency is called a transitive dependency. And that's what we have here. So someone has looked at this data and noticed that, hey, I can actually know what C5 is by just knowing what C4 is. So I don't actually need to know C1 and C3 to be able to get C5. And by the functional dependency up here, you can see that that is the case. If I knew C1 and C3, I could determine C5. However, just knowing C4 allows me to know C5. Okay, and when we have actual data here in this, in my second example, you'll actually, it'll be come a little bit more clear as to why C4 determines C5. But for now, just know that someone has determined that C4 alone can determine C5. Okay, so those are our transitive dependencies. So now what, what do we do with these? We can, we can know right by looking at this, this diagram that this functional dependency or this dependency diagram is in first normal form. It's in first normal form because it does contain partial dependencies, and it does contain transitive dependencies. So to move through the normal forms, you have to, to get to second normal form, first remove partial dependencies, and then to get to third normal form, remove transitive dependencies. So we will go through those steps today and know that there are higher normal forms. And in this example, I'm only going to go through third normal form because it's the most common and most, most transactional processing environments are, are in, should be in third normal form. However, know that, that there are more normal forms, and there's also a concept of denormalization, and that will all be discussed within your, your normal course content this week. So this is just a little bit extra just to kind of help in the normalization process. So this right here is what we started with. This is our first normal form table. And we first want to, to get it to second normal form, we have to remove any partial dependencies. So remember that right here is our partial dependency. So the, the steps involved in removing a partial dependency are always the same. So you take the primary key that is the determinant in this case, and create a new table as that being the primary key. Okay? And whatever attribute that is determined by that primary key also comes along with it. The one caveat to that is that C1 is, is going to stay in the original table as well. And the reason for that is because when you create this new table, there will need to be a, a foreign key, primary key relationship between the new table and the old table. So this C1 value staying in the old table will actually enable that. So C2 however gets removed. So this is the new table, and this is what's left of the old table. So C1 is still there, C3 is still there, C2 is gone, well it's gone from right here. And then we still have C4 and C5. Remember that when you rewrite the new table, you still continue to bring over any transitive dependencies that come along. So we can see that this table is right now in third normal form because there are no transitive dependent, or no partial dependencies and no transitive dependencies. However this one right here has only been moved to second normal form in this process because there's still a transitive dependency here. Okay? So now we'd like to create a database whose tables are in at least third normal form. Okay? So this is what we started with, table 1, table 2. And we see that the only problem here is with table, table 2 having a transitive dependency. So just table 1 is going to come over here and just stay exactly the same. And I'm going to skip right down here to table 3 first. So table 3, we're going to take C4, which is the determining attribute in this relationship, and it's going to become just like when we did it with the partial dependency. It's going to become the primary key in the new table, even though it was not a primary key in the old table. So that's what's different about transitive. The transitive determinant is going to become the primary key in the new table always. And just like with partial though, the dependent key or the dependent attribute is also going to come with it. Okay? So now it also has a functional dependence here. We always want to put the functional dependencies on the top of every single table that we create and keep it when we have it on the original ones. So now let's look back at the original table and this is what it now looks like. It still has C1 and C3, that didn't change. It still has C4. So remember any time you're taking out a primary, something that's going to become a primary key value in your new table, it needs to also remain in the old table. Okay? So now we have three tables that are in third normal form. So we have accomplished our goal. So now let's look at an example using real information and real data, not real data, but at least real attributes. So in this example, how do you know that you have all these functional dependencies, or well we know we have the functional dependencies, but how do you know that you have these partial and transitive dependencies? Well sometimes it's just common knowledge. We can look at this and say if we know the ISBN number, we also would know the book title. We'd also know the publisher and we would know the edition. Okay? We wouldn't necessarily know the last name of the author by just knowing the ISBN number, but we would know publisher, edition, and book title. Okay? And you can verify that by looking at the data. Another partial dependency we see here is if we know the author number, like basically the author ID, we automatically are going to know the name of the author, the last name here. Okay? And then a transitive dependency we have here is we can realize that if we have a book title, that's automatically going to tell us that we have what the publisher name is. Okay? So I want to point out here, this is a good time to explain the fact that this right here is in first normal form. However, it's very rare that something ever starts in first normal form. So when you see data, this is probably the only time that I've ever seen anything in first normal form is if it's just in spreadsheet format and it's just on somebody's computer and they have just haphazardly just created their own simple spreadsheet to just keep track of something, not thinking about database design in any way. So they're just thinking I want to keep track of my ISBN and the title and the author number and the last name of the author, publisher, royalty edition. Okay? So in that example, yes, we would be able to start from first normal form and move on from there. However, if a database professional was looking at this, they would automatically probably know that there's book information here, there's author information here, and there's publisher information here. So somebody that knows anything about database design is already going to know that there are three separate tables in this data and possibly more, but they're going to know that there's at least three. And so when they actually determine that there's going to be a book table and an author table and a publisher table and think about the attributes within it and then the relationships between them when they're maybe creating an ER diagram, they are going to automatically be in second, at least second, if not third normal form. So a lot of times you come at something, come at some sort of data, some tables, and it's already in second or third normal form just because whoever did it has the knowledge behind them to know that these are separate entities. Okay? So therefore, I don't want you to get confused when you're doing your course project, your final project for this class, when maybe you created your ER diagram and you did a great job and really thought through the relationships very well and thought about the nouns and what you're, you have the correct set of entities in front of you. And then when you get to the normalization part, you think, well, I don't understand, what do I do here? And it says show your steps of normalization. Well, you don't have to show them if you are already in third normal form. I don't ever want anybody to denormalize just to show first normal form, second normal form, and third normal form. What I do want you to do is create dependency diagrams like this for every single one of your tables. For your tables, I should definitely see this functional, you know, this, there should be this functional dependency up here. And then as you're walking through this process, look at each table and make sure there aren't any transitive or partial dependencies that you may have missed because it is possible. So even if you think that you just really nailed your database design, you still want to go through this process to make sure you didn't, you didn't leave something out, okay, or you actually left something in that should have been pulled out. So let's go through this example though, just assuming that this is our data, maybe we know nothing about database design, we just know we have all of this information, want to store it in the table. And we just know by looking at it that this is not good because we have identified partial and transitive dependencies. Okay, so just like with our last example, we're first going to start with going, moving to second normal form. So we have transitive or partial dependencies right here with ISBN is determining three attributes and author number is determining one. So that's going to create two additional tables. Okay, so we remember that we take the determining, the determinant in the partial dependency and it's going to come out and become a prime, not come out, it's going to copy and become a primary key value of the new table. Remember it's also going to also state existing in the old table. Okay, so it comes out and then all of its determining attributes come with so book title, publisher and edition. Okay, and remember that it has a transitive dependency that we have already identified here so that needs to come with it. Okay, when we move to third normal form we'll worry about that. So then we also want to look at author number, there's another, this last name, this is another partial dependency. So author number is going to be the primary key for the new table and it's going to bring along with it the last name. Okay, so now in the original table what we have left is ISBN because it's still there, it just got a copy over here but it's still in the original table. Book title is no longer there because it was a determining functionally, it was functionally dependent on ISBN so it got removed, publisher got removed and edition got removed. Okay, in addition to that last name got removed and got placed in the author table. So the only thing left is ISBN author number and royalty and we can look at this further and just make sure it looks correct because we really do not know the royalty amount unless we know the author number and the ISBN. So we couldn't just know ISBN and get the royalty and we also couldn't just know author number and get the royalty. So this looks like a pretty complete second normal form database here. Actually this one right here is in third and this one's in third and this one's in second but so these are at least in second normal form I guess. Okay, so then lastly we just want to change it to third normal form and remove this transitive dependency. So this table is going to stay the same, it's right here, this table is going to stay the same, it's right here and the only thing that's going to change is book title is going to be the new primary key and the new table and then publisher is going to actually come out of the table and be in the new book table. So and this is the original table here, ISBN, book title and addition are the last remaining things here. Okay, so now we have third normal form and so I told you that it's possible that you would have looked at this information and already known what entities you're going to have here and so if you had looked at this data and already known that there was a book and an author table, well you would have thought well there's a relationship between book and author and when you're looking at that relationship you would say you know a book can have many, one book can have many authors and one author can have many books therefore you would have seen a many to many relationship. So once you see that many to many relationship you would have resolved it by creating a bridge entity and putting the book author as the maybe the name of the new table. Okay and so then we also, you also would have known that there was a publisher table to begin with too because you would have just seen publisher information. So just know that we could have come up with this information by just knowing database design or we could have come up with it the way we did it today and walking through first, second and third normal forms. So we come out with the same exact end result however sometimes if you don't understand normalization and don't understand this process that if you don't create these at some point you may miss that there is a transitive dependency or a partial dependency that you just didn't see when you were creating your ER diagram. So it's an iterative process between normalization and ER diagramming to just make sure that you have really identified your entities correctly that you have relationships defined correctly and so this is a great, great process. So I do hope this helps you as you work through your normalization concepts this week and I hope it also helps clear up any questions that you may have had when you're trying to write your final project, create your final project and not wanting, I never want you to actually denormalize your tables just to show me the different levels of normalization. If you are just that good and automatically got to third normal form great but I still do want to see your functionally, your dependency diagrams in that section so that I can look at it and know that you didn't miss any of the partial or transitive dependencies somewhere along the line. So I hope that was helpful for this week. I hope you're enjoying the class so far and I look forward to the final presentations at the end.\n",
            "Transcription has been saved to normalization_transcript.txt\n",
            "Optimal number of clusters: 7\n",
            "Normalization is a very important foundational concept in database design.\n",
            "Therefore, if you had C3 and C1, you still can know C2.\n",
            "So another dependency is called a transitive dependency.\n",
            "So I don't actually need to know C1 and C3 to be able to get C5.\n",
            "If I knew C1 and C3, I could determine C5.\n",
            "Okay, so those are our transitive dependencies.\n",
            "It's in first normal form because it does contain partial dependencies, and it does contain transitive dependencies.\n",
            "So we will go through those steps today and know that there are higher normal forms.\n",
            "So this is the new table, and this is what's left of the old table.\n",
            "Remember that when you rewrite the new table, you still continue to bring over any transitive dependencies that come along.\n",
            "And we see that the only problem here is with table, table 2 having a transitive dependency.\n",
            "It's going to become the primary key in the new table, even though it was not a primary key in the old table.\n",
            "So now we have three tables that are in third normal form.\n",
            "We can look at this and say if we know the ISBN number, we also would know the book title.\n",
            "We wouldn't necessarily know the last name of the author by just knowing the ISBN number, but we would know publisher, edition, and book title.\n",
            "However, if a database professional was looking at this, they would automatically probably know that there's book information here, there's author information here, and there's publisher information here.\n",
            "So a lot of times you come at something, come at some sort of data, some tables, and it's already in second or third normal form just because whoever did it has the knowledge behind them to know that these are separate entities.\n",
            "And then as you're walking through this process, look at each table and make sure there aren't any transitive or partial dependencies that you may have missed because it is possible.\n",
            "Okay, so just like with our last example, we're first going to start with going, moving to second normal form.\n",
            "Remember it's also going to also state existing in the old table.\n",
            "So it's an iterative process between normalization and ER diagramming to just make sure that you have really identified your entities correctly that you have relationships defined correctly and so this is a great, great process.\n",
            "Summary file saved\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=t45S_MwAcOw\n",
            "[youtube] t45S_MwAcOw: Downloading webpage\n",
            "[youtube] t45S_MwAcOw: Downloading tv client config\n",
            "[youtube] t45S_MwAcOw: Downloading player 9599b765-main\n",
            "[youtube] t45S_MwAcOw: Downloading tv player API JSON\n",
            "[youtube] t45S_MwAcOw: Downloading ios player API JSON\n",
            "[youtube] t45S_MwAcOw: Downloading m3u8 information\n",
            "[info] t45S_MwAcOw: Downloading 1 format(s): 251\n",
            "[download] Destination: t45S_MwAcOw.webm\n",
            "[download] 100% of    9.25MiB in 00:00:00 at 33.20MiB/s  \n",
            "[ExtractAudio] Destination: t45S_MwAcOw.wav\n",
            "Deleting original file t45S_MwAcOw.webm (pass -k to keep)\n",
            "Downloaded and converted audio to WAV: t45S_MwAcOw.wav\n",
            "Transcribing...\n",
            "Transcription is ready\n",
            "\n",
            "Transcription:\n",
            "\n",
            " Hi, I'm Sanjana Reddy, a machine learning engineer at Google's Advanced Solutions Lab. There's been a lot of excitement around generative AI and all the new advancements, including new vertex AI features that are coming up, such as GenAI Studio, Model Garden, GenAI API. Our objective in this short session is to give you a solid footing on some of the underlying concepts that make all the GenAI magic possible. Today, I'm going to talk about transformer models and the BERT model. Language modeling has evolved over the years. The recent breakthroughs in the past 10 years include the usage of neural networks to represent text such as Word2Weck and Ngrams in 2013. In 2014, the development of sequence-to-sequence models such as RNNs and LSTMs helped improve the performance of ML models on NLP tasks such as translation and text classification. In 2015, the excitement came with attention mechanisms and the models built based on it such as Transformers and the BERT model. In this presentation, we'll focus on Transformers. Transformers is based on a 2017 paper named Attention as All You Need. Although all the models before Transformers were able to represent words as vectors, these vectors did not contain the context and the usage of words changes based on the context. For example, bank in riverbank versus bank in bank robber might have the same vector representation before attention mechanisms came about. A transformer is an encoder-decoder model that uses the attention mechanism. It can take advantage of parallelization and also process a large amount of data at the same time because of its model architecture. Attention mechanism helps improve the performance of machine translation applications. Transformer models were built using attention mechanisms at the core. A transformer model consists of encoder and decoder. The encoder encodes the input sequence and passes it to the decoder and the decoder decodes the representation for a relevant task. The encoding component is a stack of encoders of the same number. The research paper that introduced Transformers stacks six encoders on top of each other. Six is not a magical number, it's just a hyperparameter. The encoders are all identical in structure but with different weights. Each encoder can be broken down into two sub-layers. The first layer is called the self-attention. The input of the encoder first flows through a self-attention layer which helps the encoder look at relevant parts of the words as it encodes a center word in the input sentence. And the second layer is called a feedforward layer. The output of the self-attention layer is fed to the feedforward neural network. The exact same feedforward neural network is independently applied to each position. The decoder has both the self-attention and the feedforward layer but between them is the encoder-decoder-attention layer that helps the decoder focus on relevant parts of the input sentence. After embedding the words in the input sequence, each of the embedding vector flows through the two layers of the encoder. The word at each position passes through a self-attention process. Then it passes through a feedforward neural network. The exact same network with each vector flowing through it separately. Dependencies exist between these paths in the self-attention layer. However, the feedforward layer does not have these dependencies and therefore various paths can be executed in parallel while they flow through the feedforward layer. In the self-attention layer, the input embedding is broken up into query, key, and value vectors. These vectors are computed using weights that the transformer learns during the training process. All of these computations happen in parallel in the model in the form of matrix computations. Once we have the query, key, and value vectors, the next step is to multiply each value vector by the softmax score in preparation to sum them up. The intuition here is to keep intact the values of the words you want to focus on and leave out irrelevant words by multiplying them by tiny numbers like 0.001, for example. Next, we have to sum up the weighted value vectors, which produces the output of the self-attention layer at this position for the first word. You can send along the resulting vector to the feedforward neural network. To sum up this process of getting the final embeddings, these are the steps that we take. We start with the natural language sentence, embed each word in the sentence. After that, we perform multi-headed attention eight times in this case and multiply this embedded word with the respective weighted matrices. We then calculate the attention using the resulting QKV matrices. Finally, we concatenate the matrices to produce the output matrix, which is the same dimension as the final matrix that this layer initially got. There's multiple variations of transformers out there now. Some use both the encoder and the decoder component from the original architecture. Some use only the encoder and some use only the decoder. A popular encoder-only architecture is BERT. BERT is one of the trained transformer models. BERT stands for Bidirectional Encoder Representations from Transformers and was developed by Google in 2018. Since then, multiple variations of BERT have been built. Today, BERT powers Google Search. You can see how different the results provided by BERT are for the same search query before and after. BERT was trained in two variations. One model contains BERT Base, which had 12 stack of transformers with approximately 110 million parameters and the other BERT Large with 24 layers of transformers with about 340 million parameters. The BERT model is powerful because it can handle long input context. It was trained on the entire Wikipedia corpus and Books corpus. The BERT model was trained for 1 million steps. BERT is trained on different tasks, which means it has multi-task objective. This makes BERT very powerful. Because of the kind of tasks it was trained on, it works at both a sentence level and at a token level. These are the two different versions of BERT that were originally released. One is BERT Base, which had 12 layers, whereas BERT Large had 24 layers and compared to the original transformer, which had six layers. The way that BERT works is that it was trained on two different tasks. Task one is called a masked language model, where the sentences are masked and the model is trained to predict the masked words. If you were to train BERT from scratch, you would have to mask a certain percentage of the words in your corpus. The recommended percentage for masking is 15%. The masking percentage achieves a balance between too little and too much masking. Too little masking makes the training process extremely expensive and too much masking removes the contacts that the model requires. The second task is to predict the next sentence. For example, the model is given two sets of sentences. BERT aims to learn the relationships between sentences and predict the next sentence given the first one. For example, sentence A could be, a man went to the store and sentence B is, he bought a gallon of milk. BERT is responsible for classifying if sentence B is the next sentence after sentence A. This is a binary classification task. This helps BERT perform at a sentence level. In order to train BERT, you need to feed three different kinds of embeddings to the model. For the input sentence, you get three different embeddings, token, segment, and position embeddings. The token embeddings is a representation of each token as an embedding in the input sentence. The words are transformed into vector representations of certain dimensions. BERT can solve NLP tasks that involve text classification as well. An example is to classify whether two sentences say, my dog is cute and he likes playing are semantically similar. The pairs of input texts are simply concatenated and fed into the model. How does BERT distinguish the input in a given pair? The answer is to use segment embeddings. There is a special token represented by SEP that separates the two different splits of the sentence. Another problem is to learn the order of the words in the sentence. As you know, BERT consists of a stack of transformers. BERT is designed to process input sequences up to a length of 512. The order of the input sequence is incorporated into the position embeddings. This allows BERT to learn a vector representation for each position. BERT can be used for different downstream tasks. Although BERT was trained on mass language modeling and single sentence classification, it can be used for popular NLP tasks like single sentence classification, sentence pair classification, question answering, and single sentence tagging tasks. Thank you for listening.\n",
            "Corrected:  Hi, I'm Sanjana Reddy, a machine learning engineer at Google's Advanced Solutions Lab. There's been a lot of excitement around generative AI and all the new advancements, including new vertex AI features that are coming up, such as GenAI Studio, Model Garden, GenAI API. Our objective in this short session is to give you a solid footing on some of the underlying concepts that make all the GenAI magic possible. Today, I'm going to talk about transformer models and the BERT model. Language modeling has evolved over the years. The recent breakthroughs in the past 10 years include the usage of neural networks to represent text such as Word2Weck and Ngrams in 2013. In 2014, the development of sequence-to-sequence models such as RNNs and LSTMs helped improve the performance of ML models on NLP tasks such as translation and text classification. In 2015, the excitement came with attention mechanisms and the models built based on it such as Transformers and the BERT model. In this presentation, we'll focus on Transformers. Transformers is based on a 2017 paper named Attention as All You Need. Although all the models before Transformers were able to represent words as vectors, these vectors did not contain the context and the usage of words changes based on the context. For example, bank in riverbank versus bank in bank robber might have the same vector representation before attention mechanisms came about. A transformer is an encoder-decoder model that uses the attention mechanism. It can take advantage of parallelization and also process a large amount of data at the same time because of its model architecture. Attention mechanism helps improve the performance of machine translation applications. Transformer models were built using attention mechanisms at the core. A transformer model consists of encoder and decoder. The encoder encodes the input sequence and passes it to the decoder and the decoder decodes the representation for a relevant task. The encoding component is a stack of encoders of the same number. The research paper that introduced Transformers stacks six encoders on top of each other. Six is not a magical number, it's just a hyperparameter. The encoders are all identical in structure but with different weights. Each encoder can be broken down into two sub-layers. The first layer is called the self-attention. The input of the encoder first flows through a self-attention layer which helps the encoder look at relevant parts of the words as it encodes a center word in the input sentence. And the second layer is called a feedforward layer. The output of the self-attention layer is fed to the feedforward neural network. The exact same feedforward neural network is independently applied to each position. The decoder has both the self-attention and the feedforward layer but between them is the encoder-decoder-attention layer that helps the decoder focus on relevant parts of the input sentence. After embedding the words in the input sequence, each of the embedding vector flows through the two layers of the encoder. The word at each position passes through a self-attention process. Then it passes through a feedforward neural network. The exact same network with each vector flowing through it separately. Dependencies exist between these paths in the self-attention layer. However, the feedforward layer does not have these dependencies and therefore various paths can be executed in parallel while they flow through the feedforward layer. In the self-attention layer, the input embedding is broken up into query, key, and value vectors. These vectors are computed using weights that the transformer learns during the training process. All of these computations happen in parallel in the model in the form of matrix computations. Once we have the query, key, and value vectors, the next step is to multiply each value vector by the softmax score in preparation to sum them up. The intuition here is to keep intact the values of the words you want to focus on and leave out irrelevant words by multiplying them by tiny numbers like 0.001, for example. Next, we have to sum up the weighted value vectors, which produces the output of the self-attention layer at this position for the first word. You can send along the resulting vector to the feedforward neural network. To sum up this process of getting the final embeddings, these are the steps that we take. We start with the natural language sentence, embed each word in the sentence. After that, we perform multi-headed attention eight times in this case and multiply this embedded word with the respective weighted matrices. We then calculate the attention using the resulting QKV matrices. Finally, we concatenate the matrices to produce the output matrix, which is the same dimension as the final matrix that this layer initially got. There's multiple variations of transformers out there now. Some use both the encoder and the decoder component from the original architecture. Some use only the encoder and some use only the decoder. A popular encoder-only architecture is BERT. BERT is one of the trained transformer models. BERT stands for Bidirectional Encoder Representations from Transformers and was developed by Google in 2018. Since then, multiple variations of BERT have been built. Today, BERT powers Google Search. You can see how different the results provided by BERT are for the same search query before and after. BERT was trained in two variations. One model contains BERT Base, which had 12 stack of transformers with approximately 110 million parameters and the other BERT Large with 24 layers of transformers with about 340 million parameters. The BERT model is powerful because it can handle long input context. It was trained on the entire Wikipedia corpus and Books corpus. The BERT model was trained for 1 million steps. BERT is trained on different tasks, which means it has multi-task objective. This makes BERT very powerful. Because of the kind of tasks it was trained on, it works at both a sentence level and at a token level. These are the two different versions of BERT that were originally released. One is BERT Base, which had 12 layers, whereas BERT Large had 24 layers and compared to the original transformer, which had six layers. The way that BERT works is that it was trained on two different tasks. Task one is called a masked language model, where the sentences are masked and the model is trained to predict the masked words. If you were to train BERT from scratch, you would have to mask a certain percentage of the words in your corpus. The recommended percentage for masking is 15%. The masking percentage achieves a balance between too little and too much masking. Too little masking makes the training process extremely expensive and too much masking removes the contacts that the model requires. The second task is to predict the next sentence. For example, the model is given two sets of sentences. BERT aims to learn the relationships between sentences and predict the next sentence given the first one. For example, sentence A could be, a man went to the store and sentence B is, he bought a gallon of milk. BERT is responsible for classifying if sentence B is the next sentence after sentence A. This is a binary classification task. This helps BERT perform at a sentence level. In order to train BERT, you need to feed three different kinds of embeddings to the model. For the input sentence, you get three different embeddings, token, segment, and position embeddings. The token embeddings is a representation of each token as an embedding in the input sentence. The words are transformed into vector representations of certain dimensions. BERT can solve NLP tasks that involve text classification as well. An example is to classify whether two sentences say, my dog is cute and he likes playing are semantically similar. The pairs of input texts are simply concatenated and fed into the model. How does BERT distinguish the input in a given pair? The answer is to use segment embeddings. There is a special token represented by SEP that separates the two different splits of the sentence. Another problem is to learn the order of the words in the sentence. As you know, BERT consists of a stack of transformers. BERT is designed to process input sequences up to a length of 512. The order of the input sequence is incorporated into the position embeddings. This allows BERT to learn a vector representation for each position. BERT can be used for different downstream tasks. Although BERT was trained on mass language modeling and single sentence classification, it can be used for popular NLP tasks like single sentence classification, sentence pair classification, question answering, and single sentence tagging tasks. Thank you for listening.\n",
            "Transcription has been saved to transformer_transcript.txt\n",
            "Optimal number of clusters: 5\n",
            "Language modeling has evolved over the years.\n",
            "A transformer is an encoder-decoder model that uses the attention mechanism.\n",
            "A transformer model consists of encoder and decoder.\n",
            "The input of the encoder first flows through a self-attention layer which helps the encoder look at relevant parts of the words as it encodes a center word in the input sentence.\n",
            "The decoder has both the self-attention and the feedforward layer but between them is the encoder-decoder-attention layer that helps the decoder focus on relevant parts of the input sentence.\n",
            "In the self-attention layer, the input embedding is broken up into query, key, and value vectors.\n",
            "These vectors are computed using weights that the transformer learns during the training process.\n",
            "We start with the natural language sentence, embed each word in the sentence.\n",
            "BERT is one of the trained transformer models.\n",
            "Since then, multiple variations of BERT have been built.\n",
            "BERT was trained in two variations.\n",
            "Task one is called a masked language model, where the sentences are masked and the model is trained to predict the masked words.\n",
            "The second task is to predict the next sentence.\n",
            "BERT can solve NLP tasks that involve text classification as well.\n",
            "Summary file saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NWc41ypkTwqL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}