{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3yF1R4CSWmmU/ImFbbGgQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shruthimohan03/video-summarizer/blob/main/GMM_for_Extractive_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "nFeEIv_AfSql"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load and preprocess the text file\n",
        "def load_and_preprocess(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "    # Split text into sentences based on periods or question marks\n",
        "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', content)\n",
        "    # Clean sentences (keep full stops, remove other punctuations, and strip extra spaces)\n",
        "    sentences = [\n",
        "        re.sub(r'[^a-zA-Z0-9\\s\\.]', '', sentence).strip() for sentence in sentences if sentence.strip()\n",
        "    ]\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "qtwlabhMfVTw"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Preprocess and vectorize sentences\n",
        "def preprocess_and_vectorize(sentences):\n",
        "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    sentence_vectors = tfidf_vectorizer.fit_transform(sentences).toarray()\n",
        "    return sentence_vectors"
      ],
      "metadata": {
        "id": "dXsdJzrufZju"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Fit GMM\n",
        "def fit_gmm(sentence_vectors, n_clusters):\n",
        "    gmm = GaussianMixture(n_components=n_clusters, random_state=42)\n",
        "    gmm.fit(sentence_vectors)\n",
        "    labels = gmm.predict(sentence_vectors)\n",
        "    return labels"
      ],
      "metadata": {
        "id": "mEhZKHLtfbyU"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Extract representative sentences\n",
        "def extract_summary(sentences, sentence_vectors, labels):\n",
        "    summary = []\n",
        "    unique_labels = np.unique(labels)\n",
        "    for label in unique_labels:\n",
        "        # Get indices of sentences in the current cluster\n",
        "        cluster_indices = np.where(labels == label)[0]\n",
        "        # Find the most central sentence in the cluster\n",
        "        cluster_center = np.mean(sentence_vectors[cluster_indices], axis=0)\n",
        "        central_index = cluster_indices[np.argmax(cosine_similarity([cluster_center], sentence_vectors[cluster_indices])[0])]\n",
        "        summary.append(sentences[central_index])\n",
        "    return ''.join(summary)"
      ],
      "metadata": {
        "id": "OELUoQVofrA1"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "file_path = 'computer_lecture.txt'\n",
        "sentences = load_and_preprocess(file_path)\n",
        "sentence_vectors = preprocess_and_vectorize(sentences)\n",
        "\n",
        "n_clusters = 6  # no of optimal is 6 for this dataset as found using elbow method\n",
        "labels = fit_gmm(sentence_vectors, n_clusters)\n",
        "summary = extract_summary(sentences, sentence_vectors, labels)"
      ],
      "metadata": {
        "id": "JcGAELexfi-P"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the summarized text to a file\n",
        "with open(\"extractive_summarization_gmm_centroid_method.txt\", \"w\") as file:\n",
        "    file.write(summary)\n",
        "\n",
        "print(\"Summarization completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFBFammpfpAy",
        "outputId": "3109a3db-304f-44c5-ae95-6856fa3636b4"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarization completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "lK1TJJHkgQn4",
        "outputId": "2cd830bf-6113-4a8f-f32e-f8e74a67126d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'One of the significant milestones in computer history was the invention of the internet.In conclusion computers are much more than machines they are catalysts of progress and innovation.The internet transformed computers from standalone devices into interconnected tools of communication and information exchange.Innovations such as quantum computing and advanced artificial intelligence promise to redefine our understanding of computation and problemsolving.However the widespread use of computers is not without challenges.At their core computers operate by processing data using binary logic.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    }
  ]
}