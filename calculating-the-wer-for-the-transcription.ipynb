{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11271165,"sourceType":"datasetVersion","datasetId":6527480}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:01:59.539612Z","iopub.execute_input":"2025-01-22T17:01:59.540065Z","iopub.status.idle":"2025-01-22T17:01:59.964083Z","shell.execute_reply.started":"2025-01-22T17:01:59.540035Z","shell.execute_reply":"2025-01-22T17:01:59.963184Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FINAL OUTPUT","metadata":{}},{"cell_type":"markdown","source":"### Adding correct punctuations to the transcript (text preprocessing)","metadata":{}},{"cell_type":"code","source":"pip install language-tool-python","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T12:20:13.665102Z","iopub.execute_input":"2025-04-06T12:20:13.665448Z","iopub.status.idle":"2025-04-06T12:20:19.381390Z","shell.execute_reply.started":"2025-04-06T12:20:13.665419Z","shell.execute_reply":"2025-04-06T12:20:19.380326Z"}},"outputs":[{"name":"stdout","text":"Collecting language-tool-python\n  Downloading language_tool_python-2.9.2-py3-none-any.whl.metadata (54 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (5.9.5)\nRequirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (0.10.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (2024.12.14)\nDownloading language_tool_python-2.9.2-py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: language-tool-python\nSuccessfully installed language-tool-python-2.9.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import language_tool_python\n\ndef punctuate_text(text):\n    tool = language_tool_python.LanguageTool('en-US')\n\n    # Only correct punctuation-related errors\n    punctuation_matches = [\n        match for match in tool.check(text)\n        if 'PUNCTUATION' in match.ruleIssueType.upper()\n    ]\n    # Apply the corrections\n    corrected_text = language_tool_python.utils.correct(text, punctuation_matches)\n    return corrected_text\n\nwith open('/kaggle/input/transcripts/transcribed_text_small_model.txt', 'r', encoding='utf-8') as file:\n        transcript = file.read()\ncorrected = punctuate_text(transcript)\nprint(\"Corrected:\", corrected)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T12:20:19.382751Z","iopub.execute_input":"2025-04-06T12:20:19.383129Z","iopub.status.idle":"2025-04-06T12:20:41.741946Z","shell.execute_reply.started":"2025-04-06T12:20:19.383100Z","shell.execute_reply":"2025-04-06T12:20:41.740202Z"}},"outputs":[{"name":"stderr","text":"Downloading LanguageTool 6.5: 100%|██████████| 248M/248M [00:03<00:00, 78.1MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Corrected:  Hello everyone, welcome back to the final lecture of the first week. So, in the last lecture we were discussing about various empirical large in particular Jif's law and Heaps law that how the words in the vocabulary are distributed in a corpus. We saw that the distribution is not very uniform, there are certain words that are very common. So, we saw that roughly 100 words in the vocabulary make for 50 percent of the corpus, by that I mean that number of tokens. And on the other hand there are 50 percent of the words in the vocabulary that occur only once. And we discussed what are the various relationships among the vocabulary size and the number of tokens that I observe in a corpus. And also how they grow with this to each other and Jif's law gives gave me a relation between the frequency and the rank of a word. So, today in this lecture we will start with the basic pre processing in language. So, we will cover the basic concepts and what are the challenges that one might face while doing the processing. So, we are going to the basics of text processing. So, we will start with the problem of tokenization as the name would suggest. Remember the name token, token is an individual word in my corpus. So, now what happens when I am pre processing the text in given in any language. What I will face is a string of characters, the sequence of characters. Now, I need to identify what are all the different words that are there in this in this sequence. So, now tokenization is the process by which I convert this string of characters into sequence of various words. So, I am trying to segmented by the various words that I am observing. So, now before going into what is tokenization, I will just talk about a slightly related problem sentence segmentation. So, this you may or may not have to do always and it depends on what is your application. So, for example, suppose you are doing classification for the whole document into certain classes. You might not have to go to the individual sentence and you can just talk about what are various words that are present in this document. On the other hand, suppose you are trying to find out what are the important sentences in this document, in that application you will have to go to the individual sentence. So, now if you have to go to the individual sentence, the first task that you will face is how do I segment this whole document into a sequence of sentences. So, this is sentence one, sentence two and so on and this task is called sentence segmentation. So, now you might feel that this is very trivial task, but let us see is it trivial. So, what is sentence segmentation? So, problem of deciding where my sentence begins and ends. So, that I have a complete unit of words that that I call as a sentence. Now, do you think there might be certain challenge involved? Suppose I am talking about the language English, can I always say that wherever I have a dot, it is the end of the sentence. So, there are many ways in which I can end my sentence. So, I can have exclamation or question mark that ends the sentence and they are mostly unambiguous. So, whenever I have exclamation or question mark, I can say probably this is the end of the sentence, but is the case the same with a dot. So, can you think of a scenario where I have a dot in English and, but it is not the end of sentence. So, you can find all sorts of abbreviations, right, they end with a period like doctor, mister, m p h. So, you have three dots here. So, you cannot call each of the this as the end of your sentence. So, again you have numbers 2.4, 4.3 and so on. So, that means the problem of deciding whether a particular dot is the end of the sentence or not is not entirely trivial. So, I need to build certain algorithm for finding out is it my end of the sentence. So, in text processing if we face this kind of problem in nearly every simple every simple task that we are doing. So, even if it looks a trivial task, we face with this problem that can I always call dot as the end of the sentence. So, how do we go about solving this? Now, if you think about it, whenever I see a dot or question mark or exclamation, I always have to decide one of the two things. Is it the end of the sentence or is it not the end of the sentence? So, any data point that I am seeing, I have to divide into one of these two classes. So, if you think of these are two classes end of the sentence or not end of the sentence. So, each point you have to divide into one of the two classes and this in general this problem in general is called classification problem. So, you are classifying into one of the two classes. Now, so the idea is very simple. So, you have two classes and each data point you have to divide into one of the two classes. So, that means you have to build some sort of rule of algorithm for doing that. So, in this case, I have to build a binary classifier. What do I mean by a binary classifier? There are two classes end of the sentence or not end of the sentence. In general, there can be multiple classes. So, now for each dot or in general for every word, I need to decide whether this is the end of the sentence or not the end of the sentence. So, in general my classifiers that I will build can be some rules that I write by hand, some simple if then else rules or it can be some expressions. I say if my particular example matches with this structural expression, it is one class, if it does not match, it is another class or I can build a machine learning classifier. So, in this particular scenario, what can be the simplest thing to do? Let us see can we build a simple rule based classifier. So, we will start with the example of a simple decision tree. So, by decision tree I mean a set of if then else statements. So, I am at a particular word, I want to decide whether this is the end of the sentence or not. So, I can have the simple if then else kind of decision tree here. So, I am at a word and I the first thing I check is are there lots of blank lines after me. So, this would happen in a text whenever this is the end of the paragraph and there are some blank lines. So, if I feel that there are a lot of blank lines after me that means after this word I may say this might be the end of the sentence with a good confidence. So, that is why the branch here says yes this is the end of the sentence, but suppose there are not a lot of blank lines then I will check if the final punctuation is equation mark or exclamation or a colon in that case. So, there are quite an ambiguous I will say this is the end of the sentence. Now, suppose it is not then I will check if the final punctuation is a period. So, if it is a period if it is not a period this is easy to say that this is not the end of the sentence, but suppose this is a period. So, again I cannot say for certain if it is the end of the sentence. So, I will again check for simplicity I might have a list of abbreviations and I can check if the word that I am correctly facing is one of the abbreviations in my list. If it is there I will say this is not end of the sentence if it is. So, here I am etcetera or any other abbreviation if the answer is yes I am not end of the sentence if the answer is no that means this word is not in abbreviation and this will be the end of the sentence. So, this is very very simple if then else rules this may not be correct, but this is one particular way in which this problem can be solved. In general you might want to use some other sort of indications we call them as various features they are various observations that you make from your corpus. So, what are some examples? So, suppose I see the word that is ending with dot can I use this as a feature whether my word starts with an upper case lower case all caps or is it a number how will it help. So, let us see I have here I am here and my word is 4.3. So, I am at dot I want to find out if it is the end of the sentence if I can say that the previous the current word is a number it is a high probability that this will be in number and it will not be the end of the sentence. So, this can be used as another feature. So, again by feature you can think of a simple rule whether the word I am currently at is a number or I can use the fact where the case of the word with dot is upper case or lower case. So, what happens generally in abbreviations they are mostly in upper case. So, suppose I have doctor and it starts with an upper case I can say that this might be an abbreviation same with lower case lower case will give me more probability that this is not an abbreviation. Similarly, I can also use the case of the word after dot. So, is it upper case lower case, capital or number. So, how will that help? So, again whenever I have the end of the sentence the next word in general starts with a capital. So, again this can be used what can be some other features. So, I can have some numerical features. So, that is I will have certain thresholds what is the length of the word ending with dot is it if the length is small it might be an abbreviation if the length is larger it might not be an abbreviation. And I can also use probably the what is the probability that the word that is ending with dot occurs at the end of the sentence. So, if it is really the end of the sentence it might happen that in a large corpus this end sentence quite often. Same thing I can do with the next word after dot is it the start of the sentence what is the probability that it occurs in the start of the sentence in a large corpus. So, you might be able to use any of these features to decide given a purple word is it the end of the sentence or not. So, now suppose I ask you this question do you have the same problem in other languages like Hindi. So, in Hindi you will see that in general there is only a danda that you use to indicate the end of the sentence and this is not used for any other purpose. So, this problem you will see is again language dependent this problem is there for English, but not so for Hindi. But we will see there are other problems that do not exist for English language, but are there for other Indian languages. We will see some of those examples in the same lecture. Now, so how do we implement it recently. So, as you have seen this is a simple if then else statement. So, now what is important is that you choose the correct set of features. So, how do you go about choosing the set of features you will see in your from your data what are some observations that can separate my two classes here. So, my two classes here are end of the sentence and not the end of the sentence and what are the observations we were having. In general it might be an abbreviation the case of the word and that is before the dot may be upper case or lower case and one of these might indicate one class other might indicate other class. So, all these are my observations that I use as my features. Now, whenever I am using a numerical features like the length of the word before dot I need to pick some sort of threshold that is whether the length of the word is between 2 to 3 or say more than 3 between 5 to 7 like that. So, my tree can be if the length of the word is between 5 to 7 I go to one class other I go to another class. So, now here is one problem suppose I keep on increasing my features this can be both numerical or non numerical features it might be difficult to set up my if then else rules by hand. So, in that scenario I can try to use some sort of machine learning technique to learn this decision tree. In the literature there are lot of such algorithms that are available that given a data and a set of features will consider decision tree for you. So, I will just give you so the names of some of the algorithms and the basic idea on which they work is that. So, at every point you have to choose a particular split. So, you have to choose a feature value that is splits my data into certain parts and I have certain criteria to find out what is the best split. So, one particular criteria is what is the information gain by this. So, these algorithms that we have mentioned here like I d 3 c 4 1 5 and cart they all use one of these criteria. In general once you have identified what are your interesting features for this task you cannot limited to only one classifier decision tree. You can also try out some other classifiers like support vector machines, logistic regression and neural networks. All these are quite popular classifiers for various n l p applications. So, we will talk about some of these as we will go to some some advanced topics in this course. Now coming back to a problem of tokenization we said that tokenization is a process of segmenting a string of characters into words finding out what are the different words in this history. Now remember we talked about token and type distinction. So, suppose I give you a simple sentence here I have a can opener, but I cannot open these cans. How many tokens are there? If you count there are 11 different words 11 different occurrences of words. So, you have 11 word tokens, but how many unique words are there? So, you will find there are only 10 unique words which word repeats here. So, the word I repeats twice. So, there are 10 types and 11 tokens. So, my tokenization is to find out each of the 11 word tokens from this sentence. In practice at least for English you can use certain toolkits that are available like n l t k in python, core n l p in java and you can you can also use some unique commands. So, in this course we will mainly be using n l t k tool kit for doing all this pre processing task and some other tasks as well, but in general you can use any of these three possibilities. So, for English most of the problems that we will see are taking care of by the tokenizers tokenizers that we have discussed previously, but it is still it is good to know what are the challenges that are involved when I had I try to design a tokenization algorithm. So, for example, here you will see that I if I encounter a word like fill lands in my data. So, one question that I have is whether I treated as simple fill land as it is fill lands or I converted to fill lands by removing the apostrophe. So, this question you might also try to defer to the next processing step that we will see, but sometimes you might want to tackle this in the same step. Similarly, if you see what are do I treat it as a single token or two tokens what are this problem you might have to solve in the same step whether I treat it as a single token or multiple tokens same with I am should not should not and so on. Similarly, whenever you have name entities like San Francisco should I treat it as a single token or two separate tokens. Now, remember when we were talking about some of the cases Y and L page hard. So, you might have to find out this particular sequence of tokens is a single entity and treated as a single entity not as multiple different tokens. So, this problem is related similarly, if you find m dot p dot h do you call it a single token or multiple tokens. So, now there are no fixed answers to these and some of these might depend on what is the application for which you are doing this pre processing. But one thing you can always keep in mind suppose you are doing it for the application of information trivial the same sort of steps that you apply for your documents should be applied to your query as well otherwise you will not be able to match them perfectly. So, suppose if I am using it for information trivial. So, I should use the same convention for both my documents as well as the query. So, then another problem can be how do I handle hyphens in my data. So, this looks again a simple problem, but we will see it is not that simple. So, let us see some kind of examples what are the various sorts of hyphens that can be there in my corpus. So, here I have a sentence from a research paper abstract and the sentence is this paper describes mimic and adaptive mixed initiative is spoken dialogue system that provides movie show time information. So, in this sentence itself you see two different hyphens one is with initiative another is show hyphen time. So, now can you see that these two are different hyphens the first hyphen is not in general that I will I will use in my text. Second hyphen I can use in my text I can write show time with an hyphen, but how did this hyphen initiative came into the corpus. So, we have given this a title end of line hyphen. So, what happens in a research papers for example, whenever you write a sentence you might have to do some sort of justification and that is where you end the line even if it is not the end of the word. So, you will end up within hyphen. So, now when you are trying to pre process and when you are retrieving such kind of hyphens you might have to join these together and you say you have to say that this is a single word initiative and not initiate hyphen tape, but again this is this is not trivial because for show time you will not do the same show time you might want to keep it as it is. Then there are some other kind of hyphens like lexical hyphens. So, you might have these hyphens with various prefixes like co pre meta multi etcetera. Sometimes they are sententially determined hyphens also that is you put hyphens. So, that it becomes easier to interpret the sentence like here case paged hand delivered etcetera are optional. Similarly, if you see in the next sentence 3 to 5 year direct marketing plan 3 to 5 year can be written perfectly without keeping the hyphens, but here you are putting it so that it becomes easier to interpret that particular occurrence. So, again when you are doing tokenization your problem at how do I handle all these hyphens. Further there are various issues that might that you might face for certain languages, but not others. So, for example is like in French if you have a token like l'ensemble. So, you might want to match it with ensemble. So, that might be a similar problem that we are facing in English, but let us take something in German. So, I have this I have this big sentence here, but the problem is that this is not a single word. This is a compound composed of 4 different words and the corresponding English meaning is this one. So, you have 4 words in English. So, when you are putting in in French they make a compound. So, now what is the problem that we will face when you are processing the German text and you are trying to tokenize it. So, you might want to find out what are the individual words in this particular compound. So, you need some sort of compound splitter for German. So, this problem is there for German not so much for English. So, now what happens if I am taking a language like Chinese or Japanese. So, here is a sentence in Chinese. So, what do you see in Chinese words are written without any spaces in between. So, now when you are doing the preprocessing your task is to find out what are the individual word tokens in this Chinese sentence. So, this problem is also difficult because in general for a given utterance of a sequence of characters there might be more than one possible ways of breaking into sequence of words and both might be perfectly valid possibilities. So, in Chinese we do not have any space between words and I have to find out what are the places where I have to break these words and this problem is called tokenization word tokenization. Same problem happens with Japanese and you have further complications because they are using four different scripts like katakana, hiragana, kanji and romanji. So, this problem becomes a bit more severe. Now the same problem is there even for Sanskrit. So, if some of you have taken a Sanskrit course in your class 8th or 10th. So, you might be familiar with the rules of Sunday in Sanskrit language. So, let us say this is a simple single sentence in Sanskrit, but this is a huge this looks like a single word, but it is not a single word. It is composed of multiple words in Sanskrit and they are combined with a Sunday relation. This stands for nice proverb in Sanskrit that translates in English as one should tell the truth, one should say kind words, one should neither tell harsh truths nor flatting lies. This is the rule for all times. This is a proverb and this is a single sentence that talks about this proverb, but where all the words are combined with Sunday relation. So, if you try to undo the Sunday relation, this is what you will find at the segmented text. So, there are multiple words in this sentence. They are combined to make a single, it looks like a single word. No. So, this problem we saw in Chinese, Japanese and Sanskrit, but in Sanskrit the problem is slightly more complicated and why is that? So, in Japanese and in Chinese when you try to combine various words together, you simply concatenate them. You put them one after another without making any changes at the boundary. It does not happen in Sanskrit. When you combine two words, you also make certain changes at the boundary and this is called the Sunday operation. So, in this particular case, see here I have the word Bryath and the word Nath. But when I am combining, I am writing it Bryanna. So, you see here the letter T gets changed to N. So, that means when I am trying to analyze the sentence. So, this particular sentence in Sanskrit, I need to find out not only what are the breaks, but what is the corresponding word from which this sentence is derived. So, from here I have to find out that the actual words are Bryath plus Nath. That gives me this Bryanna and this is very, very common in Sanskrit that you are always combining words by doing Sunday operation. So, this further complicates my problem of word tokenization or segmentation. So, this is just a list from Wikipedia. What are the longest words in various languages? Not the sentence, but the words. You see in Sanskrit, the longest word is composed of 431 characters. It is a compound and then you have Greek and Africans and other languages. In English, you see the longest word is of 45 characters. It is non-scientific. So, what is the particular word in Sanskrit that is composed of 431 letters. So, this was from the Vardambika Parinayachampu by Thirumlamba. This is a single compound from his book. So, now when I talk about this problem of tokenization in Sanskrit or in English, this problem is also called word segmentation. I have a sequence of characters and you segmented to find out individual words. Now, what is the simplest algorithm that you can think of? Let us say the case of change. So, the simplest algorithm that works is a greedy algorithm that is called maximum matching algorithm. So, whenever you are given a string, you start your pointer at the beginning of the string. Now, suppose that you have the dictionary and the words that you are currently seeing all should be in the dictionary. So, you will find out what is the maximum match as per my dictionary in the string. You break there and put the pointer from at the next character and again do the same thing. So, this greedily chooses what are the actual words by taking the maximum matches and this works nicely for most of the cases. Now, so this is a letter question. Now, can you think of some cases where the segmentation will also be required for English text? In English in general, we do not combine words to make a single word. We do not do that, but what is the scenario where we are doing that right now? So, does do hashtags come into mind? So, for example, suppose I have hashtags like thank you Sachin and music Monday. So, here different words are combined together without putting a boundary in between. So, if you are given a hashtag and you have to analyze that, you have to actually segmented into various words. Now, so when I talk about Sanskrit, so this we have a segment available at the site Sanskrit dot India dot F R. So, we will just briefly see what is the design principle of building a segment in Sanskrit. So, first we have a generative model that says how do I generate a sentence in Sanskrit? I have a finite alphabet sigma that means a set of various characters in Sanskrit. Now, from this finite alphabet, I can generate a lot of words that are composed of various number of phonemes or letters from this alphabet. Now, when I have a set of words, I can now combine them together with an operation of Sandhi. That is what I mean by sigma star here. So, W star here. So, I have a set of words W and I do a clean it was that means I can combine any number of words together, but whenever I am combining words, I am doing them by a Sandhi operation because there is a relation between the words. So, have my set of inflected words also called Padas in Sanskrit and I have the relation of Sandhi between them and that is how I generate sentences, but my problem is how do I analyze them? So, that is the inverse problem. That is whenever I am given a sentence W, I have to analyze it by inverting the relations of Sandhi so that I can produce a finite set of word forms W 1 to W n and I am saying together with a proof that is a formal way of saying that, but what I mean is that W 1 to W n when I they combine by Sandhi operation, they give me the actual sentence, the initial sentence. So, that is how the segment is built. Now, this is a snapshot from the segmental. So, I gave the same sentence there and it gave me all the possible ways of analyzing the sentence and it says that there are 120 different solutions. So, here whenever I have Brianna, you see there are two possibilities Briath and Briam plus like that it gives me all the possible ways in which this sentence can be broken into individual word tokens. Now, this is another problem that I will have to find out what is the most likely word sequence among all these 120 possibilities, but we can use many different models that we will not talk about in this lecture probably in some other lectures. So, coming back to normalization. So, we talked about this problem that the same word might be written in multiple different ways like u dot s dot a versus u s a. Now, I should be able to match them together especially if you are doing information retrieval, you are giving a query and you are retrieving from some document. Suppose, your query contains u dot s dot a and the document contains u s a, if you are only doing the surface level match, you will not be able to map them to each other. So, that so you will have to consider this problem in advance and do the pre-processing accordingly of either your documents or the query, but using the same sort same settings. So, what I am what we are doing by this we are defining some sort of equivalence classes. We are saying u s a and u dot s dot a should go to one class and they are the same type. We also do some sort of case folding that is we can reduce all address to lower case. So, whenever I have the word like w o r d I will always write small w o r d. So, that whenever even if it is starting the sentence and it occurs in capital because of that in general I know that this is a word w o r d, but this is not a generic rule sometimes depending on application you might have certain exceptions. For example, you might have to treat the name entity separately. So, if you have a entity general motors you might want to keep it as it is without case folding. Similarly, you might want to keep u s for united states in upper case and not do the case folding and this is important for the application of machine translation also because if you do a case folding here you will know u s in lower case that means something else versus u s that is in united states. We also have the problem of lam tination that is you have individual words like m r is and you want to convert them to their lamma. That means what is the base form from which they are derived similarly, car, cars, cars, cars. So, all these are derived from car. So, again this is some sort of noblization you are saying all these are some sort of equivalence class because they come from the same word form. So, the problem of lam tination is that you have to find out the actual dictionary head word from which they are derived and for that we use morphology. So, what is morphology? I am trying to find out the structure of a word by seeing what is the particular stem the head word and what is the affix that I apply to it. So, these individual units are called various morphology. So, you have stems that are the dictionary head words and the affix that are what are the different units like as for plural etcetera you are applying to them to make the individual word. Some examples are like for prefix you have un, n t etcetera for English and a t pra etcetera for Hindi or Sanskrit. So, fix like it a t a tion etcetera and ta ka k etcetera for Hindi and in general you can also have some infix like you have a word like with and you can fix and in between this is Sanskrit. So, we will discuss in detail about it in morphology later. So, there is another concept you have lam tination where you are finding the actual dictionary head word. So, it is also concept called stemming where you do not try to find the actual dictionary head word, but you just try to remove certain suffix h and you whatever you obtain is called stem. So, this is a crude chopping of various suffix h in that word. So, this is again language dependent. So, what we are doing here words like automate, automatic automation all will be reduced to a single lemma automate here. So, this is stemming. So, you know the actual lemma is automate with an e, but here. So, I am just chopping of the affixes at the end. So, I am removing here this i c i o n all and putting it to automate. So, this is one example. So, if you try to do stemming here, so you will find from example, e is removed from compressed e d is removed and so on. So, what is the algorithm that is used for this stemming? So, we have the port as algorithm that is very very famous and this is again some set of if then else rules. So, what are some examples here? So, what is the first step? I take a word if it ends with s s e s, I remove e s from there and I end with s s. So, example is keres goes to keres if not then I see whether the word ends with i s I put it to i like ponies goes to pony. If not I see if the word ends with s s I keep bid as s s if not I see if the word ends with s I remove that as. So, keres goes to keres but keres does not go to keres with only one s because this step comes before. If there is a double s ending the word I retain it otherwise if there is a single s I remove that like that there are some other steps. So, if there is a wobble in the in the in my word and the word ends with i n g I remove i n g. So, walking goes to walk what about about king you see in in k there is no wobble. So, king will be retained as it is same there is a wobble and there is an e d I remove this e d and I have this what played to play. So, you can see that what is the use of this heuristic of having this wobble if you did not have this wobble you would have converted king to k and like there is some other use like if the word ends with aational then I put it put a t e so rational. So, relational to relate and if the word ends with i z e r I can I remove that r digitize at digitize a t u r to a t e and if the word ends with a l I remove that a l if the word ends with able I remove that able if the words end with a t e I remove that a t e. So, like that these are some steps that I take from my corpus for each word I converted to its step it does not give me the correct dictionary head word, but it still this this is a good practice in principle for information retrieval if you want to match the query with the documents. So, this is for this week. So, next week we will start with another preprocessing task that is spelling correction. Thank you.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### Evaluate the transcript","metadata":{}},{"cell_type":"code","source":"pip install jiwer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T12:20:41.743818Z","iopub.execute_input":"2025-04-06T12:20:41.744209Z","iopub.status.idle":"2025-04-06T12:20:48.437138Z","shell.execute_reply.started":"2025-04-06T12:20:41.744172Z","shell.execute_reply":"2025-04-06T12:20:48.436022Z"}},"outputs":[{"name":"stdout","text":"Collecting jiwer\n  Downloading jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\nCollecting click>=8.1.8 (from jiwer)\n  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\nCollecting rapidfuzz>=3.9.7 (from jiwer)\n  Downloading rapidfuzz-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nDownloading jiwer-3.1.0-py3-none-any.whl (22 kB)\nDownloading click-8.1.8-py3-none-any.whl (98 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rapidfuzz-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, click, jiwer\n  Attempting uninstall: click\n    Found existing installation: click 8.1.7\n    Uninstalling click-8.1.7:\n      Successfully uninstalled click-8.1.7\nSuccessfully installed click-8.1.8 jiwer-3.1.0 rapidfuzz-3.13.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def import_txt_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        content = file.read()\n    return content\n\nreference_file_path = \"/kaggle/input/transcripts/reference_nptel_python_transcript.txt\"\nreference = import_txt_file(reference_file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T12:20:48.438743Z","iopub.execute_input":"2025-04-06T12:20:48.439044Z","iopub.status.idle":"2025-04-06T12:20:48.445358Z","shell.execute_reply.started":"2025-04-06T12:20:48.439020Z","shell.execute_reply":"2025-04-06T12:20:48.444414Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import re\nimport string\n\ndef preprocess_text(text):\n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove newlines and extra spaces\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Strip leading and trailing spaces\n    text = text.strip()\n    \n    return text\n\ncleaned_reference = preprocess_text(reference)\ncleaned_hypothesis = preprocess_text(corrected)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T12:20:48.446529Z","iopub.execute_input":"2025-04-06T12:20:48.446931Z","iopub.status.idle":"2025-04-06T12:20:48.474098Z","shell.execute_reply.started":"2025-04-06T12:20:48.446858Z","shell.execute_reply":"2025-04-06T12:20:48.472964Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from jiwer import wer\n\nerror_rate = wer(cleaned_reference, cleaned_hypothesis)\nprint(\"Word Error Rate:\", error_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T12:20:48.475290Z","iopub.execute_input":"2025-04-06T12:20:48.475626Z","iopub.status.idle":"2025-04-06T12:20:48.528185Z","shell.execute_reply.started":"2025-04-06T12:20:48.475600Z","shell.execute_reply":"2025-04-06T12:20:48.527323Z"}},"outputs":[{"name":"stdout","text":"Word Error Rate: 2.4495100127822753\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Preprocess the transcript","metadata":{}},{"cell_type":"code","source":"import language_tool_python\n\ndef punctuate_text(text):\n    tool = language_tool_python.LanguageTool('en-US')\n\n    # Only correct punctuation-related errors\n    punctuation_matches = [\n        match for match in tool.check(text)\n        if 'PUNCTUATION' in match.ruleIssueType.upper()\n    ]\n\n    # Apply the corrections\n    corrected_text = language_tool_python.utils.correct(text, punctuation_matches)\n    return corrected_text\n\nwith open('/kaggle/input/transcripts/nptel_python_small_model.txt', 'r', encoding='utf-8') as file:\n        transcript = file.read()\ncorrected = punctuate_text(transcript)\nprint(\"Corrected:\", corrected)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Input Video: NPTEL PYTHON VIDEO","metadata":{}},{"cell_type":"markdown","source":"### 1. BASE MODEL","metadata":{}},{"cell_type":"code","source":"def import_txt_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        content = file.read()\n    return content\n\nreference_file_path = \"/kaggle/input/transcripts/reference_nptel_transcription_1.txt\"\nreference = import_txt_file(reference_file_path)\n\nhypothesis_file_path= \"/kaggle/input/transcripts/transcribed_text_small_model.txt\"\nhypothesis = import_txt_file(hypothesis_file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:39:52.072137Z","iopub.execute_input":"2025-04-05T18:39:52.072541Z","iopub.status.idle":"2025-04-05T18:39:52.085569Z","shell.execute_reply.started":"2025-04-05T18:39:52.072515Z","shell.execute_reply":"2025-04-05T18:39:52.084231Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from jiwer import wer\n\nerror_rate = wer(reference, hypothesis)\nprint(\"Word Error Rate:\", error_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:39:53.344636Z","iopub.execute_input":"2025-04-05T18:39:53.344990Z","iopub.status.idle":"2025-04-05T18:39:53.368592Z","shell.execute_reply.started":"2025-04-05T18:39:53.344963Z","shell.execute_reply":"2025-04-05T18:39:53.367224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport string\n\ndef preprocess_text(text):\n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    \n    # Remove newlines and extra spaces\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Strip leading and trailing spaces\n    text = text.strip()\n    \n    return text\n\ncleaned_reference = preprocess_text(reference)\ncleaned_hypothesis = preprocess_text(hypothesis)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:15:44.539384Z","iopub.execute_input":"2025-04-05T18:15:44.539749Z","iopub.status.idle":"2025-04-05T18:15:44.548772Z","shell.execute_reply.started":"2025-04-05T18:15:44.539719Z","shell.execute_reply":"2025-04-05T18:15:44.547576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from jiwer import wer\n\nerror_rate = wer(cleaned_reference, cleaned_hypothesis)\nprint(\"Word Error Rate:\", error_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:39:57.913502Z","iopub.execute_input":"2025-04-05T18:39:57.913851Z","iopub.status.idle":"2025-04-05T18:39:57.933951Z","shell.execute_reply.started":"2025-04-05T18:39:57.913801Z","shell.execute_reply":"2025-04-05T18:39:57.933032Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### SMALL MODEL","metadata":{}},{"cell_type":"code","source":"def import_txt_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        content = file.read()\n    return content\n\nreference_file_path = \"/kaggle/input/transcripts/reference_nptel_python_transcript.txt\"\nreference = import_txt_file(reference_file_path)\n\nhypothesis_file_path= \"/kaggle/input/transcripts/nptel_python_small_model.txt\"\nhypothesis = import_txt_file(hypothesis_file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:16:59.647930Z","iopub.execute_input":"2025-04-05T18:16:59.648317Z","iopub.status.idle":"2025-04-05T18:16:59.659445Z","shell.execute_reply.started":"2025-04-05T18:16:59.648288Z","shell.execute_reply":"2025-04-05T18:16:59.658269Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from jiwer import wer\n\nerror_rate = wer(reference, hypothesis)\nprint(\"Word Error Rate:\", error_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:17:01.376132Z","iopub.execute_input":"2025-04-05T18:17:01.376514Z","iopub.status.idle":"2025-04-05T18:17:01.388038Z","shell.execute_reply.started":"2025-04-05T18:17:01.376469Z","shell.execute_reply":"2025-04-05T18:17:01.386882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Calculating the Word Error Rate","metadata":{}},{"cell_type":"markdown","source":"### BASE MODEL","metadata":{}},{"cell_type":"code","source":"def import_txt_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        content = file.read()\n    return content\n\nreference_file_path = \"/kaggle/input/transcripts/reference_nptel_transcription_1.txt\"\nreference = import_txt_file(reference_file_path)\n\nhypothesis_file_path= \"/kaggle/input/transcripts/transcribed_text_nptel_video_1.txt\"\nhypothesis = import_txt_file(hypothesis_file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T08:15:54.863346Z","iopub.execute_input":"2025-04-01T08:15:54.863956Z","iopub.status.idle":"2025-04-01T08:15:54.879164Z","shell.execute_reply.started":"2025-04-01T08:15:54.863901Z","shell.execute_reply":"2025-04-01T08:15:54.877801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport string\n\ndef preprocess_text(text):\n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    \n    # Remove newlines and extra spaces\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Strip leading and trailing spaces\n    text = text.strip()\n    \n    return text\n\ncleaned_reference = preprocess_text(reference)\ncleaned_hypothesis = preprocess_text(hypothesis)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T08:15:55.285326Z","iopub.execute_input":"2025-04-01T08:15:55.285745Z","iopub.status.idle":"2025-04-01T08:15:55.300509Z","shell.execute_reply.started":"2025-04-01T08:15:55.285715Z","shell.execute_reply":"2025-04-01T08:15:55.299093Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from jiwer import wer\n\nerror_rate = wer(cleaned_reference, cleaned_hypothesis)\nprint(\"Word Error Rate:\", error_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T08:15:56.324382Z","iopub.execute_input":"2025-04-01T08:15:56.324834Z","iopub.status.idle":"2025-04-01T08:15:56.425547Z","shell.execute_reply.started":"2025-04-01T08:15:56.324796Z","shell.execute_reply":"2025-04-01T08:15:56.423821Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cleaned_reference","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:02:07.735220Z","iopub.execute_input":"2025-01-22T17:02:07.735505Z","iopub.status.idle":"2025-01-22T17:02:07.743295Z","shell.execute_reply.started":"2025-01-22T17:02:07.735469Z","shell.execute_reply":"2025-01-22T17:02:07.742395Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cleaned_hypothesis","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:02:07.745293Z","iopub.execute_input":"2025-01-22T17:02:07.745578Z","iopub.status.idle":"2025-01-22T17:02:07.763139Z","shell.execute_reply.started":"2025-01-22T17:02:07.745554Z","shell.execute_reply":"2025-01-22T17:02:07.762161Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EXAMPLE2","metadata":{}},{"cell_type":"code","source":"def import_txt_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        content = file.read()\n    return content\n\nreference_file_path = \"/kaggle/input/transcripts/reference_nptel_transcription_2.txt\"\nreference = import_txt_file(reference_file_path)\n\nhypothesis_file_path= \"/kaggle/input/transcripts/transcribed_text_nptel_video_2.txt\"\nhypothesis=import_txt_file(hypothesis_file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:02:07.764297Z","iopub.execute_input":"2025-01-22T17:02:07.764601Z","iopub.status.idle":"2025-01-22T17:02:07.790438Z","shell.execute_reply.started":"2025-01-22T17:02:07.764576Z","shell.execute_reply":"2025-01-22T17:02:07.789292Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport string\n\ndef preprocess_text(text):\n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    \n    # Remove newlines and extra spaces\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Strip leading and trailing spaces\n    text = text.strip()\n    \n    return text\n\ncleaned_reference = preprocess_text(reference)\ncleaned_hypothesis = preprocess_text(hypothesis)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:02:07.791685Z","iopub.execute_input":"2025-01-22T17:02:07.792056Z","iopub.status.idle":"2025-01-22T17:02:07.807450Z","shell.execute_reply.started":"2025-01-22T17:02:07.792015Z","shell.execute_reply":"2025-01-22T17:02:07.806255Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from jiwer import wer\n\nerror_rate = wer(cleaned_reference, cleaned_hypothesis)\nprint(\"Word Error Rate:\", error_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:02:07.808754Z","iopub.execute_input":"2025-01-22T17:02:07.809158Z","iopub.status.idle":"2025-01-22T17:02:07.845301Z","shell.execute_reply.started":"2025-01-22T17:02:07.809118Z","shell.execute_reply":"2025-01-22T17:02:07.844226Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Without any preprocessing","metadata":{}},{"cell_type":"markdown","source":"### BASE MODEL","metadata":{}},{"cell_type":"code","source":"def import_txt_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        content = file.read()\n    return content\n\nreference_file_path = \"/kaggle/input/transcripts/reference_nptel_transcription_1.txt\"\nreference = import_txt_file(reference_file_path)\n\nhypothesis_file_path= \"/kaggle/input/transcripts/transcribed_text_nptel_video_1.txt\"\nhypothesis = import_txt_file(hypothesis_file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T06:31:17.132627Z","iopub.execute_input":"2025-04-01T06:31:17.133026Z","iopub.status.idle":"2025-04-01T06:31:17.150869Z","shell.execute_reply.started":"2025-04-01T06:31:17.132997Z","shell.execute_reply":"2025-04-01T06:31:17.148986Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from jiwer import wer\n\nerror_rate = wer(reference, hypothesis)\nprint(\"Word Error Rate:\", error_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T06:32:01.399495Z","iopub.execute_input":"2025-04-01T06:32:01.399982Z","iopub.status.idle":"2025-04-01T06:32:01.427245Z","shell.execute_reply.started":"2025-04-01T06:32:01.399838Z","shell.execute_reply":"2025-04-01T06:32:01.425961Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### SMALL MODEL","metadata":{}},{"cell_type":"code","source":"def import_txt_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        content = file.read()\n    return content\n\nreference_file_path = \"/kaggle/input/transcripts/reference_nptel_transcription_1.txt\"\nreference = import_txt_file(reference_file_path)\n\nhypothesis_file_path= \"/kaggle/input/transcripts/transcribed_text_small_model.txt\"\nhypothesis = import_txt_file(hypothesis_file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T06:33:47.955875Z","iopub.execute_input":"2025-04-01T06:33:47.956251Z","iopub.status.idle":"2025-04-01T06:33:47.972545Z","shell.execute_reply.started":"2025-04-01T06:33:47.956224Z","shell.execute_reply":"2025-04-01T06:33:47.971561Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from jiwer import wer\n\nerror_rate = wer(reference, hypothesis)\nprint(\"Word Error Rate:\", error_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T06:33:50.303443Z","iopub.execute_input":"2025-04-01T06:33:50.303809Z","iopub.status.idle":"2025-04-01T06:33:50.326329Z","shell.execute_reply.started":"2025-04-01T06:33:50.303783Z","shell.execute_reply":"2025-04-01T06:33:50.324968Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### LARGE MODEL","metadata":{}},{"cell_type":"code","source":"def import_txt_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        content = file.read()\n    return content\n\nreference_file_path = \"/kaggle/input/transcripts/reference_nptel_transcription_1.txt\"\nreference = import_txt_file(reference_file_path)\n\nhypothesis_file_path= \"/kaggle/input/transcripts/transcribed_text_large_model.txt\"\nhypothesis = import_txt_file(hypothesis_file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T08:14:52.116005Z","iopub.execute_input":"2025-04-01T08:14:52.116474Z","iopub.status.idle":"2025-04-01T08:14:52.125721Z","shell.execute_reply.started":"2025-04-01T08:14:52.116442Z","shell.execute_reply":"2025-04-01T08:14:52.124405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from jiwer import wer\n\nerror_rate = wer(reference, hypothesis)\nprint(\"Word Error Rate:\", error_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T08:13:20.740272Z","iopub.execute_input":"2025-04-01T08:13:20.740760Z","iopub.status.idle":"2025-04-01T08:13:20.800448Z","shell.execute_reply.started":"2025-04-01T08:13:20.740713Z","shell.execute_reply":"2025-04-01T08:13:20.799141Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### CLEANED REFERENCE","metadata":{}},{"cell_type":"markdown","source":"#### BASE","metadata":{}},{"cell_type":"code","source":"def import_txt_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        content = file.read()\n    return content\n\nreference_file_path = \"/kaggle/input/transcripts/cleaned_reference_nptel_transcript.txt\"\nreference = import_txt_file(reference_file_path)\n\nhypothesis_file_path= \"/kaggle/input/transcripts/transcribed_text_nptel_video_1.txt\"\nhypothesis = import_txt_file(hypothesis_file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T08:30:53.056411Z","iopub.execute_input":"2025-04-01T08:30:53.056800Z","iopub.status.idle":"2025-04-01T08:30:53.069405Z","shell.execute_reply.started":"2025-04-01T08:30:53.056759Z","shell.execute_reply":"2025-04-01T08:30:53.067430Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport string\n\ndef preprocess_text(text):\n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    \n    # Remove newlines and extra spaces\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Strip leading and trailing spaces\n    text = text.strip()\n    \n    return text\n\ncleaned_reference = preprocess_text(reference)\ncleaned_hypothesis = preprocess_text(hypothesis)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T08:30:53.388019Z","iopub.execute_input":"2025-04-01T08:30:53.388510Z","iopub.status.idle":"2025-04-01T08:30:53.404134Z","shell.execute_reply.started":"2025-04-01T08:30:53.388474Z","shell.execute_reply":"2025-04-01T08:30:53.402668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from jiwer import wer\n\nerror_rate = wer(cleaned_reference, cleaned_hypothesis)\nprint(\"Word Error Rate:\", error_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T08:30:54.026346Z","iopub.execute_input":"2025-04-01T08:30:54.026828Z","iopub.status.idle":"2025-04-01T08:30:54.051161Z","shell.execute_reply.started":"2025-04-01T08:30:54.026754Z","shell.execute_reply":"2025-04-01T08:30:54.049751Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### SMALL","metadata":{}},{"cell_type":"code","source":"def import_txt_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        content = file.read()\n    return content\n\nreference_file_path = \"/kaggle/input/transcripts/cleaned_reference_nptel_transcript.txt\"\nreference = import_txt_file(reference_file_path)\n\nhypothesis_file_path= \"/kaggle/input/transcripts/transcribed_text_small_model.txt\"\nhypothesis = import_txt_file(hypothesis_file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T08:31:02.050664Z","iopub.execute_input":"2025-04-01T08:31:02.051131Z","iopub.status.idle":"2025-04-01T08:31:02.060959Z","shell.execute_reply.started":"2025-04-01T08:31:02.051095Z","shell.execute_reply":"2025-04-01T08:31:02.059686Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport string\n\ndef preprocess_text(text):\n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    \n    # Remove newlines and extra spaces\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Strip leading and trailing spaces\n    text = text.strip()\n    \n    return text\n\ncleaned_reference = preprocess_text(reference)\ncleaned_hypothesis = preprocess_text(hypothesis)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T08:31:03.199397Z","iopub.execute_input":"2025-04-01T08:31:03.199807Z","iopub.status.idle":"2025-04-01T08:31:03.215659Z","shell.execute_reply.started":"2025-04-01T08:31:03.199776Z","shell.execute_reply":"2025-04-01T08:31:03.214010Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from jiwer import wer\n\nerror_rate = wer(cleaned_reference, cleaned_hypothesis)\nprint(\"Word Error Rate:\", error_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T08:31:05.675718Z","iopub.execute_input":"2025-04-01T08:31:05.676271Z","iopub.status.idle":"2025-04-01T08:31:05.704851Z","shell.execute_reply.started":"2025-04-01T08:31:05.676235Z","shell.execute_reply":"2025-04-01T08:31:05.703566Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### LARGE MODEL","metadata":{}},{"cell_type":"code","source":"def import_txt_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        content = file.read()\n    return content\n\nreference_file_path = \"/kaggle/input/transcripts/cleaned_reference_nptel_transcript.txt\"\nreference = import_txt_file(reference_file_path)\n\nhypothesis_file_path= \"/kaggle/input/transcripts/transcribed_text_large_model.txt\"\nhypothesis = import_txt_file(hypothesis_file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T08:31:10.617992Z","iopub.execute_input":"2025-04-01T08:31:10.618449Z","iopub.status.idle":"2025-04-01T08:31:10.627365Z","shell.execute_reply.started":"2025-04-01T08:31:10.618414Z","shell.execute_reply":"2025-04-01T08:31:10.626082Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport string\n\ndef preprocess_text(text):\n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    \n    # Remove newlines and extra spaces\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Strip leading and trailing spaces\n    text = text.strip()\n    \n    return text\n\ncleaned_reference = preprocess_text(reference)\ncleaned_hypothesis = preprocess_text(hypothesis)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T08:31:12.437942Z","iopub.execute_input":"2025-04-01T08:31:12.438358Z","iopub.status.idle":"2025-04-01T08:31:12.457677Z","shell.execute_reply.started":"2025-04-01T08:31:12.438329Z","shell.execute_reply":"2025-04-01T08:31:12.456539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from jiwer import wer\n\nerror_rate = wer(cleaned_reference, cleaned_hypothesis)\nprint(\"Word Error Rate:\", error_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T08:31:14.124844Z","iopub.execute_input":"2025-04-01T08:31:14.125285Z","iopub.status.idle":"2025-04-01T08:31:14.149943Z","shell.execute_reply.started":"2025-04-01T08:31:14.125251Z","shell.execute_reply":"2025-04-01T08:31:14.148384Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ","metadata":{}}]}