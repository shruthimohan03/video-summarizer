{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:01:59.539612Z","iopub.execute_input":"2025-01-22T17:01:59.540065Z","iopub.status.idle":"2025-01-22T17:01:59.964083Z","shell.execute_reply.started":"2025-01-22T17:01:59.540035Z","shell.execute_reply":"2025-01-22T17:01:59.963184Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/transcripts/transcribed_text_nptel_video_2.txt\n/kaggle/input/transcripts/reference_nptel_transcription_1.txt\n/kaggle/input/transcripts/transcribed_text_nptel_video_1.txt\n/kaggle/input/transcripts/reference_nptel_transcription_2.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Calculating the Word Error Rate","metadata":{}},{"cell_type":"code","source":"def import_txt_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        content = file.read()\n    return content\n\nreference_file_path = \"/kaggle/input/transcripts/reference_nptel_transcription_1.txt\"\nreference = import_txt_file(reference_file_path)\n\nhypothesis_file_path= \"/kaggle/input/transcripts/transcribed_text_nptel_video_1.txt\"\nhypothesis = import_txt_file(hypothesis_file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:01:59.965449Z","iopub.execute_input":"2025-01-22T17:01:59.966001Z","iopub.status.idle":"2025-01-22T17:01:59.976366Z","shell.execute_reply.started":"2025-01-22T17:01:59.965963Z","shell.execute_reply":"2025-01-22T17:01:59.975463Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import re\nimport string\n\ndef preprocess_text(text):\n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    \n    # Remove newlines and extra spaces\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Strip leading and trailing spaces\n    text = text.strip()\n    \n    return text\n\ncleaned_reference = preprocess_text(reference)\ncleaned_hypothesis = preprocess_text(hypothesis)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:01:59.978578Z","iopub.execute_input":"2025-01-22T17:01:59.978862Z","iopub.status.idle":"2025-01-22T17:01:59.994558Z","shell.execute_reply.started":"2025-01-22T17:01:59.978837Z","shell.execute_reply":"2025-01-22T17:01:59.993493Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"pip install jiwer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:01:59.995781Z","iopub.execute_input":"2025-01-22T17:01:59.996186Z","iopub.status.idle":"2025-01-22T17:02:07.678753Z","shell.execute_reply.started":"2025-01-22T17:01:59.996150Z","shell.execute_reply":"2025-01-22T17:02:07.677683Z"}},"outputs":[{"name":"stdout","text":"Collecting jiwer\n  Downloading jiwer-3.0.5-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\nCollecting rapidfuzz<4,>=3 (from jiwer)\n  Downloading rapidfuzz-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nDownloading jiwer-3.0.5-py3-none-any.whl (21 kB)\nDownloading rapidfuzz-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\nSuccessfully installed jiwer-3.0.5 rapidfuzz-3.11.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from jiwer import wer\n\nerror_rate = wer(cleaned_reference, cleaned_hypothesis)\nprint(\"Word Error Rate:\", error_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:02:07.679892Z","iopub.execute_input":"2025-01-22T17:02:07.680208Z","iopub.status.idle":"2025-01-22T17:02:07.734373Z","shell.execute_reply.started":"2025-01-22T17:02:07.680164Z","shell.execute_reply":"2025-01-22T17:02:07.733505Z"}},"outputs":[{"name":"stdout","text":"Word Error Rate: 0.1442275884882567\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"cleaned_reference","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:02:07.735220Z","iopub.execute_input":"2025-01-22T17:02:07.735505Z","iopub.status.idle":"2025-01-22T17:02:07.743295Z","shell.execute_reply.started":"2025-01-22T17:02:07.735469Z","shell.execute_reply":"2025-01-22T17:02:07.742395Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'hello everyone welcome back to the final lecture of the first week in the last lecture we were discussing about various empirical laws in particular zipf’s law and heap’s law that how the what is the vocabulary are distributed in a corpus we saw that the distribution is not very uniform there are certain words that are very very common so we saw that roughly hundred words in the vocabulary made for 50 percent of the corpus that by the time mean that number of tokens and on the other hand there are 50 percent were words in the vocabulary that occur only once and we discussed whatever various relationships among my vocabulary size and the number of tokens that i observe in a corpus and also how they grow with respect to each other and zipfs law gave me a relation between the frequency and the rank of a word so today in this lecture we will start with the basic key processing in language so we will cover the basic concepts and what are the challenges that one might face while doing the processing so we are going to the basics of text processingso we will start with the problem of tokenization as the name would suggest remember the name token token is an individual word in my corpus so now what happens when i am preprocessing the text in given in any language what i will face is a string of characters the sequence of characters now i need to indentify what are all the different words that are there in this sequence now tokenization is a process by which i convert the string of characters into sequence of various words so i am trying to segment it by the various words that i am observing now before going into what is tokenization i will just talk about slight little problem sentence segmentation so this you may or may not have to do always and it depends on what is your application for example suppose you are doing classification for the whole document in to certain classes you might not have to go to the individual sentence and you can just talk about what are the various words that are present in this document on the other hand suppose you are trying to find out what are the important sentences in this document in that application you will have to go to the individual sentence so now if you have to go to the individual sentence the first task that you will face is how do i segment these whole documents into a sequence of sentences so this is sentence one sentence two and so on and this task is called sentence segmentationnow you might feel that this is very trivial task but let us see is it trivial so what is sentence segmentation it is a problem of deciding where my sentence begins and ends so that i have a complete unit of words that i call as a sentence now do you think there might be certain challenges involved suppose i am talking about the language english can i always say that wherever i have a dot it is end of the sentence let us see so there are many ways in which i can end my sentence so i can have exclamation or question mark that ends the sentence and they are mostly unambiguous so whenever i have exclamation or question mark i can say probably this is the end of the sentence but is the case the same with a dot so i can think of a scenario where i have a dot in english but it is not the end of the sentence so we can find all sorts of abbreviations they end with a period like doctor mistermph so you have three dots here so you cannot each of this as the end of your sentence so again you have numbers 24 43 and so on that means the problem of deciding whether a particular dot is the end of the sentence or not is not entirely trivial so i need to build certain algorithm for finding out is it my end of the sentence in text processing we face this kind of problem in nearly every simple task that we are doing so even if it looks a trivial task we face with this problem that can i always call dot as end of the sentence so how do we go about solving this now if you think about it whenever i see a dot or question mark or exclamationi always have to decide one of the two things is it the end of the sentence or is not the end of the sentence any data point that i am seeing i have to divide into one of these two classes if you think of these as two classes end of the sentence or not end of the sentence each point you have to divide into one of the two classes and this in general this problem in general is called classification problem you are classifying into one of the two classes now so the idea is very very simple so you have two classes and each data point you have to divide into one of the two classes that means you have to build some sort of plural algorithm for doing that in this case i have to build the binary classifier what they mean by a binary classifier there are two classes end of the sentence or not end of the sentence in general there can be multiple classes so now for each dot or in general for every word i need to decide whether this is the end of the sentence or not the end of the sentence so in general my classifiers that i will build can be some rules that i write by hand some simple refer time 0627 nice rules or it can be some expressions i say my particular example matches with this set of expressions it is one class if i does not match it is of other class or i can build a machine learning classifier so in this particular scenario what can be the simplest thing to do let us see can we build a simple rule based classifierso we will start with example of a simple decision tree so by decision tree i mean a set of ifthenelse say statements so i am at a particular word i want to decide whether this is the end of the sentence or not so i can have the simple ifthenelse kind of decision tree here so met a word and the first thing i check is are there lots of blank lines after that so this would happen in a text whenever this is the end of the paragraph and there are some blank lines so if i feel that there are a lot of blank lines after me that means after this word i may say this might be the end of the sentence with a good confidence so that is why the branch here says yes this is the end of the sentence but suppose there are not lots of blank lines then i will check if the final punctuation is a question mark exclamation or a colon in that case so there are quite unambiguous and i may say this is the end of the sentence now suppose it is not then i will check if the final punctuation is a period so if it is not a period this is easy to say that this is not the end of the sentence but suppose this is an end of this is a period so again i cannot say for certain if it is the end of the sentence so i give a again check for simplicity i might have a list of abbreviations and i can check if the word that i am correctly facing is one of the abbreviations in my listif it is there i say this is not end of the sentence if it is so here i am etcetera or any other abbreviation if the answer is yes i am not end of the sentence if the answer is no that means this word is not an abbreviation and this will be the end of the sentence this is very very simple if thenelse rules this may not be correct but this is one particular way in which this problem can be solved in general you might want to use some other sort of indications we call them as various features these are various observations that you make from your corpus so what are some examplessuppose i see the word that is ending with dot can i use this as a feature whether my word starts with an upper case lower case cap all caps or is it number how will that help so let us see i am here and my word is 43 so i am at dot i want to find out if it is the end of the sentence if i can say that the current word is a number it is a high probability that this will be in number and it will not be the end of the sentence so this can be used as another feature again by feature you can think of a simple rule whether the word i am currently at is a number or i can use the fact where the case of the word with dots its upper case or lower case so what happens generally in abbreviations we are mostly in upper case so suppose i have doctor and it starts with an upper case i can say that this might be an abbreviation saying with the lower case lower case will give me more probability that this is not an abbreviation similarly i can also use in the case of the word after dot so is it upper case lower case capital or number so how will that help again whenever i have the end of the sentence the next word in general starts with a capital so again this can be used what can be some other features so i can have some numerical features that is i will have certain thresholds what is the length of the word ending with dot is it if the length is small it might be an abbreviation if the length is larger it might not be an abbreviation and i can also use probably what is the probability that the word that is ending with dot occurs at the end of the sentence so if it is really the end of the sentence it might happen then that in a large corpus this end sentence quite often something i can do with the next word after dot is it the start of the sentence what is the probability that it occurs in the start of the sentence in a large corpus so you might be able to use any of these features to decide given a particular word is it the end of the sentence or not so now suppose i ask you this question do you have the same problem in other languages like hindi so in hindi you will see that in general there is only agenda that you use to indicate the end of the sentence and this is not used for any other purpose so this problem you will see is again language dependent this problem is there for english but not so for hindi but you will see there are other problems that do not exist for english language but are there for other indian languages we will see some of those examples in the sameso how do we implement a decision tree as you have seen this is simple ifthenelse statement so now what is important is that you choose the correct set of features so how do you go about choosing the set of features you will see in your from your data what are some observations that can separate my two classes here so my two classes here are end of the sentence and non end of the sentence and what are the observations we were having in general it might be an abbreviation in the case of the word and that is before the dot maybe upper case or lower case and one of these might indicate one class the other might indicate other class so all these are my observations that i use as my featuresnow whenever i am using numerical features like the length of the word before dot i need to pick some sort of threshold that is whether the length of the word is between 2 to 3 or say more than 3 between 5 to 7 like that so my tree can be if the length of the word is between 5 to 7 i could one class otherwise i could another class now here is one problem suppose i keep on increasing my features it can be both numerical and non numerical featuresit might be difficult to set up my ifthenelse rules by hand so in that scenario i can try to use some sort of machine learning technique to learn this decision tree in the literature there are lots of such algorithms available that given a data and a set of features we will construct a decision tree for you so i will just give you the names of some of the algorithms and the basic idea on this they work is that at every point we have to choose a particular split so you have to choose a feature value that it splits my data into certain parts and i have certain criteria to find out what is the best way to split so one particular criterion is what is the information given by this so these algorithms that we have mentioned here like id3 c45 cart they all use one of these criterions in general once you have identified what are your interesting features for these tasks you are not limited to only one classifier a decision tree you can also try out some other classifiers likesupport vector machines logistic regression and neural networks these all these are quite popular classifiersfor various analytic applications so we will talk about some of these as we will go to some advanced topics in this coursenow coming back to our problem tokenization we said that tokenization is a process of segmenting a string of characters into words finding out what are the different words in this question now remember we talked about token and type distinctionsuppose i give you a simple sentence here i have a can opener but i cannot open these cans how many tokens are there if you count there are 11 different occurrences of words so you have 11 word tokens but how many unique words are there so you will find there are only 10 unique words which word repeats is the word i repeats twice so there are 10 types and 11 tokens so my tokenization is to find out each of the 11 word tokens from the sentencein practice at least for english you can use certain toolkits that are available like nltk in python corenlp in java and you can you can also use the unix commands so in this course you will mainly be using nltk toolkit for doing all pre processing task and in some other tasks as well but in general you can use any of these three possibilitiesso for english most of the problems that we will see are taken care of the tokenizers that we have discussed previously but still it is good to know what are the challenges that are involved when i tried to design a tokenization algorithmsee for example here you will see that if i encounter a word like finland’s in my data so one question that i have is whether i treat it as simple finland as it is finland’s or i convert it to finland’s by removing the apostrophe so this question you might also try to defer to the next processing step that you will see but sometimes you might want to tackle this in the same refer time 1714 similarly if you see what are do i treat it as a single token or two tokens what are this trouble you might have to solve in the same step whether i treat it as a single token or multiple tokens same with i am should not and so on similarly whenever your name end at each like san francisco should i treat it as a single token or two separate tokens now remember when we were talking about some of the cases why refer time 1745 hard so you might have to find out that this particular sequence of tokens is a single entity and treat it as a single entity not as multiple different tokens so this problem is related similarly if you find m dot p dot h whether you call it a single token or multiple tokens so now there are no fixed answers to these and some of these might depend on what is the application for which you are doing this pre processing but one thing you can always keep in mind suppose you are doing for the application of information trivial if the same sort of steps that you apply for your documents should be applied to your query as well otherwise you will not be able to match them perfectly suppose if i am using it for information trivial so i should use the same convention for both my documents as well as the queriesso then another problem can be how do i handle hyphens in my day this looks again a simple problem but we will see it is not that simple so let us see some kind of examples what are the various sorts of hyphens that can be there in my corpus so here i have a sentence from a research paper abstract and the sentence says this paper describes mimic an adaptive mixed initiative spoken dialogue system that provides movie showtime information so in this sentence itself you see two different hyphens one is with initiative another is show hyphen time so now can you see that these two are different hyphens the first hyphen is not in general that i will use in my text second hyphen i can used in my text i can write show time with an hyphen but how did this hyphen initiative came into the corpus so we have given this a title end of line hyphen so what happens in research papers for example whenever you write a sentence you might have to do some sort of justification and that is where you end the line even if is not the end of this of the word so you will you will end up with in hyphen so now when you are trying to pre process and when you are retrieving such kind of hyphens you might have to join these together and you should you have to say that this is a single word initiative and not initia hyphen tive but again this is not trivial because for show time you will not do the same show time you might want to keep it as it is then there are some other kinds of hyphens like lexical hyphens so you might have these hyphens with various prefixes like co pre meta multi etcetera sometimes they are sententially determined hyphens also that is they put hyphens so that it becomes easier to interpret the sentence like here casebased handdelivered etcetera are optional similarly if you see in the next sentence threetofiveyear direct mark marketing plan three to five year can be written perfectly without keeping the hyphens but here you are putting it so that it becomes easier to interpret that particular occurrence again when you are doing tokenization your problem that how do i handle all these hyphensfurther there are various issues that you might face for certain languages but not others for an example like in french if you have a token like ensemble so you might want to match it with ensemble so that might be a similar problem that we are facing in english but let us take something in german so i have this big sentence here but the problem is that this is not a single word this is a compound composed of four different words and the corresponding english meaning is this one so you have four words in english so when you are putting in french they make a compound so now what is the problem that you will face when you are processing the german text and you are trying to tokenize it so you might want to find out what are the individual words in this particular compound so you need some sort of compound split up for german so the problem is there for german not so much for englishso now what happens if i am making a language like chinese or japanese so here is a sentence in chinese so what do you see in chinese words are written without any spaces in between now when you are doing the pre processing your task is to find out what are the individual word tokens in this chinese sentence this problem is also difficult because in general for a given utterance of a sequence of characters there might be more than one possible ways of breaking into sequence of words and both might be perfectly valid possibilities so in chinese we will not have not have any space between words and i have to find out what are the places where i have to break these words and this problem is called word tokenization same problem happens with japanese and here for the complications because they are using four different steps like katakana hiragana kanji and romaji so these problems become a bit more severenow the same problem is there even for sanskrit so if some of you have taken a sanskrit course in your class 8th or 10th you might be familiar with the rules of sandians in sanskrit language so that is it this is a simple single sentence in sanskrit but this is a huge this looks like a sing single word it is not a single word it is composed of multiple words in sanskrit and they are combined with a sandi relationthis stands for nice proverb in sanskrit that translates in english as one should tell the truth one should say kind words one should neither tell harsh truths nor flattering lies this is a rule for all times this is a proverb and this is a single sentence that talks about this proverb but there all the words are combined with sandi relation so if we try to undo the sandi this is what you will find at the segmented text so there are multiple words in this sentence they are combined to make a single it looks like a single word so this problem we saw in chinese japanese and sanskrit but in sanskrit the problem is slightly more complicated and why is that so in japanese and in chinese when you try to combine various words together you simply concatenate them you put them one after another without making any changes at the boundary it does not happen in sanskrit when you combine two words you also make certain changes at the boundary and this is called the sandi operationso in this particular case since see here i have the word ‘bruyat’ and the word ‘na’ but when i am combining i am writing it ‘bruyanna’ so you see here the letter ‘t’ gets changed to ‘n’ that means when i am trying to analyze the sentence so this particular sentence in sanskrit i need to find out not only what are the breaks but what is the corresponding word from which this sentence you derived so from here to find out the actual words are bruyat lesna that gives me this ‘bruyat’ and this is very very common in sanskrit that you are always combining words by doing a sandi operation so this further complicates my problem of word tokenization or segmentationso this is just a list from wikipedia what are the longest words in various languages then note this sentence is about the words you see in sanskrit the longest word is composed of 431 characters it is a compound and then you have greek and afrikaans and other languages in english you will see that the longest word is of 45 characters is nonscientificso what is the particular word in sanskrit that is composed of 431 letters so this was from the varadambika parinaya campu by tirumalamba this is a single compound from his bookso now when i talk about this problem of tokenization in sanskrit or in english this problem is also called word segmentation have a sequence of characters and you segment it to find out individual words now what is the simplest algorithm that you can think off let us take as in the case of chinese so the simplest algorithm that works is a greedy algorithm that is called maximum matching algorithm so whenever you are given a string you start you point to it at the beginning of the string now suppose that you have the dictionary and the words that you are currently seeing all should be the in the dictionary so you will find out what is the maximum match as per my dictionary in the string you break there and put the pointer from at the next character and again do the same thing so this greedily chooses what are actual words by taking the maximum matches and this works nicely for most of the cases so thisnow can you think of some cases where the segmentation will also be required for the english text in english in general we do not combine words to make a single word we do not do that but what is the scenario where we are doing that right now so does do hash tags come into mind for example suppose i have hash tags like thank you sachin and music monday so here different words are combined together without putting a boundary in between so if you are given a hash tag and you have to analyze that you have to actually segment it into various wordsso when i talk about sanskrit so this we have a segment to available at the site sanskrit dot inria dot fr so we will just briefly see what is the design principle of building a segmentor in sanskrit so first we have a geometry model that says how do i generate a sentence in sanskrit i have a finite alphabet sigma that means a set of various characters in sanskrit now from this finite alphabet i can generate a lot words that are composed of various number of phonemes or all letters from this alphabet now when i have a set of words i can now combine them together with an operation of sandi that is what i mean by sigma star here so w star here so i have a set of words w and i will do a cleaner closure that means i can combine any number of words together but whenever i am combining words i am doing them by a sandi operation this is the relation between the words so i have my set of inflected words also called padas in sanskrit and i have the relation of sandi between them and that is how i generate sentencesbut the problem is how do i analyze them so that is the inverse problemthat is whenever i am given a sentence w i have to analyze it by inverting the relations of sandi so that i can produce a finite set of word forms w 1 to w n and i am saying together with the proofs so that is a formal way of saying that but what i mean is that w 1 to w n whenever they combine by sandi operation they give me the actual sandi the initial sandi’s so that is how the segment is builtnow this is a snapshot from the segmentor so i gave the same sentence there and it gave me all the possible ways of analyzing the sandi’s and it says that there are 120 different solutions so here whenever i have bruyana so you see there are two possibilities bruyat and bruyam that is like that it gives me all the possible ways in which this sentence can be broken into individual word tokens now this is another problem that i will have to find out what is the most likely word sequence among all these 120 possibilitiesbut we can use many many different models that we will not talk about in this lecture probably in some other lecturesso coming back to normalization we talked about this problem that the same word might be doing multiple different ways like u dot s dot a versus usa now i should be able to match them together especially if you are doing information retrieval we are giving a query and you are retrieving from some document suppose your query contains u dot s dot a if the document contains usa if you are only doing the surface able match you will not be able to map on to each other so you will have to consider this problem in advance and do the pre processing accordingly of either your documents or the query but using the same sort same sentence so what we are doing by this we are defining some sort of equivalence classes we are saying usa and u dot s dot a should go to one class and the other same typewe also do some sort of case folding that is we can reduce all the letters to lower case so whenever i have the word like w o r d i will always write small w o r d so that whenever even if it is starting the sentence and it occurs in capitals because of that in general i know that this is a word w o r d but this is not a generic rule sometimes depending on application you might have certain exceptions for example you might put treat the name and it is separately so if you have entity general motors you might want to keep it as it is without case foldingsimilarly you might want to keep us for united states in upper case and not do the case folding and this is important for the application of machine transition also because if you do a case folding here you will know u s in lower case that means something else versus us that is in united states excuse mewe also have the problem of lemmatization that is you have individual words like am are is and you want to convert them to their lemma that means what is the base form from which they are derived similarly car cars car’s cars’ so all these are derived from car again this is some sort of normalization we are saying all these are some sort of equivalence class because they come from the same word from so in the problem of lemmatization is that you have to find out the actual dictionary head word from which they have derivedand for that we use morphology so what is morphology i am trying to find out the structure of word by seeing what is the particular stem the headword and what is the affix that is applied to it so these individual units are called various morphemes so you have a stems that are thehybrids and the affixes that are what are the different units like as for plural etcetera you are applying to them to make the individual word so my examples are like for prefix you have un anti etcetera for english and a ati pra etcetera for hindi or sanskrit suffix like ity ation etcetera and taa ka ke etcetera for hindi and in general you can also have some infix like you have the word like vid and you can infix n in between this is in sanskrit so we will discuss in detail about it in morphology later so there is another concept you have lemmatization where you are finding the actual dictionary headword so there is also a concept called stemming where you do not try to find the actual dictionary headword but you just try to remove certain suffixes and whatever you obtain is called a stem so this crude chopping of various affixes in that wordso this is again language dependent so what we are doing here words like automate automatic automation all will be reduced to a single lemma automatically so this is stemming so you know the actual lemma is automate with an e but here i am just chopping off the affixes at the end so i am removing here this ic ion all and putting it to automate so this is one example if you try to do a stemming here see you will find from example e is removed from compressed ed is removed and so on so what is the algorithm that is used for this stemmingso we have the porter’s algorithm that is very very famous and this is again some sort of ifthenelse rules so what are some examples here what is the first step i take a word if it ends with sses i remove es from there and i end with ss so example is caresses goes to caress if not then i see whether the words end with ies i put it to i like ponies goes to poni if not i see if the word ends with ss i keep it as ss if not i see if the word ends with s i remove that s cats goes to cat but caress does not go to caress with only one s because this is step comes before if there is a double s and in the word i written it otherwise if there is a single s i remove it that like that there are some other steps so if there is a vowel in my word and the word ends with ing i remove ing so walking goes to walk but what about king you see in k there is no vowel so king will be written as it is same is a vowel and there is an ed i remove this ed and i have this word played to play so you can see that what is the use of this heuristic of having this vowel if you did not have this vowel you would have converted king to kand like that there are some other ways like if the word ends with ational then i will put it put ate so rational so relational to relate and if the word ends with izer i convert i remove that r digitizer to digitize ator to ate and if the word ends with al i remove that al if the word ends with able i remove that able if the word ends with ate i remove that ate so like that these are some steps that i take from my corpus from each word i convert it to its step it does not give me the correct dictionary headword but still this is a good practice in principle for information retrieval if you want to match the query with the documents this is for this week next week we will start with another pre processing task that is a spelling correction thank you'"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"cleaned_hypothesis","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:02:07.745293Z","iopub.execute_input":"2025-01-22T17:02:07.745578Z","iopub.status.idle":"2025-01-22T17:02:07.763139Z","shell.execute_reply.started":"2025-01-22T17:02:07.745554Z","shell.execute_reply":"2025-01-22T17:02:07.762161Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'hello everyone welcome back to the final lecture of the first week so in the last lecture we were discussing about various empirical laws so in particular gifs law and heaps law that how the words in the vocabulary are distributed in a corpus and we saw that the distribution is not very uniform there are certain words that are very very common so we saw that roughly 100 words in the vocabulary make for 50 percent of the corpus but by that i mean that number of tokens and on the other hand there are 50 percent of the words in the vocabulary that occur only once and we discussed what are the various relationships among the vocabulary size and the number of tokens that i observe in a corpus and also how they grow with us to each other and gifs law gives gave me a relation between the frequency and the rank of a word so today in this lecture we will start with the basic key processing in language so we will cover the basic concepts and what are the challenges that one might face while doing the processing so we are going to the basics of text processing so we will start with the problem of tokenization as the name would suggest remember the name token token is an individual word in my corpus so now what happens when i am pre processing the text in given in any language what i will face is a string of characters the sequence of characters now i need to identify what are all the different words that are there in this sequence so tokenization is the process by which i convert the string of characters into sequence of various words so i am trying to segment it by the various words that i am observing so now before going into what is tokenization i will just talk about a slightly related problem sentence segmentation so this you may or may not have to do always and it depends on what is your application so for example suppose you are doing classification for the whole document into certain classage you might not have to go to the individual sentence and you can just talk about what are various words that are present in this document on the other hand suppose you are trying to find out what are the important sentences in this document in that application you will have to go to the individual sentence so now if you have to go to the individual sentence the first task that you will face is how do i segment this whole document into a sequence of sentences so this is sentence one sentence two and so on and this task is called sentence segmentation so now you might feel that this is very trivial task but let us see is it trivial so what is sentence segmentation so problem of deciding where my sentence begins and ends so that i have a complete unit of words that i call as a sentence now do you think there might be certain challenge involved suppose i am talking about the language english can you always say that wherever i have a dot it is the end of the sentence that is it so there are many ways in which i can end my sentence so i can have exclamation or question mark that ends the sentence and they are mostly an ambiguous so whenever i have a exclamation or question mark i can say probably this is the end of the sentence but is the case the same with a dot so can you think of a scenario where i have a dot in english and but it is not the end of sentence so you can find all sorts of abbreviations they end with a period doctor mr mph so you have three dots here so you cannot call each of the this as the end of your sentence so again you have numbers 2443 and so on so that means the problem of deciding whether a particular dot is the end of the sentence or not is not entirely trivial so i need to build certain algorithm for finding out is it my end of the sentence so in text processing if we face this kind of problem in nearly every simple task that we are doing so even if it looks at trivial task we face this problem that can i always call dot as the end of the sentence so how do we go about solving this now if you think about it whenever i see a dot or question mark or exclamation i always have to decide one of the two things is it the end of the sentence or is it not the end of the sentence so any data point that i am seeing i have to divide into one of these two classes so if you think of these as two classes end of the sentence or not end of the sentence so each point you have to divide into one of the two classes and this in general this problem in general is called classification problem you are classifying into one of the two classes now so the idea is very simple so you have two classes and each data point you have to divide into one of the two classes so that means you have to build some sort of rule of algorithm for doing that so in this case i have to build a binary classifier what do i mean by a binary classifier there are two classes end of the sentence or not end of the sentence in general there can be multiple classes so now for each dot or in general for every word i need to decide whether this is the end of the sentence or not the end of the sentence so in general my classifiers that i will build can be some rules that i write by hand some simple iftenelse rules or it can be some expressions i say if my particular example matches with this sexual expression it is one plus if it does not match it is another class or i can build a machine learning classifier so in this particular scenario what can be the simplest thing to do let us see can we build a simple rulebased classifier so we will start with the example of a simple decision tree so by decision tree i mean a set of iftenelse statements so i am at a particular word i want to decide whether this is the end of the sentence or not so i can have this simple iftenelse kind of decision tree here so i am at a word and the first thing i check is are there lots of blank lines after me so this would happen in a text whenever this is the end of the paragraph and there are some blank lines so if i feel that there are a lot of blank lines after me that means after this word i may say this might be the end of the sentence with a good confidence so that is why the branch hearsage yes this is the end of the sentence but suppose they are not a lot of blank lines then i will check if the final punctuation is equation mark or explanation or equivalent in that case so they are quite an ambiguous i will say this is the end of the sentence now suppose it is not then i will check if the final punctuation is a period so if it is a period if it is not a period it is easy to say that this is not the end of the sentence but suppose this is a period so again i cannot say for certain if it is the end of the sentence so i will again check for simplicity i might have a list of abbreviations and i can check if the word that i am currently facing is one of the abbreviations in my list if it is there i will say this is not the two sentence if it is so here i am accept or any other abbreviation if the answer is yes i am not an order sentence if the answer is no that means this word is not an abbreviation and this will be the end of the sentence so this is very very simple if then as rules this may not be correct but this is one particular way in which this problem can be solved in general you might want to use some other sort of indications we call them as various features they are various observations that you make from your corpus so what are some examples so suppose i see the word that is ending with dot can i use this as a feature whether my word starts within uppercase lowercase cap all caps all is in number how will it help so let us see i have here i am here and my word is 43 so i am at dot i want to find out if it is the end of the sentence if i can say that the previous the current word is a number it is a high probability that this will be in number and it will not be the end of the sentence so this can be used as another feature so again by feature you can think of a simple rule whether the word i am currently at is a number or i can use the fact where the case of the word with dot is uppercase or lowercase so what happens generally in abbreviations we are mostly in uppercase so suppose i have doctor and it starts with an uppercase i can say that this might be an abbreviation same with lowercase lowercase will give me more probability that this is not an abbreviation similarly i can also use the case of the word after dot so is it uppercase lowercase capital or number so how will that help so again whenever i have the end of the sentence the next word in general starts with a capital so again this can be used what can be some other features so i can have some numerical features so that is i will have certain thresholds what is the length of the word ending with dot is it if the length is small it might be an abbreviation if the length is larger it might not be an abbreviation and i can also use probably the what is the probability that the word that is ending with dot occurs at the end of the sentence so if it is really the end of the sentence it might happen that in a large corpus this end sentence quite often same thing i can do with the next word after dot is it the start of the sentence what is the probability that it occurs in the start of sentence in a large corpus so you might be able to use any of these features to decide given a particular word is it the end of the sentence or not so now suppose i ask you this question do you have the same problem in other languages like hindi so in hindi you will see that in general there is only a dunder that you use to indicate the end of the sentence and this is not used for any other purpose so this problem you will see is again language dependent this problem is there for english but not so for hindi but we will see there are other problems that do not exist for english language but are there for other indian language now we will see some of the examples in the same lecture now so how do we implement it recently so as you have seen this is a simple if then else statement so now what is important is that you choose the correct set of features so how do you go about choosing the set of features you will see in your from your data what are some observations that can separate my two classes here so my two classes here are end of the sentence and the end of the sentence and what are the observations we were having in general it might be an abbreviation the case of the word and that it is before the dot may be uppercase or lower case and one of these might indicate one class other might indicate other class so all these are my observations that i use as my features now whenever i am using a numerical features like the length of the word before dot i need to pick some sort of threshold that is whether the length of the word is between 2 to 3 or say more than 3 between 5 to 7 like that so my 3 can be if the length of the word is between 5 to 7 i go to one class otherwise i go to another class so now here is one problem suppose i keep on increasing my features this can be both numerical or nonunimegal features it might be difficult to set up my if then else rules by hand so in that scenario i can try to use some sort of machine learning technique to learn this dictionary in the literature there are lot of such algorithms that are available that give any data and a set of features will consider a dictionary for you so i will just give you so the names of some of the algorithms and the basic idea on this they work is that so at every point you have to choose a particular split so you have to choose a feature value that is splits my data into certain parts and i have certain criteria to find out what is the best split so one particular criteria is what is the information gain by this so these algorithms that we have mentioned here like id3 c415 and c4 they all use one of these criteria in general once you identify what are your interesting features for this task you are not limited to only one classifier like dictionary you can also try out some other classifiers like support vector machines large regression and neural networks all these are quite popular classifiers for various nlp applications so we will talk about some of these as we will go to some some advanced topics in this course now coming back to a problem of tokenization we said that the tokenization is a process of segmenting a string of characters in two words finding out what are the different words in this issue now remember we talked about token and type distinction so suppose i give you a simple sentence here i have a can opener but i cannot open these cans how many tokens are there if you count there are 11 different words 11 different words 11 different occurrences of words so you have 11 word tokens but how many unique words are there so you will find there are only 10 unique words which word repeats say the word i repeats twice so there are 10 types and 11 tokens so my tokenization is to find out each of the 11 word tokens from the sentence in practice at least for english you can use certain tool kits that are available like analytic in python coronal pin java and you can also use some unique commands so in this course we will mainly be using analytic toolkit for doing all these pretastic task and some other tasks as well but in general you can use any of these three possibilities so for english most of the problems that we will see are taking care of by the token areas that we have discussed previously but still it is good to know what are the challenges that are involved when i have tried to design a tokenization algorithm so for example here you will see that i if i encounter a word like fill lands in my data so one question i have is whether i treated as simple fill land as it is fill lands or i converted to fill lands by removing the first of it so this question you might also try to defer to the next processing step that we will see but sometimes you might want to tackle this in the same step similarly if you see what are do i treated as a single token or two tokens what are this problem you might have to solve in the same step whether i treated as a single token or multiple tokens same with i am shouldnt and so on similarly whenever we have named and it is like san francisco should i treated as a single token or two separate tokens now remember when we were talking about some of the cases y and l page hard so you might have to find out this particular sequence of tokens is a single entity and treated as a single entity not as multiple different tokens so this problem is related similarly if you find m dot p dot h do you call it a single token or multiple tokens so now they are no fixed answers to these and some of these might depend on what is the application for which you are doing this pre processing but one thing you can always keep in mind suppose you are doing it for the application of information tree well the same sort of steps that you apply for your documents should be applied to your query as well otherwise you will not be able to match them perfectly so suppose if i am using it for information tree well so i should use the same convention for both my documents as well as the query so then another problem can be how do i handle hyphens in my data so this looks again a simple problem but we will see it is not that simple so let us see some kind of examples what are the various sorts of hyphens that can be there in my corpus so here i have a sentence from a research paper abstract and the sentence says this paper describes mimic an adaptive mixed initiative spoken dialogue system that provides movie show time information so in the sentence itself you see two different hyphens one is with initiative initiative another is show hyphen time so now can you see that these two are different hyphens the first hyphen is not in general that i will i will use in my text second hyphen i can use in my text i can write show time within hyphen but how did this hyphen initiative came into the corpus so we have given this title end of line hyphen so what happens in research papers for example whenever you write a sentence you might to do some sort of justification and that is where you end the line even if it is not the end of the word so you will end up within hyphen so now when you are trying to pre process and when you are retrieving such kind of hyphens you might have to join these together and you say you have to say that this is single word initiative and not initiate hyphen tape but again this is this is not trivial because for show time you will not do the same show time you might want to keep it as it is then there are some other kind of hyphens like lexical hyphens so you might have these hyphens with various prefixes like copie meta multi etcetera sometimes they are sententially determined hyphens also that is you put hyphens so that it becomes easier to interpret the sentence like here case paste handle evoked etcetera are optional similarly if you see in the next sentence 3 to 5 year direct marketing plan 3 to 5 year can be written perfectly without keeping the hyphens but here you are putting it so that it becomes easier to interpret that particular occurrence so again when you are doing tokenization you have problem at how do i handle all these hyphens further there are various issues that might that you might face for certain language but not others so for example is like in french if you have a token like lunch ensemble so you might want to match it with ensemble so that might be a similar problem that you are facing in english but let us take something in german so i have this i have this big sentence here but the problem is that this is not a single word this is a term bound composed of four different words and the corresponding english meaning is this one so you have four words in english when you are putting in french they make a compound so now what is the problem that you will face when you are processing the german text and you are trying to tokenize it so you might want to find out what are the individual words in this particular compound so you need some sort of compound splitter for german so this problem is there for german not so much for english so now what happens if i am taking a language like chinese or japanese so here is a sentence in chinese so what do you see in chinese words are written without any spaces in between so now when you are doing the prefossessing your task is to find out what are the individual words tokens in this chinese sentence so this problem is also difficult because in general for a given utterance of a sequence of characters they might be more than one possible ways of breaking into sequence of words and both might be perfectly valid possibilities so in chinese we do not have any space between words and have to find out what are the places where i have to break these words and this problem is called tokenization word tokenization same problem happens with japanese and you have further complications because they are using four different scripts like katakana hiragana kanjana rumanji so this problem becomes a bit more severe now the same problem is there even for sanskrit so if some of you have taken sanskrit course in your class 8th or 10th so you might be familiar with the rules of sanskrit language so let us say this is a simple single sentence in sanskrit but this is a huge this looks like a simple word but it is not a single word it is composed of multiple words in sanskrit and they are combined with this on the relation this stands for a nice proverb in sanskrit that translation english as one should tell the truth one should say kind words one should neither tell harsh truth not flatly flattening lies this is a rule for all times this is a proverb and this is a single sentence that talks about this proverb but where all the words are combined with some relation so if you try to undo the sun this is what you will find at the segmented text so there are multiple words in this sentence they are combined to make a single it looks like a single word so this problem is so in chinese japanese and sanskrit but in sanskrit the problem is slightly more complicated and why is that so in japanese and in chinese when you try to combine various words together you simply concatenate them you put them one after another without making any changes at the boundary it does not happen in sanskrit when you combine two words you also make certain changes at the boundary and this is called a sanda operation so in this particular case you see here i have the word briyath and the word na but when i am combining i am writing it briyana so you see here the letter t gets changed to n so that means when i am trying to analyze the sentence so this purpose sentence in sanskrit i need to find out not only what are the breaks but what is the corresponding word from which this sentence is right so from here i have to find out the actual words are briyath plus na that gives me this briyana and this is very very common in sanskrit that you are always combining words by doing sanda operation so this further complicates my problem of word segmentation or segmentation of so this is just a list from wikipedia what are the longest words in various languages not the sentence is what the words so you see in sanskrit the longest word is composed of 431 characters it is the compound and then you have greek and african and other languages in english you see the longest word is of 45 characters this is non scientific so what is the particular word in sanskrit that is composed of 431 letters so this was from the word ambika parinaya champu by tirum lamba this is a single compound from his book so now when i talk about this problem of tokenization in sanskrit or in english this problem is also called word segmentation i have a sequence of characters and you segment it to find out individual words now what is the simplest algorithm that you can think of let us say it is a case of chinese so the simplest algorithm that works is eglidi algorithm that is called maximum matching algorithm so whenever you are given a string you start your pointer at the beginning of the string now suppose that you have the dictionary and the words that you are that you are currently seeing all should be in the dictionary so you will find out what is the maximum match as per my dictionary in the string you break there and put the pointer from at the next correct and again do the same thing so this griddle chooses what are the actual words by taking the maximum match and this works nicely for most of the cases now so this silly iteration now can you think of some cases where the segmentation will also be required for english text in english in general we do not combine words to make a single word we do not do that but what is the scenario where we are doing that right now so does do hashtags come into mind so for example suppose i have hashtags like thank you searching in music monday so here different words are combined together without putting a boundary in between so if you are given a hashtag and you have to analyze that you have to actually segment it into various words now so when i talk about sanskrit so this we have a segment are available at the site sanskritngfr so we will just briefly see what is the design principle of building a segment or in sanskrit so first we have a generative model that says how do i generate a sentence in sanskrit i have a finite alphabet sigma that means a set of various characters in sanskrit now from this finite alphabet i can generate a lot of words that are composed of various number of phonemes or letters from this alphabet now when i have a set of words i can now combine them together with an operation of sunday that is what i mean by sigma star here so w star here so i have a set of words w and i do a clean it words here that means i can combine any number of words together but whenever i am combining words i am doing them by a sunday operation because there is a relation between the words so so i have my set of inserted words also called padaj in sanskrit and i have the relation of sunday between them and that is how i generate a sentence but my problem is how do i analyze them so that is the inverse problem that is whenever i am given a sentence w i have to analyze it by inverting the relations of sunday so that i can produce a finite set of word forms w 1 to w n and i am saying together with a proof that is a formal way of saying that but what i mean is that w 1 to w n when i they combine by sunday operation they give me the actual sentence the initial sentence so that is how the segmentarage is segmentarage built now this is a snapshot from the segmentar so i gave the same sentence there and it gave me all the possible ways of analyzing the sentence and it says that there are 120 different solutions so here whenever i have brianna so you see there are two possibilities briath and briam plus now like that it gives me all the possible ways in which the sentence can be broken into individual word tokens now this is another problem that i will have to find out what is the most likely word sequence among all these 120 possibilities but we can use many many different models that we will not talk about in this lecture probably in some some other lectures so coming back to normalization so we talked about this problem that the same word might be written multiple different ways like u dot s dot a versus u s a now i should be able to match them together especially if you are doing information retrieval you are giving a query and you are retrieving from some document suppose you query contains u dot s dot a and the document contains u s a if you are only doing the surface level match you will not be able to map them to each other so that so you will have to consider this problem in advance and do the preversus in accordingly of either your documents or the query but using the same sort same settings so what we are doing by this we are defining some sort of equivalence class h we are saying u s a and u dot s dot a should go to one class and they are the same type we also do some sort of case folding that is we can reduce all that as to lower case so whenever i have the word like w r d i will always write small w r d so that whenever even if it is starting the sentence and it occurs in capitals because of that in general i know that this is a word good w r d but this is not a generic rule sometimes depending on application you might have certain exceptions for example you might have to treat the name and its separately so if you have a entity general model you might want to keep it as it is without case folding similarly you might want to keep u s for united states in upper case and not do the case folding and this is important for the application of machine translation also because if you do a case folding here you will know u s in lower case that means something else was such u s that is in united states we also have the problem of lamtaization that is you have individual words like m r is and you want to convert them to their lemma that means what is the base form from this they are derived similarly car cars cars cars so all these are derived from car so again this is some sort of nobulation you are saying all these are some sort of equivalence class because they come from the same word form so in the problem of lamtaization is that you have to find out the actual dictionary head word from this they are derived and for that we use morphology so what is morphology i am trying to find out the structure of a word by seeing what is the perqueous stem the head word and what is the affix that i apply to it so these individual units are called various morphim so you have which stems that are the the originally head words and the affix is that are what are the different units like as for plural etcetera you are applying to them to make the individual word some examples are like for prefix you have unante etcetera for english and a teapra etcetera for hindi or sanskrit suffix like etiation etcetera and tarqa k etcetera for hindi and in general you can also have some infix like you have a word like width and you can fix an in between this is in sanskrit so we will discuss in detail about it in morphology later so there is another concept you have lamtaization where you are finding the actual dictionary head word so this is also concept called stemming where you do not try to find the actual dictionary head word but you just try to remove certain suffixage and you or whatever you obtain is called stem so this is a crude chopping of various suffixage in that word so this is again language dependent so what we are doing here words like automate automatic automation all will be reduced to a single lemma automate here so this is stemming so you know the actual lemma is automate within e but here so i am just chopping off the affixage at that so i am removing here this i c i o n all and putting it to automate so this is one example so if you try to do a stemming here this is you will find from example e is removed from compressed edged removed and so on so what is the algorithm that is used for for this stemming so we have the portus algorithm that is very very famous and this is again simpsons set of if the niles rules so what are some examples here so what is the first step i take a word if it ends with s s e s i remove e s from there and i end with s s so example is carousers goes to carous if not then i see whether the word ends with i s i put it to i like ponies goes to ponies if not i see if the word ends with s s i keep bidage s s if not i see if the word ends with s i remove that s so cats goes goes to cat but carous does not go to carousers with only one s because this step comes before if there is a double s ending the word i retain it otherwise if there is a single s i remove that like that there are some other steps so if there is a vowel in the in my word and the word ends with i and g i remove i and g so walking goes to walk but what about king you see in in k there is no vowel so i will be retained as it is same is a vowel and there is an e d i remove this e d and i have this what played to play so you can see that what is the use of this heuristic of having this vowel if you did not have this vowel you would have converted king to k and like that there is some other rules like if the word ends with aishinal then i put a t e so relational so relational to relate and if the words and what ends with i z r i can i remove that r digitize i z digitize a t e r to a t e and if the word ends with l i remove that l if the word ends with able i remove that able if the words and with a t e i remove that a t e so like that these are some steps that i take from my coppers for each word i converted to its its time it does not give me the correct dictionary had word but it is still this is a good practice in principle for information tree if you want to match the query with the documents so this is for this week so next week we will start with another pto sinsing task that is spilling correction thank you'"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"# EXAMPLE2","metadata":{}},{"cell_type":"code","source":"def import_txt_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        content = file.read()\n    return content\n\nreference_file_path = \"/kaggle/input/transcripts/reference_nptel_transcription_2.txt\"\nreference = import_txt_file(reference_file_path)\n\nhypothesis_file_path= \"/kaggle/input/transcripts/transcribed_text_nptel_video_2.txt\"\nhypothesis=import_txt_file(hypothesis_file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:02:07.764297Z","iopub.execute_input":"2025-01-22T17:02:07.764601Z","iopub.status.idle":"2025-01-22T17:02:07.790438Z","shell.execute_reply.started":"2025-01-22T17:02:07.764576Z","shell.execute_reply":"2025-01-22T17:02:07.789292Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import re\nimport string\n\ndef preprocess_text(text):\n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    \n    # Remove newlines and extra spaces\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Strip leading and trailing spaces\n    text = text.strip()\n    \n    return text\n\ncleaned_reference = preprocess_text(reference)\ncleaned_hypothesis = preprocess_text(hypothesis)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:02:07.791685Z","iopub.execute_input":"2025-01-22T17:02:07.792056Z","iopub.status.idle":"2025-01-22T17:02:07.807450Z","shell.execute_reply.started":"2025-01-22T17:02:07.792015Z","shell.execute_reply":"2025-01-22T17:02:07.806255Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from jiwer import wer\n\nerror_rate = wer(cleaned_reference, cleaned_hypothesis)\nprint(\"Word Error Rate:\", error_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:02:07.808754Z","iopub.execute_input":"2025-01-22T17:02:07.809158Z","iopub.status.idle":"2025-01-22T17:02:07.845301Z","shell.execute_reply.started":"2025-01-22T17:02:07.809118Z","shell.execute_reply":"2025-01-22T17:02:07.844226Z"}},"outputs":[{"name":"stdout","text":"Word Error Rate: 0.14123109180769808\n","output_type":"stream"}],"execution_count":10}]}