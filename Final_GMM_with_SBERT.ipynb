{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shruthimohan03/video-summarizer/blob/main/Final_GMM_with_SBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install the dependencies"
      ],
      "metadata": {
        "id": "rV6sc8NEKjL0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYXG5h3aZoIY",
        "outputId": "0a40ee17-5bf6-4fde-d2f7-b293a1192854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FINAL - NPTEL PYTHON VIDEO TEXT SUMMARIZATION"
      ],
      "metadata": {
        "id": "CXm_7O3WJr1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Import the preprocessed transcript"
      ],
      "metadata": {
        "id": "-vLYJHWwJ3L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/nptel_python_transcript_preprocessed.txt','r') as file:\n",
        "  text=file.read()"
      ],
      "metadata": {
        "id": "Lquv3ETyJrBM"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Split the transcript to sentences using NLTK"
      ],
      "metadata": {
        "id": "-BNgK9b8KHyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sentences = sent_tokenize(text)"
      ],
      "metadata": {
        "id": "4qeyaO9zKPTs"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Embed the sentences using SBERT"
      ],
      "metadata": {
        "id": "XXjKVqgcKoU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "sentence_embeddings = model.encode(sentences, convert_to_tensor=False)"
      ],
      "metadata": {
        "id": "wbGWA3E_Jq9o"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Using BIC to find the optimal number of clusters"
      ],
      "metadata": {
        "id": "1rFxFlJiLRPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Smart Thresholding depending on the number of sentences present in the transcript"
      ],
      "metadata": {
        "id": "vWy6aPW-MugP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def get_cluster_range_percent(N, min_ratio=0.05, max_ratio=0.2):\n",
        "    min_c = max(3, math.ceil(N * min_ratio))\n",
        "    max_c = min(N - 1, math.ceil(N * max_ratio))\n",
        "    return (min_c, max_c)"
      ],
      "metadata": {
        "id": "u1SxNo1GMtNN"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Range of clusters for trial and error"
      ],
      "metadata": {
        "id": "SCuqJuBaNT61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Range of clusters to try\n",
        "N = len(sentences)\n",
        "clusters=get_cluster_range_percent(N)\n",
        "n_components = np.arange(clusters[0],clusters[1]+1)\n",
        "bics = []"
      ],
      "metadata": {
        "id": "HXR6RvSiNSfO"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gaussian Mixture Model is fixed for all the cluster sizes and BIC is calculated"
      ],
      "metadata": {
        "id": "LeN1W58sN4zT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "for n in n_components:\n",
        "    gmm = GaussianMixture(n_components=n, covariance_type='full', random_state=42)\n",
        "    gmm.fit(sentence_embeddings)\n",
        "    bics.append(gmm.bic(sentence_embeddings))"
      ],
      "metadata": {
        "id": "Dgy7mOVwJq7O"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plotting the BIC values"
      ],
      "metadata": {
        "id": "L1njUG6SOPRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(n_components, bics, marker='o')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('BIC Score')\n",
        "plt.title('BIC to choose optimal number of clusters')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "8zmLMnYtJqzM",
        "outputId": "f874076a-9c08-419f-cfaa-f839b0dd6737"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Finding the most optimal no of clusters"
      ],
      "metadata": {
        "id": "stiK3etdOTkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_clusters = n_components[np.argmin(bics)]\n",
        "print(\"Optimal number of clusters:\", optimal_clusters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG31ICTBJqvg",
        "outputId": "efbe275d-e061-4d62-98e8-6d1fe10c4bef"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal number of clusters: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Fit GMM using the optimal number of clusters"
      ],
      "metadata": {
        "id": "_RD8Kv7nnLC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_gmm = GaussianMixture(n_components=optimal_clusters, covariance_type='full', random_state=42)\n",
        "best_gmm.fit(sentence_embeddings)\n",
        "\n",
        "# Predict cluster labels\n",
        "labels = best_gmm.predict(sentence_embeddings)"
      ],
      "metadata": {
        "id": "18n6IJZmnQYl"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Extract one representative sentence per cluster (summary)"
      ],
      "metadata": {
        "id": "mmgh3pyenWrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from math import ceil\n",
        "\n",
        "summary_sentences = []\n",
        "seen_indices = set()\n",
        "\n",
        "# You can tune this ratio\n",
        "summary_ratio = 0.2\n",
        "min_sentences_per_cluster = 1\n",
        "max_sentences_per_cluster = 3\n",
        "\n",
        "for cluster_idx in range(optimal_clusters):\n",
        "    cluster_indices = np.where(labels == cluster_idx)[0]\n",
        "    cluster_size = len(cluster_indices)\n",
        "\n",
        "    # Skip clusters with only one sentence (likely noise)\n",
        "    if cluster_size <= 1:\n",
        "        continue\n",
        "\n",
        "    cluster_embeddings = np.array([sentence_embeddings[i] for i in cluster_indices])\n",
        "    cluster_sentences = [sentences[i] for i in cluster_indices]\n",
        "\n",
        "    # Get centroid of the cluster\n",
        "    centroid = best_gmm.means_[cluster_idx].reshape(1, -1)\n",
        "\n",
        "    # Rank all cluster sentences by similarity to centroid\n",
        "    similarities = cosine_similarity(centroid, cluster_embeddings)[0]\n",
        "    sorted_idx = np.argsort(similarities)[::-1]  # Descending order\n",
        "\n",
        "    # Determine how many sentences to extract from this cluster\n",
        "    num_to_select = min(\n",
        "        max(min_sentences_per_cluster, ceil(summary_ratio * cluster_size)),\n",
        "        max_sentences_per_cluster\n",
        "    )\n",
        "\n",
        "    selected_indices = sorted_idx[:num_to_select]\n",
        "\n",
        "    for idx in selected_indices:\n",
        "        global_idx = cluster_indices[idx]\n",
        "        if global_idx not in seen_indices:\n",
        "            summary_sentences.append((global_idx, sentences[global_idx]))\n",
        "            seen_indices.add(global_idx)\n",
        "\n",
        "# Sort by original sentence order for coherence\n",
        "summary_sentences = sorted(summary_sentences, key=lambda x: x[0])\n",
        "\n",
        "# Final summary\n",
        "summary = \" \".join([s[1] for s in summary_sentences])\n",
        "\n",
        "# Print summary\n",
        "for _, sent in summary_sentences:\n",
        "    print(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otOqSmcLnQVP",
        "outputId": "5499a957-26fd-4a0c-e722-5b276d553cca"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "So in this first introductory lecture I am just going to talk about why are we looking at Python for data science.\n",
            "So to look at that first we are going to look at what data science is.\n",
            "Data science is basically the science of analyzing raw data and deriving insights from this data.\n",
            "Nonetheless the key focus of data science is in actually deriving these insights using whatever techniques that you want to use.\n",
            "So when we talk about decisions the decisions could be across multiple verticals in an industry and data science is not only useful from an industrial perspective it is also useful in actual sciences themselves so where you look at lots of data to model your system or test your hypotheses or theories about systems and so on.\n",
            "So with this I will stop this brief introduction on why python for data science.\n",
            "I hope I have given you an idea of the fact that while we are going to teach you python as a programming language please keep in mind that each module that we teach in this is actually geared towards data science.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Looking into the clusters"
      ],
      "metadata": {
        "id": "xZApToWxoU6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Group sentences by their assigned topic\n",
        "clustered_sentences = defaultdict(list)\n",
        "for i, label in enumerate(labels):\n",
        "    clustered_sentences[label].append(sentences[i])\n",
        "\n",
        "# Print topics and their sentences\n",
        "for topic, sentences in clustered_sentences.items():\n",
        "    print(f\"\\n🟢 Topic {topic}:\")\n",
        "    for s in sentences:\n",
        "        print(f\"  - {s}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlv4pNY2nQSq",
        "outputId": "e8aae7e8-2103-4560-89b2-fd2138e82f31"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟢 Topic 1:\n",
            "  -  Welcome to this course on Python for Data Science.\n",
            "  - This is a 4-week course where we are going to teach you some very basic programming aspects in Python.\n",
            "  - So in this first introductory lecture I am just going to talk about why are we looking at Python for data science.\n",
            "  - Now why python for doing all of this the number one reason is that there are these python libraries which already are geared towards doing many of the things that we talked about so that it becomes easy for one to program and very quickly you can get some interesting outcomes out of what we are trying to do so there are as we talked about in the previous slide you need to do data manipulation and preprocessing there are lots of functions libraries in python where you can do data wrangling manipulation and so on.\n",
            "  - The next step we talked about visualization there are libraries in python which can be used to do the visualization and finally for the more sophisticated analysis that we talked about all kinds of machine learning algorithms are already pre-coded available as libraries in python so again once you understand some bit about these functions and once you get comfortable working in python then applying certain machine learning algorithms for these problems become trivial you simply call these libraries and then run these algorithms.\n",
            "  - At a higher level so in the previous slide we talked about flow process for how I get the data in, clean it and all the way up to insights and then parallely we said why python makes it easy for us to do all of this.\n",
            "  - If you go forward a little more and then ask in terms of the other advantages of python which are little more than just very simple data science activities python provides you several libraries and it is being continuously improved so anytime there is any algorithm those are coming into the set of libraries so in that sense it is very varied and there is also a good user community so if there are some issues with new libraries and so on those are fixed so that you get robust library to work with.\n",
            "  - And we talk about data and data can be of different scales so the examples that you will see in this course are data of reasonably small size but in real life problems you are going to look at data which is much larger which we call as big data so python has an ability to integrate with big data frameworks like Hadoop, Spark and so on and python also allows you to do more sophisticated programming object oriented programming and functional programming.\n",
            "  - Python with all of this sophisticated tools and abilities is still reasonably simple language to learn it is reasonably fast to prototype and it also gives you the ability to work with data which is in your local machine or in a cloud and so on.\n",
            "  - So these are all things that one looks for when one looks at a programming platform which is capable of solving problems in real life right so these are real problems that you can solve these are not only toy examples but real applications that you can build data science applications that you can build with python.\n",
            "  - And just as another pointer in terms of why I we believe that python is something that lot of our students and professionals in India should learn as you know there are tools which are paid tools for machine learning with all of these libraries and so on and there are also open source tools and in India based on a survey most people of course prefer open source tools for a variety of reasons cause being one because it is free to use but also if it just free to use but it does not have a robust user community then it is not really very useful that is where python really scores in terms of a robust user community which can help with people working in python.\n",
            "  - So it is both open source and there is robust user community both of which are advantages for python.\n",
            "  - And if you think of other competing languages for machine learning if you look at this chart in India about 44% of the people who have surveyed said they use python or they prefer python and of course the close second is R. In fact R was much more preferred a few years back but over the last few years in India python is starting to become the programming platform of choice.\n",
            "  - So in that sense it is a good language to learn because the opportunities for jobs and so on lot more when you are comfortable with python as a language.\n",
            "  - So with this I will stop this brief introduction on why python for data science.\n",
            "  - I hope I have given you an idea of the fact that while we are going to teach you python as a programming language please keep in mind that each module that we teach in this is actually geared towards data science.\n",
            "  - So as we teach python we will make the connections to how you will use some of the things that you are seeing in data science and all of this will culminate with these two case studies that will bring all of these ideas together in terms of both giving you an idea and an understanding of how the data science problem will be solved and also how it will be solved in python which is a program of choice currently in India.\n",
            "\n",
            "🟢 Topic 2:\n",
            "  - And since this is a course that is geared towards data science towards end of the course based on what has been taught in the course we will also show you two different case studies.\n",
            "  - One is what we call as a function approximation case study, another one a classification case study and then tell you how to solve those case studies using the programming platform that you have learned.\n",
            "  - So to look at that first we are going to look at what data science is.\n",
            "  - This is something that you would have seen in other videos of courses in NPTEL and other places.\n",
            "  - Data science is basically the science of analyzing raw data and deriving insights from this data.\n",
            "  - Nonetheless the key focus of data science is in actually deriving these insights using whatever techniques that you want to use.\n",
            "  - Now there is a lot of excitement about data science and this excitement comes because it is been shown that you can get very valuable insights from large data and you can get insights about how different variables change together, how one variable affects another variable and so on with large data which is not very easy to simply see by very simple computation.\n",
            "  - So you need to invest some time and energy into understanding how you could look at this data and derive these insights from data.\n",
            "  - And from a utilitarian viewpoint if you look at data science in industries if you do proper data science it allows these industries to make better decisions.\n",
            "  - So when we talk about data science we start by assuming that we have a large amount of data for the problem of interest and we are going to basically look at this data we are going to inspect the data we are going to clean and curate the data then we will do some transformation of the data modeling and so on before we can derive insights that are valuable to the organization or to test a theory and so on.\n",
            "  - Now coming to a more practical viewpoint of what we do once we have data I have these four bullet points which roughly tell you supposing you were solving a data science problem what are the steps you will do so you will start with just having data someone gives you data and you are trying to derive insights from this data so the very first step is really to bring this data into your system so you have to read the data so that the data comes into this programming platform so that you can use this data.\n",
            "  - Now data could be in multiple formats so you could have data in a simple excel sheet or some other format so we will teach you how to pull data in to your programming platform from multiple data formats so that is the first step really if you think about how you are going to solve a problem the steps would be first to simply read the data and then once you read the data many times you have to do some processing with this data you could have data that is not correct for example we all know that if you have your mobile numbers there are 10 numbers in a mobile number and if there is a column of mobile numbers and then say there is a one row where there are just 5 numbers then you know there is something wrong okay so this is a very simple check I am talking about in real data processing this gets much more complicated so once you bring the data in when you try to process this data you are going to get errors such as this so how do you remove such errors how do you clean the data is one activity that usually proceeds doing you more useful stuff with the data this is not the only issue that we look at there could be data that is missing so for example there is a variable for which you get a value in multiple situations but in some situations the value is missing so what do you do with this data do you throw the record away or you do something to fill the data and so on so these are all data processing cleaning steps so in this course we will tell you the tools that are available in python so that you can do this data processing cleaning and so on now what you have done at this point is you have been able to get the data into the system you have been able to process and clean the data and get to a certain data file or data structure that is reasonably complete so that you think you can work with this data set at which point what you will do is you will try to summarize this data and usually summarization of this data very simple technique would be very very simple statistical measures that you will compute you could for example compute a median mode mean of a particular column okay so those are simple ideas of summarizing the data you could compute variance and so on so we are going to teach you how to use this notions of statistical quantities that you can use to summarize the data once you summarize the data then other activity which is usually taken up is what is called visualization right so visualization means you look at this data and more pictorially to get insights about the data before you bring in heavy-duty algorithms to bear on this data and this is a creative aspect of data science the same data could be visualized by multiple people in multiple ways and some visualizations are not only eye-catching but are also much more informative than other types of visualization so this notion of plotting this data so that some of the attributes or aspects of the data are made apparent is this notion of visualization and there are tools in python that will teach you in terms of how you visualize this data so at this point you have taken the data you have cleaned the data got a set of data points or data structure that you can work with you have done some basic summary of this data that gives you some insights you also looked at it more visually and you have got some more insights but when you have large amount of data big data the last step is really deriving those insights which are not readily apparent either through visualization or through simple summary of data so how do we then go and look at more sophisticated analytics or analysis of data so that these insights come out and that is where machine learning comes and as a part of this course when you see the progress of this course you will notice that you will go through all of this so that you are ready to look at data science problems in a structured format and then use python as a tool to solve some of these problems.\n",
            "  - From a data summary viewpoint there are many of these statistical calculations that you want to do are already pre-programmed and you have to simply invoke them with your data to be able to show data summary.\n",
            "  - So I hope this short 4 week course helps you quickly get on to this programming platform and then learn data science and then you can enhance your skills with much more detailed understanding of both the programming language and data science techniques.\n",
            "  - Thank you.\n",
            "\n",
            "🟢 Topic 0:\n",
            "  - And you could use multiple techniques to derive insights, you could use simple statistical techniques to derive insights, you could use more complicated and more sophisticated machine learning techniques to derive insights and so on.\n",
            "  - These decisions could be in multiple fields for example companies could make better purchasing decisions, better hiring decisions, better decisions in terms of how to operate their processes and so on.\n",
            "  - So when we talk about decisions the decisions could be across multiple verticals in an industry and data science is not only useful from an industrial perspective it is also useful in actual sciences themselves so where you look at lots of data to model your system or test your hypotheses or theories about systems and so on.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CcNKHdysnQQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KVY03bPMJqtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "py9evBpQJqq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gs7cMREiJqom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5au0OeKrgbww"
      },
      "source": [
        "### Load and Preprocess Transcript from a TXT File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUgyVYbHVXSY"
      },
      "source": [
        "changing the code to include preprocessing of transcripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMSOAbqSgdob",
        "outputId": "0e22108d-946a-48b2-c941-e288708a88a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sentences after basic preprocessing: 323\n",
            "Sentence 1: hello everyone, welcome back to the final lecture of the first week.\n",
            "Sentence 2: , in the last lecture we were discussing about various empirical laws.\n",
            "Sentence 3: , , gifs law and heaps law that how the words in the vocabulary are distributed in a corpus.\n",
            "Sentence 4: and we saw that the distribution is not very uniform.\n",
            "Sentence 5: there are certain words that are very, very common.\n",
            "Sentence 6: , we saw that roughly 100 words in the vocabulary make for 50 percent of the corpus, but by that i mean that number of tokens.\n",
            "Sentence 7: and on the other hand, there are 50 percent of the words in the vocabulary that occur only once.\n",
            "Sentence 8: and we discussed what are the various relationships among the vocabulary size and the number of tokens that i observe in a corpus.\n",
            "Sentence 9: and also how they grow with us to each other.\n",
            "Sentence 10: and gifs law gives gave me a relation between the frequency and the rank of a word.\n",
            "Sentence 11: , we will start with the basic key processing in language.\n",
            "Sentence 12: , we will cover the basic concepts and what are the challenges that one might face while doing the processing.\n",
            "Sentence 13: , we are going to the basics of text processing.\n",
            "Sentence 14: , we will start with the problem of tokenization.\n",
            "Sentence 15: the name token, token is an individual word in my corpus.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "# Define phrases to remove (extended list)\n",
        "REMOVE_PHRASES = [\n",
        "    r'\\b[Ii] repeat\\b',\n",
        "    r'\\buh\\b', r'\\bum\\b', r'\\bhmm\\b', r'\\buhh\\b', r'\\bmmm\\b',\n",
        "    r'\\bokay\\b', r'\\bOkay\\b', r'\\bOK\\b', r'\\bok\\b',\n",
        "    r'\\bso\\b', r'\\bSo,\\b',\n",
        "    r'\\byou know\\b', r'\\blike\\b', r'\\bbasically\\b',\n",
        "    r'\\bactually\\b', r'\\bkind of\\b', r'\\bsort of\\b',\n",
        "    r'\\bat the end of the day\\b', r'\\bwell\\b', r'\\bright\\b',\n",
        "    r'\\bjust\\b', r'\\bnow\\b', r'\\bthen\\b',\n",
        "    r'\\bin particular\\b', r'\\bin general\\b',\n",
        "    r'\\bI mean\\b', r'\\blet\\'s see\\b', r'\\bas I said\\b',\n",
        "    r'\\bremember\\b', r'\\bto be honest\\b', r'\\bI suppose\\b',\n",
        "    r'\\bin fact\\b', r'\\bI think\\b', r'\\bfor example\\b',\n",
        "    r'\\bin this particular\\b', r'\\bfor instance\\b',\n",
        "    r'\\bas you know\\b', r'\\bas you can see\\b',\n",
        "    r'\\blet me just\\b', r'\\bI\\'ll just\\b',\n",
        "    r'\\bif you think about it\\b', r'\\bas we discussed\\b',\n",
        "    r'\\bsuppose\\b', r'\\bby that I mean\\b',\n",
        "    r'\\bspecifically\\b', r'\\bbriefly\\b',\n",
        "    r'\\bas you have seen\\b', r'\\bas we have discussed\\b',\n",
        "    r'\\bas I have mentioned\\b',\n",
        "    r'\\bnow coming back to\\b', r'\\bnow, coming back to\\b',\n",
        "    r'\\btoday in this lecture\\b', r'\\bin this lecture\\b',\n",
        "    r'\\bas the name would suggest\\b', r'\\bremember the name\\b',\n",
        "    r'\\bthat is\\b', r'\\bthat means\\b',\n",
        "    r'\\bas I was saying\\b', r'\\bas we were discussing\\b'\n",
        "]\n",
        "\n",
        "REMOVE_REGEX = re.compile('|'.join(REMOVE_PHRASES))\n",
        "\n",
        "def load_transcript(file_path):\n",
        "    \"\"\"Load and preprocess a transcript from a text file.\"\"\"\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "        return preprocess_transcript(text)\n",
        "\n",
        "def preprocess_transcript(text):\n",
        "    \"\"\"Apply comprehensive preprocessing to transcript text.\"\"\"\n",
        "    # Convert to lowercase (optional, depending on requirements)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove filler phrases\n",
        "    text = REMOVE_REGEX.sub('', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Fix common transcription issues\n",
        "    text = fix_transcription_errors(text)\n",
        "\n",
        "    # Normalize punctuation\n",
        "    text = normalize_punctuation(text)\n",
        "\n",
        "    # Split into sentences (using NLTK for better segmentation)\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Optional: Remove short sentences (likely fragments)\n",
        "    sentences = [s for s in sentences if len(s.split()) > 3]\n",
        "\n",
        "    return sentences\n",
        "\n",
        "def fix_transcription_errors(text):\n",
        "    \"\"\"Fix common transcription errors.\"\"\"\n",
        "    # Fix repeated words (e.g., \"the the\")\n",
        "    text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)\n",
        "\n",
        "    # Fix common ASR errors\n",
        "    replacements = {\n",
        "        'Sanda': 'Sandhi',\n",
        "        'Sunday operation': 'Sandhi operation',\n",
        "        'GIFs law': 'Zipf\\'s law',\n",
        "        'GIF\\'s law': 'Zipf\\'s law',\n",
        "        'Heaps law': 'Heaps\\' law',\n",
        "        'Giph law': 'Zipf\\'s law',\n",
        "        'Tokenarage': 'Tokenizer',\n",
        "        'tokenarize': 'tokenize',\n",
        "        'lamtaization': 'lemmatization',\n",
        "        'lamtization': 'lemmatization',\n",
        "        'perqueous': 'previous',\n",
        "        'griddle': 'greedy',\n",
        "        'Eglidi': 'greedy',\n",
        "        'on the relation': 'sandhi relation',\n",
        "        'portus algorithm': 'Porter\\'s algorithm',\n",
        "        'clean it': 'kleene',\n",
        "        'affix is': 'affixes',\n",
        "        'prefossessing': 'preprocessing',\n",
        "        'infix': 'infix',\n",
        "        'perqueous': 'previous',\n",
        "        'coppers': 'corpus'\n",
        "    }\n",
        "\n",
        "    for old, new in replacements.items():\n",
        "        text = re.sub(r'\\b' + re.escape(old) + r'\\b', new, text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def normalize_punctuation(text):\n",
        "    \"\"\"Standardize punctuation in transcript.\"\"\"\n",
        "    # Fix spacing around punctuation\n",
        "    #text = re.sub(r'\\s+([.,;:!?])', r'\\1', text)\n",
        "\n",
        "    # Standardize quotes\n",
        "    text = re.sub(r'[\"\"\"]', '\"', text)\n",
        "    text = re.sub(r\"['\\']\", \"'\", text)\n",
        "\n",
        "    # Fix ellipses\n",
        "    text = re.sub(r'\\.\\.\\.', '…', text)\n",
        "\n",
        "    # Fix dashes\n",
        "    text = re.sub(r'--', '—', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def remove_stopwords(sentences, exclude_list=None):\n",
        "    \"\"\"Remove stopwords from sentences.\"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    if exclude_list:\n",
        "        stop_words = stop_words - set(exclude_list)\n",
        "\n",
        "    result = []\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "        result.append(' '.join(filtered_words))\n",
        "\n",
        "    return result\n",
        "\n",
        "def stem_sentences(sentences):\n",
        "    \"\"\"Apply Porter stemming to sentences.\"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    result = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        stemmed_words = [stemmer.stem(word) for word in words]\n",
        "        result.append(' '.join(stemmed_words))\n",
        "\n",
        "    return result\n",
        "\n",
        "def lemmatize_sentences(sentences):\n",
        "    \"\"\"Apply lemmatization to sentences.\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    result = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "        result.append(' '.join(lemmatized_words))\n",
        "\n",
        "    return result\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/transcribed_text_nptel_video_1.txt\"\n",
        "\n",
        "    # Basic preprocessing\n",
        "    sentences = load_transcript(file_path)\n",
        "    print(f\"Number of sentences after basic preprocessing: {len(sentences)}\")\n",
        "\n",
        "    # Optional: Further processing based on needs\n",
        "    # sentences_without_stopwords = remove_stopwords(sentences)\n",
        "    # lemmatized_sentences = lemmatize_sentences(sentences)\n",
        "    # stemmed_sentences = stem_sentences(sentences)\n",
        "\n",
        "    # Print first few sentences\n",
        "    for i, sentence in enumerate(sentences[:15]):\n",
        "        print(f\"Sentence {i+1}: {sentence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02U18Z5GeUuR"
      },
      "source": [
        "### Preprocess and Convert Sentences into BERT Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4gkNpXVeBJE"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load SBERT model\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "# Convert transcript sentences to embeddings\n",
        "def get_sentence_embeddings(sentences):\n",
        "    return model.encode(sentences, convert_to_numpy=True)\n",
        "\n",
        "# Generate embeddings\n",
        "sentence_vectors = get_sentence_embeddings(sentences)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED_OKgCTeYEp"
      },
      "source": [
        "### Apply Gaussian Mixture Model (GMM) for Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XMXOAhp8eD_Y"
      },
      "outputs": [],
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Choose the number of clusters based on BIC/AIC\n",
        "num_clusters = 6\n",
        "\n",
        "# Fit GMM\n",
        "gmm = GaussianMixture(n_components=num_clusters, random_state=42)\n",
        "gmm.fit(sentence_vectors)\n",
        "\n",
        "# Predict cluster labels\n",
        "labels = gmm.predict(sentence_vectors)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCQKj0DCecyk"
      },
      "source": [
        "### Group Sentences by Cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_MAYH8Ojd3Tj",
        "outputId": "9d633d4e-31e9-46ec-e1ff-1c37f64a5ebc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🟢 Topic 0:\n",
            "  - hello everyone, welcome back to the final lecture of the first week.\n",
            "  - , in the last lecture we were discussing about various empirical laws.\n",
            "  - and we saw that the distribution is not very uniform.\n",
            "  - and also how they grow with us to each other.\n",
            "  - , we will cover the basic concepts and what are the challenges that one might face while doing the processing.\n",
            "  - , this you may or may not have to do always and it depends on what is your application.\n",
            "  - , , you are doing classification for the whole document into certain classage.\n",
            "  - , you might feel that this is very trivial task, but let us see is it trivial.\n",
            "  - , do you think there might be certain challenge involved?\n",
            "  - , again you have numbers 2.44.3 and on.\n",
            "  - , in text processing, if we face this problem in nearly every simple task that we are doing.\n",
            "  - , how do we go about solving this.\n",
            "  - , any data point that i am seeing, i have to divide into one of these two classes.\n",
            "  - , each point you have to divide into one of the two classes.\n",
            "  - you are classifying into one of the two classes.\n",
            "  - , the idea is very simple.\n",
            "  - , you have two classes and each data point you have to divide into one of the two classes.\n",
            "  - , there can be multiple classes.\n",
            "  - , scenario, what can be the simplest thing to do?\n",
            "  - , this is very, very simple, if as rules.\n",
            "  - this may not be correct, but this is one particular way in which this problem can be solved.\n",
            "  - , you might want to use some other indications.\n",
            "  - we call them as various features.\n",
            "  - , what are some examples?\n",
            "  - how will it help?\n",
            "  - , this can be used as another feature.\n",
            "  - , how will that help?\n",
            "  - , again this can be used.\n",
            "  - what can be some other features?\n",
            "  - , i will have certain thresholds.\n",
            "  - , this problem you will see is again language dependent.\n",
            "  - , we will see some of the examples in the same lecture.\n",
            "  - , how do we implement it recently?\n",
            "  - , this is a simple if else statement.\n",
            "  - you will see in your from your data what are some observations that can separate my two classes here.\n",
            "  - , all these are my observations that i use as my features.\n",
            "  - , here is one problem i keep on increasing my features this can be both numerical or non-unimegal features.\n",
            "  - , one particular criteria is what is the information gain by this.\n",
            "  - they all use one of these criteria.\n",
            "  - , we will talk about some of these as we will go to some advanced topics in this course.\n",
            "  - , i give you a simple sentence here i have a can opener but i cannot open these cans.\n",
            "  - but you can use any of these three possibilities.\n",
            "  - , one question i have is whether i treated as simple fill land as it is fill lands or i converted to fill lands by removing the first of it.\n",
            "  - , this question you might also try to defer to the next processing step that we will see.\n",
            "  - but sometimes you might want to tackle this in the same step.\n",
            "  - when we were talking about some of the cases y and l page hard.\n",
            "  - , this problem is related.\n",
            "  - , this looks again a simple problem but we will see it is not that simple.\n",
            "  - but again this is this is not trivial because for show time you will not do the same show time you might want to keep it as it is.\n",
            "  - but here you are putting it that it becomes easier to interpret that particular occurrence.\n",
            "  - further there are various issues that might that you might face for certain language but not others.\n",
            "  - , that might be a similar problem that you are facing in english but let us take something in german.\n",
            "  - , you need some compound splitter for german.\n",
            "  - , what happens if i am taking a language chinese or japanese.\n",
            "  - , this problem becomes a bit more severe.\n",
            "  - , if you try to undo the sun, this is what you will find at the segmented text.\n",
            "  - you put them one after another without making any changes at the boundary.\n",
            "  - let us say it is a case of chinese.\n",
            "  - and this works nicely for most of the cases.\n",
            "  - we do not do that, but what is the scenario where we are doing that ?\n",
            "  - what i mean by sigma star here.\n",
            "  - , w star here.\n",
            "  - but my problem is how do i analyze them?\n",
            "  - , how the segmentarage is segmentarage built.\n",
            "  - , this is a snapshot from the segmentar.\n",
            "  - , here whenever i have brianna.\n",
            "  - , coming back to normalization.\n",
            "  - , that you will have to consider this problem in advance and do the pre-versus in accordingly of either your documents or the query, but using the same sort same settings.\n",
            "  - , what we are doing by this, we are defining some equivalence class h. we are saying u s a and u dot s dot a should go to one class and they are the same type.\n",
            "  - sometimes depending on application, you might have certain exceptions.\n",
            "  - , you might have to treat the name and its separately.\n",
            "  - , if you have a entity general model, you might want to keep it as it is without case folding.\n",
            "  - what is the base form from this they are derived.\n",
            "  - , again this is some nobulation.\n",
            "  - , this is again language dependent.\n",
            "  - , the actual lemma is automate within e, but here.\n",
            "  - , i am chopping off the affixage at that.\n",
            "  - , i am removing here this i c i o n all and putting it to automate.\n",
            "  - , this is one example.\n",
            "  - , what are some examples here.\n",
            "  - , what is the first step?\n",
            "  - , next week we will start with another pto sinsing task spilling correction.\n",
            "\n",
            "🟢 Topic 2:\n",
            "  - , , gifs law and heaps law that how the words in the vocabulary are distributed in a corpus.\n",
            "  - there are certain words that are very, very common.\n",
            "  - , we saw that roughly 100 words in the vocabulary make for 50 percent of the corpus, but by that i mean that number of tokens.\n",
            "  - and on the other hand, there are 50 percent of the words in the vocabulary that occur only once.\n",
            "  - and we discussed what are the various relationships among the vocabulary size and the number of tokens that i observe in a corpus.\n",
            "  - and gifs law gives gave me a relation between the frequency and the rank of a word.\n",
            "  - , we will start with the basic key processing in language.\n",
            "  - , we are going to the basics of text processing.\n",
            "  - , we will start with the problem of tokenization.\n",
            "  - the name token, token is an individual word in my corpus.\n",
            "  - , what happens when i am pre processing the text in given in any language.\n",
            "  - what i will face is a string of characters, the sequence of characters.\n",
            "  - , i need to identify what are all the different words that are there in this sequence.\n",
            "  - , tokenization is the process by which i convert the string of characters into sequence of various words.\n",
            "  - , i am trying to segment it by the various words that i am observing.\n",
            "  - , before going into what is tokenization, i will talk about a slightly related problem sentence segmentation.\n",
            "  - you might not have to go to the individual sentence and you can talk about what are various words that are present in this document.\n",
            "  - on the other hand, you are trying to find out what are the important sentences in this document.\n",
            "  - in that application you will have to go to the individual sentence.\n",
            "  - , if you have to go to the individual sentence, the first task that you will face is how do i segment this whole document into a sequence of sentences.\n",
            "  - , this is sentence one sentence two and on and this task is called sentence segmentation.\n",
            "  - , what is sentence segmentation?\n",
            "  - , that i have a complete unit of words that i call as a sentence.\n",
            "  - or it can be some expressions, i say if my particular example matches with this sexual expression, it is one plus, if it does not match, it is another class.\n",
            "  - they are various observations that you make from your corpus.\n",
            "  - can i use this as a feature whether my word starts within uppercase, lowercase, cap, all caps, all is in number.\n",
            "  - , again by feature you can think of a simple rule whether the word i am currently at is a number or i can use the fact where the case of the word with dot is uppercase or lowercase.\n",
            "  - , is it uppercase, lowercase, capital or number.\n",
            "  - what is the probability that it occurs in the start of sentence in a large corpus?\n",
            "  - , my 3 can be if the length of the word is between 5 to 7 i go to one class otherwise i go to another class.\n",
            "  - , coming back to a problem of tokenization.\n",
            "  - we said that the tokenization is a process of segmenting a string of characters in two words finding out what are the different words in this issue.\n",
            "  - , we talked about token and type distinction.\n",
            "  - how many tokens are there?\n",
            "  - if you count there are 11 different words 11 different words 11 different occurrences of words.\n",
            "  - , you have 11 word tokens but how many unique words are there.\n",
            "  - , you will find there are only 10 unique words which word repeats say the word i repeats twice.\n",
            "  - , there are 10 types and 11 tokens.\n",
            "  - , my tokenization is to find out each of the 11 word tokens from the sentence.\n",
            "  - , for english most of the problems that we will see are taking care of by the token areas that we have discussed previously.\n",
            "  - but still it is good to know what are the challenges that are involved when i have tried to design a tokenization algorithm.\n",
            "  - , , here you will see that i if i encounter a word fill lands in my data.\n",
            "  - similarly, if you see what are do i treated as a single token or two tokens what are this problem you might have to solve in the same step whether i treated as a single token or multiple tokens same with i am shouldn't and on.\n",
            "  - similarly, whenever we have named and it is san francisco should i treated as a single token or two separate tokens.\n",
            "  - , you might have to find out this particular sequence of tokens is a single entity and treated as a single entity not as multiple different tokens.\n",
            "  - similarly, if you find m dot p dot h do you call it a single token or multiple tokens.\n",
            "  - , if i am using it for information tree i should use the same convention for both my documents as the query.\n",
            "  - , here i have a sentence from a research paper abstract and the sentence says this paper describes mimic an adaptive mixed initiative spoken dialogue system that provides movie show time information.\n",
            "  - , again when you are doing tokenization you have problem at how do i handle all these hyphens.\n",
            "  - , is in french if you have a token lunch ensemble you might want to match it with ensemble.\n",
            "  - but the problem is that this is not a single word.\n",
            "  - this is a term bound composed of four different words and the corresponding english meaning is this one.\n",
            "  - , what is the problem that you will face when you are processing the german text and you are trying to tokenize it.\n",
            "  - , this problem is there for german not much for english.\n",
            "  - , here is a sentence in chinese.\n",
            "  - , when you are doing the preprocessing your task is to find out what are the individual words tokens in this chinese sentence.\n",
            "  - , this problem is also difficult because for a given utterance of a sequence of characters they might be more than one possible ways of breaking into sequence of words and both might be perfectly valid possibilities.\n",
            "  - , in chinese we do not have any space between words and have to find out what are the places where i have to break these words and this problem is called tokenization word tokenization.\n",
            "  - this is a proverb and this is a single sentence that talks about this proverb, but where all the words are combined with some relation.\n",
            "  - , this further complicates my problem of word segmentation or segmentation of.\n",
            "  - i have a sequence of characters and you segment it to find out individual words.\n",
            "  - , whenever you are given a string, you start your pointer at the beginning of the string.\n",
            "  - , that you have the dictionary and the words that you are that you are currently seeing all should be in the dictionary.\n",
            "  - , you will find out what is the maximum match as per my dictionary in the string, you break there and put the pointer from at the next correct and again do the same thing.\n",
            "  - , this silly iteration, can you think of some cases where the segmentation will also be required for english text?\n",
            "  - , does do hashtags come into mind?\n",
            "  - , , i have hashtags thank you searching in music monday.\n",
            "  - , if you are given a hashtag and you have to analyze that, you have to segment it into various words.\n",
            "  - , from this finite alphabet i can generate a lot of words that are composed of various number of phonemes or letters from this alphabet.\n",
            "  - , when i have a set of words i can combine them together with an operation of sunday.\n",
            "  - , the inverse problem whenever i am given a sentence w i have to analyze it by inverting the relations of sunday.\n",
            "  - , that i can produce a finite set of word forms w 1 to w n. and i am saying together with a proof a formal way of saying that, but what i mean is that w 1 to w n when i they combine by sunday operation they give me the actual sentence, the initial sentence.\n",
            "  - , i gave the same sentence there and it gave me all the possible ways of analyzing the sentence and it says that there are 120 different solutions.\n",
            "  - , you see there are two possibilities, briath and briam plus that it gives me all the possible ways in which the sentence can be broken into individual word tokens.\n",
            "  - , this is another problem that i will have to find out what is the most likely word sequence among all these 120 possibilities, but we can use many different models that we will not talk about , probably in some other lectures.\n",
            "  - , you query contains u dot s dot a and the document contains u s a. if you are only doing the surface level match, you will not be able to map them to each other.\n",
            "  - we also have the problem of lemmatization you have individual words m r is and you want to convert them to their lemma.\n",
            "  - , what is morphology?\n",
            "  - , these individual units are called various morphim.\n",
            "  - it does not give me the correct dictionary had word but it is still this is a good practice in principle for information tree.\n",
            "  - if you want to match the query with the documents, this is for this week.\n",
            "\n",
            "🟢 Topic 5:\n",
            "  - , problem of deciding where my sentence begins and ends.\n",
            "  - , i am talking about the language english, can you always say that wherever i have a dot it is the end of the sentence.\n",
            "  - , there are many ways in which i can end my sentence.\n",
            "  - , i can have exclamation or question mark that ends the sentence and they are mostly an ambiguous.\n",
            "  - , whenever i have a exclamation or question mark, i can say probably this is the end of the sentence, but is the case the same with a dot.\n",
            "  - , can you think of a scenario where i have a dot in english and but it is not the end of sentence.\n",
            "  - , you can find all sorts of abbreviations they end with a period.\n",
            "  - , you have three dots here.\n",
            "  - , you cannot call each of the this as the end of your sentence.\n",
            "  - , the problem of deciding whether a particular dot is the end of the sentence or not is not entirely trivial.\n",
            "  - , i need to build certain algorithm for finding out is it my end of the sentence.\n",
            "  - , even if it looks at trivial task, we face this problem that can i always call dot as the end of the sentence.\n",
            "  - , whenever i see a dot or question mark or exclamation, i always have to decide one of the two things.\n",
            "  - is it the end of the sentence or is it not the end of the sentence.\n",
            "  - , if you think of these as two classes, end of the sentence or not end of the sentence.\n",
            "  - there are two classes, end of the sentence or not end of the sentence.\n",
            "  - , for each dot or for every word, i need to decide whether this is the end of the sentence or not the end of the sentence.\n",
            "  - , i am at a particular word, i want to decide whether this is the end of the sentence or not.\n",
            "  - , i am at a word and the first thing i check is, are there lots of blank lines after me?\n",
            "  - , this would happen in a text whenever this is the end of the paragraph and there are some blank lines.\n",
            "  - , if i feel that there are a lot of blank lines after me, , after this word, i may say, this might be the end of the sentence with a good confidence.\n",
            "  - , why the branch hearsage, yes, this is the end of the sentence.\n",
            "  - but, they are not a lot of blank lines, i will check if the final punctuation is equation mark or explanation or equivalent in that case.\n",
            "  - , they are quite an ambiguous, i will say this is the end of the sentence.\n",
            "  - , it is not, i will check if the final punctuation is a period.\n",
            "  - , if it is a period, if it is not a period, it is easy to say that this is not the end of the sentence.\n",
            "  - but, this is a period.\n",
            "  - , again, i cannot say for certain if it is the end of the sentence.\n",
            "  - if it is there, i will say, this is not the two sentence.\n",
            "  - if it is, here, i am accept or any other abbreviation, if the answer is yes, i am not an order sentence.\n",
            "  - if the answer is no, , this word is not an abbreviation and this will be the end of the sentence.\n",
            "  - , i see the word ending with dot.\n",
            "  - , let us see, i have here, i am here and my word is 4.3. , i am at dot, i want to find out if it is the end of the sentence.\n",
            "  - if i can say that the previous the current word is a number, it is a high probability that this will be in number and it will not be the end of the sentence.\n",
            "  - similarly, i can also use the case of the word after dot.\n",
            "  - , again whenever i have the end of the sentence, the next word starts with a capital.\n",
            "  - what is the length of the word ending with dot?\n",
            "  - is it if the length is small, it might be an abbreviation if the length is larger, it might not be an abbreviation.\n",
            "  - and i can also use probably the, what is the probability that the word ending with dot occurs at the end of the sentence.\n",
            "  - , if it is really the end of the sentence, it might happen that in a large corpus, this end sentence quite often.\n",
            "  - is it the start of the sentence?\n",
            "  - , you might be able to use any of these features to decide given a particular word, is it the end of the sentence or not.\n",
            "  - , in hindi you will see that there is only a dunder that you use to indicate the end of the sentence and this is not used for any other purpose.\n",
            "  - , my two classes here are end of the sentence and the end of the sentence and what are the observations we were having it might be an abbreviation the case of the word and that it is before the dot may be uppercase or lower case and one of these might indicate one class other might indicate other class.\n",
            "  - , what happens in research papers , whenever you write a sentence you might to do some justification and where you end the line even if it is not the end of the word.\n",
            "  - , i have this i have this big sentence here.\n",
            "  - not the sentence is what the words.\n",
            "  - , whenever i have the word w r d, i will always write small w r d. , that whenever even if it is starting the sentence and it occurs in capitals because of that , i know that this is a word good w r d, but this is not a generic rule.\n",
            "\n",
            "🟢 Topic 1:\n",
            "  - and this , this problem is called classification problem.\n",
            "  - , , you have to build some rule of algorithm for doing that.\n",
            "  - , in this case, i have to build a binary classifier.\n",
            "  - what do i mean by a binary classifier?\n",
            "  - , , my classifiers that i will build can be some rules that i write by hand, some simple if-ten-else rules.\n",
            "  - or i can build a machine learning classifier.\n",
            "  - let us see, can we build a simple rule-based classifier?\n",
            "  - , we will start with the example of a simple decision tree.\n",
            "  - , by decision tree, i mean a set of if-ten-else statements.\n",
            "  - , i can have this simple if-ten-else decision tree here.\n",
            "  - , i can have some numerical features.\n",
            "  - , what is important is that you choose the correct set of features.\n",
            "  - , how do you go about choosing the set of features?\n",
            "  - , whenever i am using a numerical features the length of the word before dot i need to pick some threshold whether the length of the word is between 2 to 3 or say more than 3 between 5 to 7 that.\n",
            "  - it might be difficult to set up my if else rules by hand.\n",
            "  - , in that scenario i can try to use some machine learning technique to learn this dictionary in the literature there are lot of such algorithms that are available that give any data and a set of features will consider a dictionary for you.\n",
            "  - , i will give you the names of some of the algorithms and the basic idea on this they work is that at every point you have to choose a particular split.\n",
            "  - , you have to choose a feature value splits my data into certain parts and i have certain criteria to find out what is the best split.\n",
            "  - , these algorithms that we have mentioned here id3, c415 and c4.\n",
            "  - once you identify what are your interesting features for this task you are not limited to only one classifier dictionary.\n",
            "  - you can also try out some other classifiers support vector machines, large regression and neural networks.\n",
            "  - all these are quite popular classifiers for various nlp applications.\n",
            "  - in practice at least for english you can use certain tool kits that are available analytic in python, coronal pin java and you can also use some unique commands.\n",
            "  - , in this course we will mainly be using analytic toolkit for doing all these pre-tastic task and some other tasks as .\n",
            "  - , they are no fixed answers to these and some of these might depend on what is the application for which you are doing this pre processing.\n",
            "  - but one thing you can always keep in mind you are doing it for the application of information tree the same steps that you apply for your documents should be applied to your query as otherwise you will not be able to match them perfectly.\n",
            "  - , what is the simplest algorithm that you can think of?\n",
            "  - , the simplest algorithm that works is eglidi algorithm called maximum matching algorithm.\n",
            "  - , we have the Porter's algorithm very famous and this is again simpson's set of if the niles rules.\n",
            "\n",
            "🟢 Topic 4:\n",
            "  - , i will again check, for simplicity, i might have a list of abbreviations and i can check if the word that i am currently facing is one of the abbreviations in my list.\n",
            "  - , what happens generally in abbreviations?\n",
            "  - we are mostly in uppercase.\n",
            "  - , i have doctor and it starts with an uppercase, i can say that this might be an abbreviation.\n",
            "  - same with lowercase, lowercase will give me more probability that this is not an abbreviation.\n",
            "  - same thing i can do with the next word after dot.\n",
            "  - , another problem can be how do i handle hyphens in my data.\n",
            "  - , let us see some examples what are the various sorts of hyphens that can be there in my corpus.\n",
            "  - , in the sentence itself you see two different hyphens one is with initiative another is show hyphen time.\n",
            "  - , can you see that these two are different hyphens the first hyphen is not that i will i will use in my text.\n",
            "  - second hyphen i can use in my text i can write show time within hyphen but how did this hyphen initiative came into the corpus.\n",
            "  - , we have given this title end of line hyphen.\n",
            "  - , you will end up within hyphen.\n",
            "  - , when you are trying to pre process and when you are retrieving such hyphens you might have to join these together and you say you have to say that this is single word initiative and not initiate hyphen tape.\n",
            "  - there are some other hyphens lexical hyphens.\n",
            "  - , you might have these hyphens with various prefixes copie, meta, multi, etcetera.\n",
            "  - sometimes they are sententially determined hyphens also you put hyphens that it becomes easier to interpret the sentence here case paste, handle evoked, etcetera are optional.\n",
            "  - similarly, if you see in the next sentence 3 to 5 year direct marketing plan 3 to 5 year can be written perfectly without keeping the hyphens.\n",
            "  - , you have four words in english when you are putting in french they make a compound.\n",
            "  - , you might want to find out what are the individual words compound.\n",
            "  - , there are multiple words in this sentence, they are combined to make a single, it looks a single word.\n",
            "  - , in japanese and in chinese, when you try to combine various words together, you simply concatenate them.\n",
            "  - when you combine two words, you also make certain changes at the boundary and this is called a sanda operation.\n",
            "  - , case, you see here i have the word briyath and the word na, but when i am combining, i am writing it briyana.\n",
            "  - , this greedy chooses what are the actual words by taking the maximum match.\n",
            "  - in english we do not combine words to make a single word.\n",
            "  - , here different words are combined together without putting a boundary in between.\n",
            "  - , i have a set of words w and i do a kleene words here.\n",
            "  - i can combine any number of words together, but whenever i am combining words i am doing them by a sunday operation because there is a relation between the words.\n",
            "  - , we talked about this problem that the same word might be written multiple different ways u dot s dot a versus u s a. , i should be able to match them together, especially if you are doing information retrieval, you are giving a query and you are retrieving from some document.\n",
            "  - we also do some case folding we can reduce all that as to lower case.\n",
            "  - similarly, you might want to keep u s for united states in upper case and not do the case folding.\n",
            "  - and this is important for the application of machine translation also because if you do a case folding here, you will know u s in lower case something else was such u s in united states.\n",
            "  - similarly, car, cars, cars, cars, all these are derived from car.\n",
            "  - you are saying all these are some equivalence class because they come from the same word form.\n",
            "  - , in the problem of lemmatization is that you have to find out the actual dictionary head word from this they are derived and for that we use morphology.\n",
            "  - i am trying to find out the structure of a word by seeing what is the previous stem, the head word and what is the affix that i apply to it.\n",
            "  - , you have which stems that are the originally head words and the affixes that are what are the different units as for plural etcetera you are applying to them to make the individual word.\n",
            "  - and you can also have some infix you have a word width and you can fix an in between.\n",
            "  - , we will discuss in detail about it in morphology later.\n",
            "  - , there is another concept you have lemmatization where you are finding the actual dictionary head word.\n",
            "  - , this is also concept called stemming where you do not try to find the actual dictionary head word, but you try to remove certain suffixage and you or whatever you obtain is called stem.\n",
            "  - , this is a crude chopping of various suffixage in that word.\n",
            "  - , what we are doing here words automate automatic automation all will be reduced to a single lemma automate here.\n",
            "  - , this is stemming.\n",
            "  - , if you try to do a stemming here, this is you will find from example, e is removed from compressed edged removed and on.\n",
            "  - , what is the algorithm used for this stemming.\n",
            "  - i take a word if it ends with s e s, i remove e s from there and i end with s. , example is carousers goes to carous.\n",
            "  - if not i see whether the word ends with i s i put it to i ponies goes to ponies.\n",
            "  - if not i see if the word ends with s i keep bidage s. if not i see if the word ends with s i remove that s. , cats goes to cat, but carous does not go to carousers with only one s because this step comes before.\n",
            "  - if there is a double s ending the word i retain it otherwise if there is a single s i remove that there are some other steps.\n",
            "  - , if there is a vowel in the in my word and the word ends with i and g i remove i and g. , walking goes to walk, but what about king?\n",
            "  - you see in k there is no vowel.\n",
            "  - , i will be retained as it is same is a vowel and there is an e d i remove this e d and i have this what played to play.\n",
            "  - , you can see that what is the use of this heuristic of having this vowel.\n",
            "  - if you did not have this vowel you would have converted king to k and that there is some other rules if the word ends with aishinal i put a t e, relational, relational to relate and if the words and what ends with i z r i can i remove that r, digitize i z digitize, a t e r to a t e and if the word ends with l i remove that l if the word ends with able i remove that able if the words and with a t e i remove that a t e. , that these are some steps that i take from my corpus for each word i converted to its time.\n",
            "\n",
            "🟢 Topic 3:\n",
            "  - , i ask you this question, do you have the same problem in other languages hindi?\n",
            "  - this problem is there for english, but not for hindi, but we will see there are other problems that do not exist for english language, but are there for other indian language.\n",
            "  - , what do you see in chinese words are written without any spaces in between.\n",
            "  - same problem happens with japanese and you have further complications because they are using four different scripts katakana, hiragana, kanjana, rumanji.\n",
            "  - , the same problem is there even for sanskrit.\n",
            "  - , if some of you have taken sanskrit course in your class 8th or 10th.\n",
            "  - , you might be familiar with the rules of sanskrit language.\n",
            "  - , let us say this is a simple single sentence in sanskrit, but this is a huge, this looks a simple word, but it is not a single word.\n",
            "  - it is composed of multiple words in sanskrit and they are combined with this sandhi relation.\n",
            "  - this stands for a nice proverb in sanskrit that translation english as one should tell the truth, one should say kind words, one should neither tell harsh truth, not flatly flattening lies, this is a rule for all times.\n",
            "  - , this problem is in chinese, japanese and sanskrit, but in sanskrit, the problem is slightly more complicated and why is that?\n",
            "  - it does not happen in sanskrit.\n",
            "  - , you see here the letter t gets changed to n. , , when i am trying to analyze the sentence, this purpose sentence in sanskrit, i need to find out not only what are the breaks, but what is the corresponding word from which this sentence is .\n",
            "  - , from here i have to find out the actual words are briyath plus na, that gives me this briyana and this is very, very common in sanskrit, that you are always combining words by doing sanda operation.\n",
            "  - , this is a list from wikipedia, what are the longest words in various languages?\n",
            "  - , you see in sanskrit the longest word is composed of 431 characters, it is the compound and you have greek and african and other languages.\n",
            "  - in english you see the longest word is of 45 characters, this is non scientific.\n",
            "  - , what is the particular word in sanskrit, composed of 431 letters.\n",
            "  - , this was from the word ambika, parinaya champu by tirum lamba, this is a single compound from his book.\n",
            "  - , when i talk about this problem of tokenization in sanskrit or in english this problem is also called word segmentation.\n",
            "  - , when i talk about sanskrit, this we have a segment are available at the site sanskrit.ng.fr.\n",
            "  - , we will see what is the design principle of building a segment or in sanskrit.\n",
            "  - , first we have a generative model that says how do i generate a sentence in sanskrit?\n",
            "  - i have a finite alphabet sigma a set of various characters in sanskrit.\n",
            "  - , i have my set of inserted words also called padaj in sanskrit and i have the relation of sunday between them and how i generate a sentence.\n",
            "  - some examples are for prefix you have un-ant-e etcetera for english and a teapra etcetera for hindi or sanskrit suffix etiation etcetera and tarqa, k etcetera for hindi.\n",
            "  - this is in sanskrit.\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Group sentences by their assigned topic\n",
        "clustered_sentences = defaultdict(list)\n",
        "for i, label in enumerate(labels):\n",
        "    clustered_sentences[label].append(sentences[i])\n",
        "\n",
        "# Print topics and their sentences\n",
        "for topic, sentences in clustered_sentences.items():\n",
        "    print(f\"\\n🟢 Topic {topic}:\")\n",
        "    for s in sentences:\n",
        "        print(f\"  - {s}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97pUOQ02enfW"
      },
      "source": [
        "### Extract Keywords for Each Cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCd5zEEFerol"
      },
      "source": [
        "To get topic keywords, use TF-IDF on sentences inside each cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j44OEqCwd_ah",
        "outputId": "bf0bead5-d8ed-4e4c-e59a-8e7c482b06cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔹 Topic 0 Keywords: ['simple', 'problem', 'help', 'step', 'used']\n",
            "\n",
            "🔹 Topic 2 Keywords: ['features', 'classes', 'examples', 'use', 'general']\n",
            "\n",
            "🔹 Topic 3 Keywords: ['word', 'words', 'sanskrit', 'sentence', 'problem']\n",
            "\n",
            "🔹 Topic 1 Keywords: ['sentence', 'tokens', 'problem', 'tokenization', 'words']\n",
            "\n",
            "🔹 Topic 4 Keywords: ['case', 'uppercase', 'language', 'problem', 'lowercase']\n",
            "\n",
            "🔹 Topic 5 Keywords: ['sentence', 'end', 'word', 'dot', 'say']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Get keywords for each topic\n",
        "def get_top_keywords(sentences, num_words=5):\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform(sentences)\n",
        "    feature_array = vectorizer.get_feature_names_out()\n",
        "    tfidf_sorting = X.sum(axis=0).A1.argsort()[::-1]\n",
        "    return [feature_array[i] for i in tfidf_sorting[:num_words]]\n",
        "\n",
        "# Print top words per topic\n",
        "for topic, sentences in clustered_sentences.items():\n",
        "    keywords = get_top_keywords(sentences)\n",
        "    print(f\"\\n🔹 Topic {topic} Keywords: {keywords}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS9Ryw_Y4Vd1"
      },
      "source": [
        "Looks like clustering happened based on: Tokenization, Stopword/word/lemmatization, importance, NER, POS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYpS4Y2qtzZ9"
      },
      "source": [
        "### SUMMARIZE by picking the most important sentence in each cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "id": "apu5LAlgsizC",
        "outputId": "2a86b5c5-b31a-4df3-a868-868f5e861f3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Collecting sympy\n",
            "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy) (1.3.0)\n",
            "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sympy\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires sympy==1.13.1; python_version >= \"3.9\", but you have sympy 1.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed sympy-1.13.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "21183f1eeceb4b318a9df5106b8aecfb",
              "pip_warning": {
                "packages": [
                  "sympy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install sympy --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwqU2rbNt464"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def extractive_summary(sentences,topic):\n",
        "    # Embed all the sentences in each cluster using SBERT\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    embeddings = model.encode(sentences)\n",
        "\n",
        "    # Compute similarity matrix\n",
        "    similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "    # Find the most central sentence (highest avg similarity)\n",
        "    avg_sim = np.mean(similarity_matrix, axis=1)\n",
        "    central_idx = np.argmax(avg_sim) # np.argmax gives the index of the max value\n",
        "\n",
        "    selected_indices = [central_idx] # storing the selected sentences' indices\n",
        "    #print(sentences[central_idx],max(avg_sim))\n",
        "\n",
        "    if topic==0:\n",
        "        similarity_matrix[:, selected_indices[-1]] = -1\n",
        "        similarity_matrix[selected_indices[-1], :] = -1\n",
        "        avg_sim = np.mean(similarity_matrix, axis=1)\n",
        "        central_idx = np.argmax(avg_sim)\n",
        "        selected_indices.append(central_idx)\n",
        "    return ''.join([sentences[i] for i in selected_indices])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qdcBFWduO6J",
        "outputId": "6532862d-ba8f-4938-c1ce-c367954bc377"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "So, this looks again a simple problem but we will see it is not that simple.So, this problem is related.\n",
            "So, we will start with the example of a simple decision tree.\n",
            "In English in general we do not combine words to make a single word.\n",
            "So, now before going into what is tokenization, I will just talk about a slightly related problem sentence segmentation.\n",
            "And this is important for the application of machine translation also because if you do a case folding here, you will know u s in lower case that means something else was such u s that is in United States.\n",
            "So, now for each dot or in general for every word, I need to decide whether this is the end of the sentence or not the end of the sentence.\n"
          ]
        }
      ],
      "source": [
        "summary=''\n",
        "for topic, sentences in clustered_sentences.items():\n",
        "    summary = extractive_summary(sentences,topic)\n",
        "    print(f\"{summary}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DxGUjx8WG3l"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUpAl9jzcxQW6iPeNS0O6i",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}